This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  workflows/
    integration-tests.yml
    unit-tests.yml
src/
  react_agent/
    graphs/
      research.py
    prompts/
      __init__.py
      analysis.py
      market.py
      query.py
      reflection.py
      research.py
      synthesis.py
      templates.py
      validation.py
    tools/
      jina.py
    utils/
      __init__.py
      content.py
      defaults.py
      extraction.py
      llm.py
      logging.py
      statistics.py
      validations.py
    __init__.py
    configuration.py
    state.py
tests/
  integration_tests/
    __init__.py
    test_graph.py
  unit_tests/
    __init__.py
    test_configuration.py
    test_extraction.py
.gitignore
.repomixignore
langgraph.json
LICENSE
Makefile
pyproject.toml
README.md
repomix.config.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/integration-tests.yml">
# This workflow will run integration tests for the current project once per day

name: Integration Tests

on:
  schedule:
    - cron: "37 14 * * *" # Run at 7:37 AM Pacific Time (14:37 UTC) every day
  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI

# If another scheduled run starts while this workflow is still running,
# cancel the earlier run in favor of the next run.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  integration-tests:
    name: Integration Tests
    strategy:
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.11", "3.12"]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          uv venv
          uv pip install -r pyproject.toml
          uv pip install -U pytest-asyncio vcrpy
      - name: Run integration tests
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_TRACING: true
          LANGSMITH_TEST_CACHE: tests/cassettes
        run: |
          uv run pytest tests/integration_tests
</file>

<file path=".github/workflows/unit-tests.yml">
# This workflow will run unit tests for the current project

name: CI

on:
  push:
    branches: ["main"]
  pull_request:
  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI

# If another push to the same PR or branch happens while this workflow is still running,
# cancel the earlier run in favor of the next run.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  unit-tests:
    name: Unit Tests
    strategy:
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.11", "3.12"]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          uv venv
          uv pip install -r pyproject.toml
      - name: Lint with ruff
        run: |
          uv pip install ruff
          uv run ruff check .
      - name: Lint with mypy
        run: |
          uv pip install mypy
          uv run mypy --strict src/
      - name: Check README spelling
        uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_file: .codespellignore
          path: README.md
      - name: Check code spelling
        uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_file: .codespellignore
          path: src/
      - name: Run tests with pytest
        run: |
          uv pip install pytest
          uv run pytest tests/unit_tests
</file>

<file path="src/react_agent/graphs/research.py">
"""Enhanced modular research framework using LangGraph.

This module implements a modular approach to the research process,
with specialized components for different research categories and
improved error handling and validation.
"""


from __future__ import annotations
import contextlib
import json
import asyncio
from typing import Any, Dict, List, Optional, Sequence, Union, cast, Tuple, Literal, Hashable
from datetime import datetime, timezone
from urllib.parse import urlparse
import re

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.constants import START
from langgraph.graph.state import CompiledStateGraph
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.runnables import RunnableConfig, ensure_config
from langchain_core.documents import Document
from typing_extensions import Annotated, TypedDict

from react_agent.utils.validations import is_valid_url
from react_agent.utils.llm import call_model, call_model_json
from react_agent.tools.jina import search
from react_agent.prompts.research import (
    QUERY_ANALYSIS_PROMPT,
    CLARIFICATION_PROMPT,
    EXTRACTION_PROMPTS,
    SYNTHESIS_PROMPT,
    VALIDATION_PROMPT,
    SEARCH_QUALITY_THRESHOLDS,
    get_extraction_prompt,
    get_default_extraction_result
)
from react_agent.prompts.synthesis import ENHANCED_REPORT_TEMPLATE
from react_agent.utils.logging import get_logger, log_dict, info_highlight, warning_highlight, error_highlight, log_step
from react_agent.utils.content import (
    preprocess_content,
    should_skip_content,
    chunk_text,
    merge_chunk_results,
    validate_content,
    detect_content_type
)
from react_agent.utils.extraction import (
    extract_statistics,
    enrich_extracted_fact,
    extract_category_information as extract_category_info
)
from react_agent.utils.statistics import (
    calculate_category_quality_score,
    calculate_overall_confidence,
    assess_synthesis_quality
)
from react_agent.prompts.query import (
    optimize_query,
    detect_vertical,
    expand_acronyms
)

# Initialize logger
logger = get_logger(__name__)

# Define SearchType as a Literal type
SearchType = Literal['general', 'authoritative', 'recent', 'comprehensive', 'technical']

# Add at module level after imports
from typing import Dict, List, Any, Optional, Tuple, cast

_document_cache: Dict[Tuple[str, str], Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]] = {}

class ResearchCategory(TypedDict):
    """State for a specific research category."""
    category: str  # The category being researched (market_dynamics, etc.)
    query: str  # The search query for this category
    search_results: List[Dict[str, Any]]  # Raw search results
    extracted_facts: List[Dict[str, Any]]  # Extracted facts
    sources: List[Dict[str, Any]]  # Source information
    complete: bool  # Whether this category is complete
    quality_score: float  # Quality score for this category (0.0-1.0)
    retry_count: int  # Number of retry attempts
    last_search_query: Optional[str]  # Last search query used
    status: str  # Status of this category (pending, in_progress, complete, failed)
    statistics: List[Dict[str, Any]]  # Extracted statistics from facts
    confidence_score: float  # Confidence score for this category (0.0-1.0)
    cross_validation_score: float  # Cross-validation score for facts (0.0-1.0)
    source_quality_score: float  # Quality score for sources (0.0-1.0)
    recency_score: float  # Recency score for sources (0.0-1.0)
    statistical_content_score: float  # Score for statistical content (0.0-1.0)

class ResearchState(TypedDict):
    """Main research state with modular components."""
    # Basic conversation data
    messages: Annotated[Sequence[BaseMessage], add_messages]

    # Original query and analysis
    original_query: str
    query_analysis: Optional[Dict[str, Any]]

    # Clarity and context
    missing_context: List[str]
    needs_clarification: bool
    clarification_request: Optional[str]
    human_feedback: Optional[str]

    # Category-specific research
    categories: Dict[str, ResearchCategory]

    # Synthesis and validation
    synthesis: Optional[Dict[str, Any]]
    validation_result: Optional[Dict[str, Any]]

    # Overall status
    status: str
    error: Optional[Dict[str, Any]]
    complete: bool
    
# --------------------------------------------------------------------
# 2. Core control flow nodes
# --------------------------------------------------------------------

async def initialize_research(state: ResearchState) -> Dict[str, Any]:
    """Initialize the research process with the user's query."""
    log_step("Initializing research process", 1, 10)

    if not state["messages"]:
        warning_highlight("No messages found in state")
        return {"error": {"message": "No messages in state", "phase": "initialization"}}

    last_message = state["messages"][-1]
    query = last_message.content if isinstance(last_message, BaseMessage) else ""

    if not query:
        warning_highlight("Empty query")
        return {"error": {"message": "Empty query", "phase": "initialization"}}

    info_highlight(f"Initializing research for query: {query}")

    categories = {
        category: {
            "category": category,
            "query": "",  # Will be filled by query analysis
            "search_results": [],
            "extracted_facts": [],
            "sources": [],
            "complete": False,
            "quality_score": 0.0,
            "retry_count": 0,
            "last_search_query": None,
            "status": "pending",
            "statistics": [],
            "confidence_score": 0.0,
            "cross_validation_score": 0.0,
            "source_quality_score": 0.0,
            "recency_score": 0.0,
            "statistical_content_score": 0.0
        }
        for category in SEARCH_QUALITY_THRESHOLDS.keys()
    }
    return {
        "original_query": query,
        "status": "initialized",
        "categories": categories,
        "missing_context": [],
        "needs_clarification": False,
        "complete": False
    }

async def analyze_query(state: ResearchState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    """Analyze the query to determine research categories and search terms."""
    log_step("Analyzing research query", 2, 10)

    query = state["original_query"].strip()
    if not query:
        warning_highlight("No query to analyze")
        return {"error": {"message": "No query to analyze", "phase": "query_analysis"}}

    if human_feedback := state.get("human_feedback", ""):
        info_highlight(f"Including user feedback in analysis: {human_feedback}")
        query = f"{query}\n\nAdditional context: {human_feedback}"

    # Prepare the analysis prompt
    analysis_prompt = QUERY_ANALYSIS_PROMPT.format(query=query)

    try:
        analysis_result = await call_model_json(
            messages=[{"role": "human", "content": analysis_prompt}],
            config=ensure_config(config)
        )

        # Ensure the response has the required structure
        if not isinstance(analysis_result, dict):
            error_highlight("Invalid response format from query analysis")
            return {"error": {"message": "Invalid response format", "phase": "query_analysis"}}

        # Initialize default values
        analysis_result = {
            "unspsc_categories": analysis_result.get("unspsc_categories", []),
            "search_components": analysis_result.get("search_components", {
                "primary_topic": "",
                "industry": "",
                "product_type": "",
                "geographical_focus": ""
            }),
            "search_terms": analysis_result.get("search_terms", {
                "market_dynamics": [],
                "provider_landscape": [],
                "technical_requirements": [],
                "regulatory_landscape": [],
                "cost_considerations": [],
                "best_practices": [],
                "implementation_factors": []
            }),
            "boolean_query": analysis_result.get("boolean_query", ""),
            "missing_context": analysis_result.get("missing_context", [])
        }

        log_dict(analysis_result, title="Query Analysis Result")

        # Check for missing context
        missing_context = analysis_result.get("missing_context", [])
        needs_clarification = len(missing_context) >= 2  # Only request clarification for multiple missing elements

        # Update category queries based on analysis
        categories = state["categories"]
        for category, category_state in categories.items():
            if search_terms := analysis_result.get("search_terms", {}).get(
                category, []
            ):
                # Detect vertical from the query
                vertical = detect_vertical(query)

                # Create optimized query for this category using the enhanced optimization
                optimized_query = optimize_query(
                    original_query=query,
                    category=category,
                    vertical=vertical,
                    include_all_keywords=len(search_terms) > 5  # Include all keywords for complex queries
                )
                category_state["query"] = optimized_query
                info_highlight(f"Set optimized query for {category}: {optimized_query}")
            else:
                # Use default query if no specific terms
                category_state["query"] = query

        return {
            "query_analysis": analysis_result,
            "categories": categories,
            "missing_context": missing_context,
            "needs_clarification": needs_clarification,
            "status": "analyzed"
        }
    except Exception as e:
        error_highlight(f"Error in query analysis: {str(e)}")
        return {"error": {"message": f"Error in query analysis: {str(e)}", "phase": "query_analysis"}}

async def request_clarification(state: ResearchState) -> Dict[str, Any]:
    """Request clarification from the user for missing context."""
    log_step("Requesting clarification", 3, 10)
    
    missing_context = state["missing_context"]
    if not missing_context:
        return {"needs_clarification": False}
    
    # Format the missing context items
    missing_sections = "\n".join([f"- {item}" for item in missing_context])
    
    # Get analysis data
    analysis = state.get("query_analysis", {})
    search_components = analysis.get("search_components", {}) if analysis else {}
    
    # Prepare the clarification request
    try:
        clarification_message = CLARIFICATION_PROMPT.format(
            query=state["original_query"],
            product_vs_service=search_components.get("product_type", "Unknown"),
            industry_context=search_components.get("industry", "Unknown"),
            geographical_focus=search_components.get("geographical_focus", "Unknown"),
            missing_sections=missing_sections
        )
        
        info_highlight("Generated clarification request")
        
        # Set up interrupt for user input
        return {
            "clarification_request": clarification_message,
            "__interrupt__": {
                "value": {
                    "question": clarification_message,
                    "missing_context": missing_context
                },
                "resumable": True,
                "ns": ["request_clarification"],
                "when": "during"
            }
        }
    except Exception as e:
        error_highlight(f"Error creating clarification request: {str(e)}")
        # Proceed without clarification in case of error
        return {"needs_clarification": False}

async def process_clarification(state: ResearchState) -> Dict[str, Any]:
    """Process user clarification and update the research context."""
    log_step("Processing clarification", 4, 10)
    
    if not state["messages"]:
        warning_highlight("No messages found for clarification")
        return {}
    
    # Get the latest message which should contain the user's clarification
    last_message = state["messages"][-1]
    clarification_content = str(last_message.content).strip()
    
    info_highlight(f"Processing user clarification: {clarification_content}")
    
    # Update the original query with the clarification
    updated_query = f"{state['original_query']}\n\nAdditional context: {clarification_content}"
    
    return {
        "human_feedback": clarification_content,
        "original_query": updated_query,  # Update the original query with clarification
        "needs_clarification": False,
        "missing_context": [],
        "status": "clarified"
    }

# --------------------------------------------------------------------
# 3. Module-specific research nodes
# --------------------------------------------------------------------

async def execute_category_search(
    state: ResearchState, 
    category: str,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Execute search for a specific research category."""
    log_step(f"Executing search for category: {category}", 5, 10)
    
    categories = state["categories"]
    if category not in categories:
        warning_highlight(f"Unknown category: {category}")
        return {}
    
    category_state = categories[category]
    query = category_state["query"]
    
    if not query:
        warning_highlight(f"No query available for category: {category}")
        return {}
    
    # Update status
    category_state["status"] = "searching"
    category_state["retry_count"] += 1
    
    # Get category-specific search parameters
    thresholds = SEARCH_QUALITY_THRESHOLDS.get(category, {})
    recency_days = int(str(thresholds.get("recency_threshold_days", 365)), base=10)
    
    # Configure search params based on category
    search_type_mapping: Dict[str, SearchType] = {
        "market_dynamics": "recent",  # Need fresh market data
        "provider_landscape": "comprehensive",  # Need diverse providers
        "technical_requirements": "technical",  # Technical content
        "regulatory_landscape": "authoritative",  # Authoritative sources
        "cost_considerations": "recent",  # Fresh pricing data
        "best_practices": "comprehensive",  # Diverse practices
        "implementation_factors": "comprehensive"  # Diverse approaches
    }
    
    search_type = search_type_mapping.get(category, "general")
    
    info_highlight(f"Executing {search_type} search for {category} with query: {query}")
    
    try:
        # Keep track of the query for retry tracking
        category_state["last_search_query"] = query
        
        # Execute the search with category parameter
        search_results = await search(
            query=query,
            search_type=search_type,
            recency_days=recency_days,
            category=category,  # Pass the category parameter
            config=ensure_config(config)
        )
        
        if not search_results:
            warning_highlight(f"No search results for {category}")
            category_state["status"] = "search_failed"
            return {"categories": categories}
        
        # Convert to the expected format and filter problematic content
        formatted_results = []
        for doc in search_results:
            url = doc.metadata.get("url", "")
            
            # Skip problematic content types
            if should_skip_content(url):
                info_highlight(f"Skipping problematic content type: {url}")
                continue
                
            # Validate and detect content type
            content = doc.page_content
            if not validate_content(content):
                info_highlight(f"Invalid content from {url}, skipping")
                continue
                
            content_type = detect_content_type(url, content)
            info_highlight(f"Detected content type: {content_type} for {url}")
            
            result = {
                "url": url,
                "title": doc.metadata.get("title", ""),
                "snippet": content,
                "source": doc.metadata.get("source", ""),
                "quality_score": doc.metadata.get("quality_score", 0.5),
                "published_date": doc.metadata.get("published_date"),
                "content_type": content_type
            }
            formatted_results.append(result)
        
        # Update the category state
        category_state["search_results"] = formatted_results
        category_state["status"] = "searched"
        
        info_highlight(f"Found {len(formatted_results)} results for {category}")
        return {"categories": categories}
        
    except Exception as e:
        error_highlight(f"Error in search for {category}: {str(e)}")
        category_state["status"] = "search_failed"
        return {"categories": categories}

async def _process_search_result(
    result: Dict[str, Any],
    category: str,
    original_query: str,
    config: Optional[RunnableConfig] = None
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """Process a single search result and extract information."""
    url = result.get("url", "")
    if not url or should_skip_content(url):
        return [], [], []
    
    # Check cache first
    cache_key = (url, category)
    if cache_key in _document_cache:
        info_highlight(f"Using cached result for {url} in {category}")
        return _document_cache[cache_key]
        
    content = result.get("snippet", "")
    if not validate_content(content):
        return [], [], []
        
    content_type = detect_content_type(url, content)
    prompt_template = get_extraction_prompt(
        category=category,
        query=original_query,
        url=url,
        content=content
    )
    
    try:
        facts, relevance_score = await extract_category_info(
            content=content,
            url=url,
            title=result.get("title", ""),
            category=category,
            original_query=original_query,
            prompt_template=prompt_template,
            extraction_model=call_model_json,
            config=ensure_config(config)
        )
        
        statistics = extract_statistics(content) or []
        
        # Add source information to facts
        for fact in facts:
            fact["source_url"] = url
            fact["source_title"] = result.get("title", "")
            
        source = {
            "url": url,
            "title": result.get("title", ""),
            "published_date": result.get("published_date"),
            "fact_count": len(facts),
            "relevance_score": relevance_score,
            "quality_score": result.get("quality_score", 0.5),
            "content_type": content_type
        }
        
        result_tuple = (facts, [source], statistics)
        _document_cache[cache_key] = result_tuple
        return result_tuple
        
    except Exception as e:
        warning_highlight(f"Error extracting from {url}: {str(e)}")
        return [], [], []

async def extract_category_information(
    state: ResearchState,
    category: str,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Extract information from search results for a specific category."""
    log_step(f"Extracting information for category: {category}", 6, 10)
    
    categories = state["categories"]
    if category not in categories:
        warning_highlight(f"Unknown category: {category}")
        return {}
    
    category_state = categories[category]
    search_results = category_state["search_results"]
    
    if not search_results:
        category_state["status"] = "extraction_failed"
        return {"categories": categories}
    
    category_state["status"] = "extracting"
    
    # Process all search results in parallel
    tasks = [
        asyncio.create_task(_process_search_result(
            result, category, state["original_query"], config
        ))
        for result in search_results
    ]
    
    results = await asyncio.gather(*tasks)
    
    # Aggregate results
    extracted_facts = []
    sources = []
    all_statistics = []
    
    for facts, source, statistics in results:
        extracted_facts.extend(facts)
        sources.extend(source)
        all_statistics.extend(statistics)
    
    # Update category state
    category_state.update({
        "extracted_facts": extracted_facts,
        "sources": sources,
        "statistics": all_statistics,
        "quality_score": calculate_category_quality_score(
            category=category,
            extracted_facts=extracted_facts,
            sources=sources,
            thresholds=SEARCH_QUALITY_THRESHOLDS.get(category, {})
        )
    })
    
    # Calculate derived scores
    quality_score = category_state["quality_score"]
    category_state.update({
        "confidence_score": quality_score,
        "cross_validation_score": quality_score * 0.8,
        "source_quality_score": quality_score * 0.9,
        "recency_score": quality_score * 0.7,
        "statistical_content_score": quality_score * 0.85
    })
    
    # Determine completion status
    thresholds = SEARCH_QUALITY_THRESHOLDS.get(category, {})
    min_facts = thresholds.get("min_facts", 3)
    min_sources = thresholds.get("min_sources", 2)
    
    category_state["complete"] = (
        len(extracted_facts) >= min_facts and len(sources) >= min_sources
    ) or category_state["retry_count"] >= 3
    
    category_state["status"] = "extracted" if category_state["complete"] else "extraction_incomplete"
    
    return {"categories": categories}

# Helper function to calculate quality score for a category
def calculate_category_quality_score(
    category: str,
    extracted_facts: List[Dict[str, Any]],
    sources: List[Dict[str, Any]],
    thresholds: Dict[str, Any]
) -> float:
    """Calculate quality score for a category based on extracted data."""
    # Base score starts at 0.3
    score = 0.3

    # Add points for number of facts
    min_facts = thresholds.get("min_facts", 3)
    fact_ratio = min(1.0, len(extracted_facts) / (min_facts * 2))
    score += fact_ratio * 0.2

    # Add points for number of sources
    min_sources = thresholds.get("min_sources", 2)
    source_ratio = min(1.0, len(sources) / (min_sources * 1.5))
    score += source_ratio * 0.2

    # Add points for authoritative sources
    auth_ratio = thresholds.get("authoritative_source_ratio", 0.5)
    authoritative_sources = [
        s for s in sources 
        if s.get("quality_score", 0.0) > 0.7 or
        any(domain in s.get("url", "") for domain in ['.edu', '.gov', '.org'])
    ]
    auth_source_ratio = len(authoritative_sources) / len(sources) if sources else 0
    if auth_source_ratio >= auth_ratio:
        score += 0.2
    else:
        score += (auth_source_ratio / auth_ratio) * 0.1

    # Add points for recency
    recency_threshold = thresholds.get("recency_threshold_days", 365)
    recent_sources = 0
    for source in sources:
        date_str = source.get("published_date")
        if not date_str:
            continue

        with contextlib.suppress(Exception):
            from dateutil import parser
            from datetime import datetime, timezone
            date = parser.parse(date_str)
            now = datetime.now(timezone.utc)
            days_old = (now - date).days
            if days_old <= recency_threshold:
                recent_sources += 1
    if sources:
        recency_ratio = recent_sources / len(sources)
        score += recency_ratio * 0.1

    return min(1.0, score)

async def execute_research_for_categories(
    state: ResearchState,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Execute research for all categories in parallel with improved caching."""
    log_step("Executing research for all categories", 5, 10)
    
    categories = state["categories"]
    research_tasks = []
    
    # Track completed categories to avoid reprocessing
    completed_categories = set()
    
    for category, category_state in categories.items():
        if category_state["complete"]:
            info_highlight(f"Category {category} already complete, skipping")
            completed_categories.add(category)
            continue
            
        # Check cache for this category
        cache_key = (state["original_query"], category)
        if cache_key in _document_cache:
            cached_results = _document_cache[cache_key]
            if cached_results:
                info_highlight(f"Using cached results for category: {category}")
                category_state["search_results"] = cached_results[0]
                category_state["extracted_facts"] = cached_results[1]
                category_state["sources"] = cached_results[2]
                category_state["complete"] = True
                completed_categories.add(category)
                continue
        
        # Execute search followed by extraction
        info_highlight(f"Adding research task for {category}")
        
        # Create async task for this category
        async def process_category(cat: str, cat_state: Dict[str, Any]) -> None:
            try:
                # Execute search
                search_result = await execute_category_search(state, cat, config)
                
                # Only extract if search was successful
                if cat in search_result.get("categories", {}) and search_result["categories"][cat]["status"] == "searched":
                    # Execute extraction
                    extraction_result = await extract_category_information(state, cat, config)
                    
                    # Cache the results if successful
                    if extraction_result and cat in extraction_result.get("categories", {}):
                        cat_state["search_results"] = extraction_result["categories"][cat]["search_results"]
                        cat_state["extracted_facts"] = extraction_result["categories"][cat]["extracted_facts"]
                        cat_state["sources"] = extraction_result["categories"][cat]["sources"]
                        _document_cache[(state["original_query"], cat)] = (
                            cat_state["search_results"],
                            cat_state["extracted_facts"],
                            cat_state["sources"]
                        )
            except Exception as e:
                error_highlight(f"Error processing category {cat}: {str(e)}")
                cat_state["status"] = "failed"
        
        task = asyncio.create_task(process_category(category, cast(Dict[str, Any], category_state)))
        research_tasks.append(task)
    
    # Wait for all research tasks to complete
    if research_tasks:
        await asyncio.gather(*research_tasks)
    
    # Check if all categories are complete
    all_complete = all(
        category_state["complete"] for category_state in categories.values()
    )
    
    if all_complete:
        info_highlight("All categories research complete")
        return {
            "status": "researched",
            "categories": categories
        }
    else:
        # Some categories still incomplete
        incomplete = [
            category for category, category_state in categories.items()
            if not category_state["complete"]
        ]
        info_highlight(f"Categories still incomplete: {', '.join(incomplete)}")
        return {
            "status": "research_incomplete",
            "categories": categories
        }

# --------------------------------------------------------------------
# 4. Synthesis and validation nodes
# --------------------------------------------------------------------

async def synthesize_research(
    state: ResearchState,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Synthesize all research data into a comprehensive result."""
    log_step("Synthesizing research results", 7, 10)

    categories = state["categories"]
    original_query = state["original_query"]

    research_data = {
        category: {
            "facts": category_state["extracted_facts"],
            "sources": category_state["sources"],
            "quality_score": category_state["quality_score"],
            "statistics": category_state["statistics"],
            "confidence_score": category_state["confidence_score"],
            "cross_validation_score": category_state["cross_validation_score"],
            "source_quality_score": category_state["source_quality_score"],
            "recency_score": category_state["recency_score"],
            "statistical_content_score": category_state[
                "statistical_content_score"
            ],
        }
        for category, category_state in categories.items()
    }
    # Generate prompt
    synthesis_prompt = SYNTHESIS_PROMPT.format(
        query=original_query,
        research_json=json.dumps(research_data, indent=2)
    )

    try:
        synthesis_result = await call_model_json(
            messages=[{"role": "human", "content": synthesis_prompt}],
            config=ensure_config(config)
        )

        # Calculate overall confidence
        category_scores = {
            category: state["quality_score"]
            for category, state in categories.items()
        }

        synthesis_quality = assess_synthesis_quality(synthesis_result)
        validation_score = 0.8  # Default validation score, can be updated later

        overall_confidence = calculate_overall_confidence(
            category_scores=category_scores,
            synthesis_quality=synthesis_quality,
            validation_score=validation_score
        )

        # Add confidence assessment to synthesis result
        synthesis_result["confidence_assessment"] = {
            "overall_score": overall_confidence,
            "synthesis_quality": synthesis_quality,
            "validation_score": validation_score,
            "category_scores": category_scores
        }

        log_dict(
            {
                "synthesis_sections": list(synthesis_result.get("synthesis", {}).keys()),
                "confidence_score": overall_confidence,
                "synthesis_quality": synthesis_quality,
                "validation_score": validation_score
            },
            title="Synthesis Overview"
        )

        info_highlight(f"Research synthesis complete with confidence score: {overall_confidence:.2f}")

        return {
            "synthesis": synthesis_result,
            "status": "synthesized"
        }
    except Exception as e:
        error_highlight(f"Error in research synthesis: {str(e)}")
        return {"error": {"message": f"Error in research synthesis: {str(e)}", "phase": "synthesis"}}

async def validate_synthesis(
    state: ResearchState,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Validate the synthesized research results."""
    log_step("Validating research synthesis", 8, 10)
    
    synthesis = state.get("synthesis")
    if not synthesis:
        warning_highlight("No synthesis to validate")
        return {"error": {"message": "No synthesis to validate", "phase": "validation"}}
    
    # Generate validation prompt
    validation_prompt = VALIDATION_PROMPT.format(
        synthesis_json=json.dumps(synthesis, indent=2)
    )
    
    try:
        validation_result = await call_model_json(
            messages=[{"role": "human", "content": validation_prompt}],
            config=ensure_config(config)
        )
        
        # Get validation status
        validation_results = validation_result.get("validation_results", {})
        is_valid = validation_results.get("is_valid", False)
        validation_score = validation_results.get("validation_score", 0.0)
        
        # Log validation results
        log_dict(
            {
                "is_valid": is_valid,
                "validation_score": validation_score,
                "critical_issues": validation_results.get("critical_issues", [])
            },
            title="Validation Results"
        )
        
        if is_valid:
            info_highlight(f"Validation passed with score: {validation_score:.2f}")
            return {
                "validation_result": validation_result,
                "status": "validated",
                "complete": True
            }
        else:
            warning_highlight(f"Validation failed with score: {validation_score:.2f}")
            return {
                "validation_result": validation_result,
                "status": "validation_failed"
            }
    except Exception as e:
        error_highlight(f"Error in synthesis validation: {str(e)}")
        return {"error": {"message": f"Error in synthesis validation: {str(e)}", "phase": "validation"}}

def format_citation(citation: Dict[str, Any]) -> str:
    """Format a citation into a readable string."""
    if not isinstance(citation, dict):
        return ""

    title = citation.get("title", "")
    url = citation.get("url", "")
    if title or url:
        return f"[{title}]({url})" if title and url else title or url
    else:
        return ""

def highlight_statistics_in_content(content: str, statistics: List[Dict[str, Any]]) -> str:
    """Highlight statistics in content by wrapping them in markdown bold."""
    if not statistics:
        return content
        
    highlighted = content
    for stat in statistics:
        if value := stat.get("value"):
            highlighted = highlighted.replace(str(value), f"**{value}**")
    return highlighted

def _format_section_content(section_data: Dict[str, Any]) -> str:
    """Format a single section's content with statistics and citations."""
    if not isinstance(section_data, dict) or "content" not in section_data:
        return ""
        
    content = section_data.get("content", "")
    statistics = section_data.get("statistics", [])
    citations = section_data.get("citations", [])
    
    # Highlight statistics in content
    highlighted_content = highlight_statistics_in_content(content, statistics)
    
    # Add citations if present
    if citations and isinstance(citations, list):
        citation_text = "\n\n**Sources:** " + ", ".join(
            format_citation(citation) for citation in citations if citation
        )
        return highlighted_content + citation_text
    
    return highlighted_content

def format_statistic(stat: Dict[str, Any]) -> str:
    """Format a statistic into a readable string."""
    if not isinstance(stat, dict):
        return ""
        
    value = stat.get("value")
    context = stat.get("context", "")
    if not value:
        return ""
        
    return f"{value} ({context})" if context else str(value)

def _format_key_statistics(statistics: List[Dict[str, Any]]) -> str:
    """Format key statistics section."""
    key_stats = [
        f"- {format_statistic(stat)}"
        for stat in sorted(statistics, key=lambda x: x.get("quality_score", 0), reverse=True)[:10]
        if format_statistic(stat)
    ]
    return "\n".join(key_stats) if key_stats else "No key statistics available."

def _format_sources(synthesis_content: Dict[str, Any]) -> str:
    """Format sources section from citations."""
    sources = {
        format_citation(citation)
        for section_data in synthesis_content.values()
        if isinstance(section_data, dict)
        for citation in section_data.get("citations", [])
        if isinstance(citation, dict) and format_citation(citation)
    }
    return "\n".join(f"- {source}" for source in sorted(sources)) if sources else "No sources available."

def _format_confidence_notes(confidence: Dict[str, Any]) -> str:
    """Format confidence notes from limitations and knowledge gaps."""
    notes = []
    if limitations := confidence.get("limitations", []):
        notes.append("**Limitations:** " + ", ".join(limitations))
    if knowledge_gaps := confidence.get("knowledge_gaps", []):
        notes.append("**Knowledge Gaps:** " + ", ".join(knowledge_gaps))
    return "\n".join(notes)

async def generate_recommendations(
    synthesis_content: Dict[str, Any],
    query: str,
    config: Optional[RunnableConfig] = None
) -> str:
    """Generate recommendations based on synthesis content."""
    if not synthesis_content:
        return "No recommendations available."
        
    recommendations = []
    for section_data in synthesis_content.values():
        if isinstance(section_data, dict) and "recommendations" in section_data:
            recommendations.extend(section_data["recommendations"])
            
    if not recommendations:
        return "No specific recommendations available based on the research."
        
    return "\n".join(f"- {rec}" for rec in recommendations)

async def prepare_final_response(
    state: ResearchState,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Prepare the final response with enhanced statistics and citations."""
    log_step("Preparing final response", 8, 10)
    
    synthesis = state.get("synthesis", {})
    synthesis_content = synthesis.get("synthesis", {}) if synthesis else {}
    confidence = synthesis.get("confidence_assessment", {}) if synthesis else {}
    
    # Collect all statistics
    all_statistics = [
        stat
        for category_state in state["categories"].values()
        for stat in category_state.get("statistics", [])
    ]
    
    # Format sections
    sections = {
        name: _format_section_content(data)
        for name, data in synthesis_content.items()
    }
    
    # Format final response
    response = ENHANCED_REPORT_TEMPLATE.format(
        title=f"Research Results: {state['original_query'].capitalize()}",
        executive_summary=sections.get("executive_summary", ""),
        key_statistics=_format_key_statistics(all_statistics),
        domain_overview=sections.get("domain_overview", ""),
        market_size=synthesis_content.get("market_dynamics", {}).get("market_size", ""),
        competitive_landscape=synthesis_content.get("market_dynamics", {}).get("competitive_landscape", ""),
        market_trends=synthesis_content.get("market_dynamics", {}).get("trends", ""),
        key_vendors=synthesis_content.get("provider_landscape", {}).get("key_vendors", ""),
        vendor_comparison=synthesis_content.get("provider_landscape", {}).get("vendor_comparison", ""),
        technical_requirements=sections.get("technical_requirements", ""),
        regulatory_landscape=sections.get("regulatory_landscape", ""),
        implementation_factors=sections.get("implementation_factors", ""),
        cost_structure=synthesis_content.get("cost_considerations", {}).get("cost_structure", ""),
        pricing_models=synthesis_content.get("cost_considerations", {}).get("pricing_models", ""),
        roi_considerations=synthesis_content.get("cost_considerations", {}).get("roi_considerations", ""),
        best_practices=sections.get("best_practices", ""),
        procurement_strategy=sections.get("contract_procurement_strategy", ""),
        recommendations=await generate_recommendations(synthesis_content, state["original_query"], config),
        sources=_format_sources(synthesis_content),
        confidence_score=confidence.get("overall_score", 0.0),
        generation_date=datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC"),
        confidence_notes=_format_confidence_notes(confidence)
    )
    
    return {
        "messages": [AIMessage(content=response)],
        "status": "complete",
        "complete": True
    }

# --------------------------------------------------------------------
# 5. Conditional routing functions
# --------------------------------------------------------------------

def should_request_clarification(state: ResearchState) -> Hashable:
    """Determine if clarification is needed."""
    needs_clarification = state.get("needs_clarification", False)
    has_feedback = bool(state.get("human_feedback"))
    
    if needs_clarification and not has_feedback:
        return "request_clarification"
    else:
        return "execute_research_for_categories"

def check_retry_or_continue(state: ResearchState) -> Hashable:
    """Determine if we should retry incomplete categories or continue."""
    categories = state["categories"]

    # Check if any categories need retry
    categories_to_retry = []
    categories_to_retry.extend(
        category
        for category, category_state in categories.items()
        if not category_state["complete"] and category_state["retry_count"] < 3
    )
    if categories_to_retry:
        # Still have categories to retry
        info_highlight(f"Categories to retry: {categories_to_retry}")
        return "execute_research_for_categories"
    else:
        # All categories either complete or max retries reached
        info_highlight("Research complete or max retries reached for all categories")
        return "synthesize_research"

def validate_or_complete(state: ResearchState) -> Hashable:
    """Determine if we should validate the synthesis or finish."""
    validation_result = state.get("validation_result", {})
    validation_results = validation_result.get("validation_results", {}) if validation_result else {}
    if is_valid := validation_results.get("is_valid", False):
        info_highlight("Validation passed, preparing final response")
    else:
        # Not valid, but we'll still finish
        warning_highlight("Validation failed, but preparing final response anyway")
    return "prepare_final_response"

def handle_error_or_continue(state: ResearchState) -> Hashable:
    """Determine if we should handle an error or continue."""
    error = state.get("error")
    
    if error:
        error_highlight(f"Error detected: {error.get('message')} in phase {error.get('phase')}")
        return "handle_error"
    else:
        return "continue"

# --------------------------------------------------------------------
# 6. Error handling
# --------------------------------------------------------------------

def _format_fact(fact: Dict[str, Any]) -> Optional[str]:
    """Format a single fact into a readable string."""
    if not isinstance(fact, dict):
        return None
        
    data = fact.get("data", {})
    fact_text = (
        data.get("fact") or
        data.get("requirement") or
        next((v for v in data.values() if isinstance(v, str) and v), None) or
        fact.get("fact")
    )
    return f"\n- {fact_text}" if fact_text else None

async def handle_error(state: ResearchState) -> Dict[str, Any]:
    """Handle errors gracefully and return a helpful message."""
    error = state.get("error") or {}
    phase = error.get("phase", "unknown")
    message = error.get("message", "An unknown error occurred")

    error_highlight(f"Handling error in phase {phase}: {message}")

    # Build error response with partial results
    error_response = [
        f"I encountered an issue while researching your query: {message}",
        "\nHere's what I was able to find before the error occurred:"
    ]

    # Add completed categories and their facts
    categories = state.get("categories", {})
    if complete_categories := [
        cat
        for cat, state in categories.items()
        if state.get("complete", False)
    ]:
        error_response.append(f"\n\nI completed research on: {', '.join(complete_categories)}")

        for category in complete_categories:
            if facts := categories[category].get("extracted_facts", []):
                error_response.append(f"\n\n## {category.replace('_', ' ').title()}")
                error_response.extend(
                    fact_text for fact in facts[:3]
                    if (fact_text := _format_fact(fact))
                )
    else:
        error_response.append("\n\nUnfortunately, I wasn't able to complete any research categories before the error occurred.")

    error_response.append("\n\nWould you like me to try again with a more specific query?")

    return {
        "messages": [AIMessage(content="".join(error_response))],
        "status": "error",
        "complete": True
    }

# --------------------------------------------------------------------
# 7. Build the graph
# --------------------------------------------------------------------

def create_research_graph() -> CompiledStateGraph:
    """Create the modular research graph."""
    graph = StateGraph(ResearchState)

    # Add the main nodes
    graph.add_node("initialize", initialize_research)
    graph.add_node("analyze_query", analyze_query)
    graph.add_node("request_clarification", request_clarification)
    graph.add_node("process_clarification", process_clarification)
    graph.add_node("execute_research_for_categories", execute_research_for_categories)
    graph.add_node("synthesize_research", synthesize_research)
    graph.add_node("validate_synthesis", validate_synthesis)
    graph.add_node("prepare_final_response", prepare_final_response)
    graph.add_node("handle_error", handle_error)
    graph.add_node("check_retry", lambda state: {"next_step": check_retry_or_continue(state)})
    graph.add_node("validate_or_complete", lambda state: {"next_step": validate_or_complete(state)})

    # Add error handling edges
    graph.add_conditional_edges(
        "initialize",
        handle_error_or_continue,
        {
            "handle_error": "handle_error",
            "continue": "analyze_query"
        }
    )

    # Set start point
    graph.add_edge(START, "initialize")

    # Add conditional branch for clarification from analyze_query
    graph.add_conditional_edges(
        "analyze_query",
        should_request_clarification,
        {
            "request_clarification": "request_clarification",
            "execute_research_for_categories": "execute_research_for_categories"
        }
    )
    
    graph.add_edge("request_clarification", "process_clarification")
    graph.add_edge("process_clarification", "analyze_query")

    # Research flow
    graph.add_conditional_edges(
        "execute_research_for_categories",
        handle_error_or_continue,
        {
            "handle_error": "handle_error",
            "continue": "check_retry"
        }
    )

    graph.add_conditional_edges(
        "check_retry",
        lambda state: state.get("next_step", "synthesize_research"),
        {
            "execute_research_for_categories": "execute_research_for_categories",
            "synthesize_research": "synthesize_research"
        }
    )

    # Synthesis and validation
    graph.add_conditional_edges(
        "synthesize_research",
        handle_error_or_continue,
        {
            "handle_error": "handle_error",
            "continue": "validate_synthesis"
        }
    )

    graph.add_conditional_edges(
        "validate_synthesis",
        handle_error_or_continue,
        {
            "handle_error": "handle_error",
            "continue": "validate_or_complete"
        }
    )

    graph.add_conditional_edges(
        "validate_or_complete",
        lambda state: state.get("next_step", "prepare_final_response"),
        {
            "prepare_final_response": "prepare_final_response"
        }
    )

    # Final steps
    graph.add_edge("prepare_final_response", END)
    graph.add_edge("handle_error", END)

    return graph.compile(interrupt_before=["process_clarification"])

# Create the graph instance
research_graph = create_research_graph()

# def create_category_graph(category: str) -> CompiledStateGraph:
#     """Create a specialized graph for a single research category."""
#     graph = StateGraph(ResearchState)
    
#     # Add nodes for single category research
#     graph.add_node("initialize", initialize_research)
#     graph.add_node("analyze_query", analyze_query)
#     graph.add_node("execute_category_search", lambda state, config: execute_category_search(state, category, config))
#     graph.add_node("extract_category_information", lambda state, config: extract_category_information(state, category, config))
#     graph.add_node("prepare_final_response", prepare_final_response)
#     graph.add_node("handle_error", handle_error)
    
#     # Add edges
#     graph.add_edge(START, "initialize")
#     graph.add_edge("initialize", "analyze_query")
#     graph.add_edge("analyze_query", "execute_category_search")
#     graph.add_edge("execute_category_search", "extract_category_information")
#     graph.add_edge("extract_category_information", "prepare_final_response")
#     graph.add_edge("prepare_final_response", END)
#     graph.add_edge("handle_error", END)
    
#     return graph.compile()
</file>

<file path="src/react_agent/prompts/__init__.py">
"""Prompt exports.

This module provides functionality for prompt exports
in the agent framework.
"""

from typing import Any, Dict, Final, List, Tuple

from react_agent.prompts.analysis import (
    ANALYSIS_PROMPT,
    TOOL_SELECTION_PROMPT,
)
from react_agent.prompts.market import (
    MARKET_DATA_PROMPT,
    MARKET_PROMPT,
)
from react_agent.prompts.reflection import (
    REFLECTION_PROMPT,
)
from react_agent.prompts.research import (
    ADDITIONAL_TOPICS_PROMPT,
    RESEARCH_AGENT_PROMPT,
    RESEARCH_BASE_PROMPT,
    TOPICS_PROMPT,
    QUERY_ANALYSIS_PROMPT,
    CLARIFICATION_PROMPT,
)

# Import all prompts from modules
from react_agent.prompts.templates import (
    ANALOGICAL_REASONING_PROMPT,
    COUNTERFACTUAL_PROMPT,
    CRITIQUE_PROMPT_TEMPLATE,
    EVALUATION_PROMPT_TEMPLATE,
    # Reflection prompts
    FEEDBACK_PROMPT_TEMPLATE,
    MAIN_PROMPT,
    METACOGNITION_PROMPT,
    NEWS_SEARCH_DESC,
    SCRAPE_DESC,
    STRUCTURED_OUTPUT_VALIDATION,
    SUMMARIZER_DESC,
    TOOL_INSTRUCTIONS,
    VALIDATION_REQUIREMENTS,
    WEB_SEARCH_DESC,
)
from react_agent.prompts.validation import (
    VALIDATION_AGENT_PROMPT,
    VALIDATION_BASE_PROMPT,
)

# Re-export everything for backward compatibility
__all__ = [
    # Templates
    "STRUCTURED_OUTPUT_VALIDATION",
    "VALIDATION_REQUIREMENTS",
    "MAIN_PROMPT",
    "WEB_SEARCH_DESC",
    "SCRAPE_DESC",
    "SUMMARIZER_DESC",
    "NEWS_SEARCH_DESC",
    "TOOL_INSTRUCTIONS",
    # Research
    "RESEARCH_BASE_PROMPT",
    "RESEARCH_AGENT_PROMPT",
    "MARKET_PROMPT",
    "TOPICS_PROMPT",
    "ADDITIONAL_TOPICS_PROMPT",
    "QUERY_ANALYSIS_PROMPT",
    "CLARIFICATION_PROMPT",
    # Validation
    "VALIDATION_BASE_PROMPT",
    "VALIDATION_AGENT_PROMPT",
    # Analysis
    "ANALYSIS_PROMPT",
    "TOOL_SELECTION_PROMPT",
    # Reflection
    "REFLECTION_PROMPT",
    # Reflection Templates
    "FEEDBACK_PROMPT_TEMPLATE",
    "EVALUATION_PROMPT_TEMPLATE",
    "CRITIQUE_PROMPT_TEMPLATE",
    "ANALOGICAL_REASONING_PROMPT",
    "COUNTERFACTUAL_PROMPT",
    "METACOGNITION_PROMPT",
    # Functions
    "get_report_template",
    "get_analysis_template",
]


# Common utility functions
def get_report_template() -> Dict[str, Any]:
    """Get the template for the final report."""
    return {
        "summary": "",
        "research_findings": {},
        "market_analysis": {},
        "generated_at": "",
    }


def get_analysis_template() -> Dict[str, Any]:
    """Get the template for research analysis."""
    return {
        "citations": [],
        "porters_five_forces": {},
        "swot_analysis": {},
        "pestel_analysis": {},
        "gap_analysis": {},
        "cost_benefit_analysis": {},
        "risk_assessment": {},
        "tco_analysis": {},
        "vendor_analysis": {},
        "benchmarking": {},
        "stakeholder_analysis": {},
        "compliance_analysis": {},
        "business_impact_analysis": {},
    }


# Required analysis topics
REQUIRED_ANALYSIS_TOPICS: List[Tuple[str, str]] = [
    ("Porter's Five Forces", "Analysis of competitive forces in the industry"),
    ("SWOT Analysis", "Strengths, weaknesses, opportunities, and threats"),
    (
        "PESTEL Analysis",
        "Political, economic, social, technological, environmental, and legal factors",
    ),
    ("GAP Analysis", "Current state vs desired state analysis"),
    ("Cost-Benefit Analysis", "Analysis of costs and benefits"),
    ("Risk Assessment", "Identification and analysis of potential risks"),
    ("Total Cost of Ownership", "Complete cost analysis including indirect costs"),
    ("Vendor Analysis", "Analysis of potential vendors and suppliers"),
    ("Benchmarking", "Comparison with industry standards and best practices"),
    ("Stakeholder Analysis", "Analysis of key stakeholders and their needs"),
    ("Compliance Analysis", "Analysis of regulatory and compliance requirements"),
    ("Business Impact Analysis", "Analysis of business impact and strategic alignment"),
]

# System prompts
SYSTEM_PROMPT_ANALYST: Final[str] = "You are an expert market research analyst."

# Finalization prompts
FINALIZATION_BASE_PROMPT: Final[
    str
] = """You are a Finalization Agent for RFP market analysis.
Your goal is to generate comprehensive reports and outputs from the validated research.

{STRUCTURED_OUTPUT_VALIDATION}

FINALIZATION REQUIREMENTS:
1. Research Report
   - Expand each analysis element into well-written sections
   - Maintain professional and clear writing style
   - Include supporting evidence and citations
   - Organize content logically and cohesively
   - Ensure all insights are actionable

2. Analysis Sections
   - Porter's 5 Forces analysis
   - SWOT analysis
   - PESTEL analysis
   - GAP analysis
   - Cost-benefit analysis
   - Risk assessment
   - Total cost of ownership
   - Vendor analysis
   - Benchmarking results
   - Stakeholder analysis
   - Compliance requirements
   - Business impact assessment

3. Market Basket Output
   - Generate CSV format
   - Include all line items
   - Maintain data accuracy
   - Format for easy review
   - Include citations and sources

4. Quality Requirements
   - Professional writing style
   - Clear section headings
   - Consistent formatting
   - Proper citation formatting
   - Executive summary
   - Recommendations section

RESPONSE_FORMAT:
{
    "outputs": {
        "research_report": {
            "format": "markdown",
            "content": "",
            "sections": []
        },
        "market_basket": {
            "format": "csv",
            "headers": [],
            "rows": []
        },
        "executive_summary": "",
        "recommendations": [],
        "confidence_scores": {},
        "key_findings": []
    },
    "metadata": {
        "generated_at": "",
        "version": "1.0",
        "validation_status": {
            "is_valid": false,
            "errors": [],
            "warnings": []
        }
    }
}

Current state: {state}
"""

FINALIZATION_AGENT_PROMPT: Final[str] = FINALIZATION_BASE_PROMPT.replace(
    "Your goal is to generate comprehensive reports and outputs from the validated research.\n",
    "Your goal is to generate comprehensive reports and outputs from the validated research.\n\n{STRUCTURED_OUTPUT_VALIDATION}\n",
)

ENRICHMENT_AGENT_PROMPT: Final[
    str
] = """You are an Enrichment Agent for RFP market analysis.
Enhance the following validated data while maintaining the JSON structure:
Validated Data:
{validated_data}
Required Schema:
{
    "rfp_analysis": {
        "analysis": {
            "porters_5_forces": {
                "competitive_rivalry": "",
                "threat_of_new_entrants": "",
                "threat_of_substitutes": "",
                "bargaining_power_buyers": "",
                "bargaining_power_suppliers": ""
            },
            "swot": {
                "strengths": [],
                "weaknesses": [],
                "opportunities": [],
                "threats": []
            },
            "recent_breakthroughs_and_disruptors": "",
            "cost_trends_and_projections": "",
            "typical_contract_clauses_and_pricing_nuances": "",
            "competitive_landscape": "",
            "citations": {
                "porters_5_forces": [],
                "swot": [],
                "recent_breakthroughs_and_disruptors": [],
                "cost_trends_and_projections": [],
                "typical_contract_clauses_and_pricing_nuances": [],
                "competitive_landscape": []
            }
        },
        "market_basket": [
            {
                "manufacturer_or_distributor": "",
                "item_number": "",
                "item_description": "",
                "uom": "",
                "estimated_qty_per_uom": 0.0,
                "unit_cost": 0.0,
                "citation": ""
            }
        ]
    },
    "confidence_score": 0.0
}
Enrichment Focus Areas:
1. Market Intelligence
   - Add emerging technology trends with citations
   - Include regulatory impact analysis with sources
   - Highlight market consolidation trends with references
   - Ensure at least 2 citations per section

2. Supplier Intelligence
   - Add supplier financial health indicators with sources
   - Include supplier innovation capabilities with citations
   - Note supplier market share trends with references
   - Validate supplier information from multiple sources

3. Pricing Intelligence
   - Add volume discount structures with citations
   - Include regional pricing variations with sources
   - Note seasonal pricing factors with references
   - Verify pricing data from reliable sources

4. Risk Analysis
   - Add supply chain risk factors with citations
   - Include mitigation strategies with sources
   - Note alternative sourcing options with references
   - Cross-reference risk data from multiple sources

5. Citation Requirements
   - Each analysis section must have at least 2 citations
   - Market basket items must each have a valid citation
   - Citations must be from reliable industry sources
   - Avoid using the same citation across multiple sections

CONFIDENCE_SCORING:
- Start with a base score of 0.5
- Add 0.1 for each section with 2+ unique citations
- Add 0.1 for each market basket item with verified pricing
- Subtract 0.1 for any section with fewer than 2 citations
- Maximum score is 0.95 until all data is fully verified

RESPONSE_REQUIREMENTS:
1. Output must be valid JSON only
2. All fields must be populated with enriched data
3. No explanatory text or comments
4. Include enrichment notes in "enrichment_details" if needed
5. Ensure complete, untruncated JSON output
6. Every section must have multiple citations
7. Market basket items must have verified sources

Current enrichment state: {current_state}
Conversation history:
{chat_history}
"""

# Export finalization prompts
__all__.extend(
    [
        "FINALIZATION_BASE_PROMPT",
        "FINALIZATION_AGENT_PROMPT",
        "ENRICHMENT_AGENT_PROMPT",
        "REQUIRED_ANALYSIS_TOPICS",
        "SYSTEM_PROMPT_ANALYST",
    ]
)
</file>

<file path="src/react_agent/prompts/analysis.py">
"""Analysis-specific prompts.

This module provides functionality for analysis-specific prompts
in the agent framework.
"""

from typing import Final

# Prompt for tool selection
TOOL_SELECTION_PROMPT: Final[str] = (
    """What information do we need to research about {current_topic}?"""
)

# Prompt for analysis of tool results
ANALYSIS_PROMPT: Final[str] = """
Based on the following research about {current_topic}, provide a comprehensive analysis:

{formatted_results}

Your analysis should include:
1. Key insights from the research
2. Patterns or trends identified
3. Implications for the business
4. Recommendations based on the findings
"""

# Prompt for analysis plan formulation
ANALYSIS_PLAN_PROMPT: Final[str] = """
Analysis Task: {task}
Available Data:
{data_summary}

Create a comprehensive plan for analyzing this data that will address the task.
Your plan should include:
1. Data preparation steps needed (e.g., cleaning, transformation)
2. Analysis methods to apply (e.g., descriptive statistics, correlation, regression)
3. Visualizations to create (e.g., histograms, scatter plots, bar charts)
4. Hypotheses to test (if applicable)
5. Statistical methods to use (e.g., t-tests, ANOVA, chi-squared)
6. Expected insights (what do you expect to learn from the analysis?)

Format your response as a JSON object with these sections.
"""

# Prompt for results interpretation
INTERPRET_RESULTS_PROMPT: Final[str] = """
Analysis Task: {task}
Analysis Results:
{analysis_results}
Analysis Plan:
{analysis_plan}

Interpret these results in the context of the original task.
Your interpretation should include:
1. Key findings and insights
2. Patterns and trends identified
3. Anomalies or unexpected results
4. Limitations of the analysis
5. Answers to specific questions in the task (if any)
6. Business implications (if applicable)

Format your response as a JSON object with these sections.
"""

# Prompt for report compilation
COMPILE_REPORT_PROMPT: Final[str] = """
Analysis Task: {task}
Analysis Results:
{analysis_results}
Interpretations:
{interpretations}
Visualizations:
{visualization_metadata}

Compile a comprehensive analysis report addressing the original task.
The report should:
1. Start with an executive summary of key findings.
2. Include an introduction explaining the context and objectives.
3. Describe the methodology and data sources.
4. Present the detailed findings with references to visualizations.
5. Discuss implications and recommendations.
6. Note limitations and potential future analysis.

Format the report as markdown with proper headings, lists, and sections.
"""
</file>

<file path="src/react_agent/prompts/market.py">
"""Market-specific prompts.

This module provides functionality for market data processing prompts
in the agent framework.
"""

from typing import Final

# Market data processing prompt
MARKET_DATA_PROMPT: Final[str] = """You are a Market Data Processor specialized in extracting pricing and sourcing information.

Your task is to analyze the following item and identify potential market sources, pricing, and manufacturer information.

INSTRUCTIONS:
1. Analyze the provided item description
2. Identify potential manufacturers or distributors
3. Find item numbers, descriptions, and pricing information
4. Format the response as a structured JSON object

RESPONSE FORMAT:
{
    "market_items": [
        {
            "manufacturer": "Name of manufacturer or distributor",
            "item_number": "Product/catalog number",
            "item_description": "Detailed description of the item",
            "unit_of_measure": "Each, Box, Case, etc.",
            "unit_cost": 0.00,
            "source": "Where this information was found"
        }
    ],
    "confidence_score": 0.0,
    "notes": "Any additional information or context"
}

IMPORTANT:
- Include multiple sources if available
- Provide accurate pricing information
- Include detailed item descriptions
- Assign a confidence score (0.0-1.0) based on data reliability
- Only include items that match the original description

Item to process: {state}
""" 

# Market research prompt
MARKET_PROMPT: Final[str] = """You are a Market Research Agent focused on building comprehensive market baskets.

Your task is to analyze the market for the following items and provide detailed market research information.

INSTRUCTIONS:
1. Analyze the market for each item
2. Identify market trends and dynamics
3. Research pricing and availability
4. Find potential suppliers and manufacturers
5. Analyze market competition
6. Identify regulatory requirements
7. Provide market forecasts and insights

RESPONSE FORMAT:
{
    "market_analysis": {
        "market_size": "Total market size with units",
        "growth_rate": "Annual growth rate",
        "trends": ["Key market trends"],
        "competition": ["Major competitors"],
        "regulations": ["Relevant regulations"]
    },
    "items": [
        {
            "item_name": "Name of item",
            "market_price": "Price range",
            "suppliers": ["List of suppliers"],
            "availability": "Supply status",
            "quality_metrics": ["Quality indicators"]
        }
    ],
    "confidence_score": 0.0,
    "notes": "Additional market insights"
}

IMPORTANT:
- Provide accurate market data with sources
- Include recent market trends and forecasts
- Consider both local and global market factors
- Note any market risks or uncertainties
- Assign confidence scores based on data reliability
"""
</file>

<file path="src/react_agent/prompts/query.py">
"""Enhanced query optimization to improve search relevance.

This enhances the query optimization to include more procurement-specific terms
and domain-specific vocabularies to increase search precision.
"""

from typing import Dict, List, Set, Optional
import re

# Domain-specific keyword repositories
PROCUREMENT_TERMS = {
    "sourcing": ["strategic sourcing", "supplier selection", "supplier qualification", "vendor selection"],
    "contracts": ["contract terms", "contract management", "agreement", "obligations", "SLAs", "KPIs"],
    "pricing": ["volume discounts", "rebates", "bulk discounts", "pricing models", "cost-plus", "fixed price"],
    "rfp": ["request for proposal", "request for information", "request for quote", "bid", "tender"],
    "procurement": ["procurement strategy", "procurement process", "buying", "purchasing"],
    "payment": ["invoicing", "payment terms", "purchase orders", "net-30", "net-60"],
    "suppliers": ["vendors", "distributors", "manufacturers", "providers", "supply base"],
    "strategy": ["category strategy", "category management", "spend analysis", "cost reduction"],
    "risk": ["risk management", "risk mitigation", "compliance", "qualifications", "certifications"],
    "process": ["auction", "reverse auction", "e-procurement", "p2p", "procure to pay"]
}

# Industry vertical specializations - can be expanded as needed
INDUSTRY_VERTICALS = {
    "education": ["university", "college", "campus", "academic", "educational"],
    "healthcare": ["hospital", "clinic", "medical", "patient care", "healthcare"],
    "manufacturing": ["factory", "industrial", "production", "assembly", "plant"],
    "government": ["public sector", "government agency", "municipal", "federal", "state"],
    "retail": ["retail operations", "store", "outlet", "retail chain", "merchandising"],
    "utilities": ["energy", "water", "electricity", "gas", "utility provider"]
}

# Maintenance categories
MAINTENANCE_CATEGORIES = {
    "preventive": ["preventive maintenance", "scheduled maintenance", "routine service"],
    "corrective": ["corrective maintenance", "repair", "fix", "troubleshooting"],
    "predictive": ["predictive maintenance", "condition monitoring", "predictive analytics"],
    "supplies": ["consumables", "disposables", "tools", "equipment", "spare parts"]
}

def detect_vertical(query: str) -> str:
    """Detect the industry vertical from the query."""
    query_lower = query.lower()

    return next(
        (
            vertical
            for vertical, keywords in INDUSTRY_VERTICALS.items()
            if any(keyword in query_lower for keyword in keywords)
        ),
        "general",
    )

def expand_acronyms(query: str) -> str:
    """Expand common industry acronyms in the query."""
    acronyms = {
        "mro": "maintenance repair operations",
        "rfp": "request for proposal",
        "rfq": "request for quote",
        "rfi": "request for information",
        "eam": "enterprise asset management",
        "cmms": "computerized maintenance management system",
        "kpi": "key performance indicator",
        "sla": "service level agreement",
        "tcoo": "total cost of ownership",
        "p2p": "procure to pay"
    }
    
    words = query.split()
    for i, word in enumerate(words):
        word_lower = word.lower().strip(",.;:()[]{}\"'")
        if word_lower in acronyms:
            # Replace acronym with expansion while preserving original casing and punctuation
            prefix = ""
            suffix = ""
            if not word.isalnum():
                prefix = word[:len(word) - len(word.lstrip(",.;:()[]{}\"'"))]
                suffix = word[len(word.rstrip(",.;:()[]{}\"'")):]
            words[i] = prefix + acronyms[word_lower] + " (" + word.strip(",.;:()[]{}\"'") + ")" + suffix
    
    return " ".join(words)

def optimize_query(
    original_query: str, 
    category: str, 
    vertical: Optional[str] = None,
    include_all_keywords: bool = False
) -> str:
    """Create optimized queries for specific research categories with enhanced domain-specific terms."""
    # Clean the original query
    original_query = original_query.split("Additional context:")[0].strip()
    
    # Expanded acronyms for better search results
    expanded_query = expand_acronyms(original_query)
    
    # Detect vertical if not provided
    if vertical is None:
        vertical = detect_vertical(original_query)
    
    # Get vertical-specific terms
    vertical_terms = INDUSTRY_VERTICALS.get(vertical, [""])
    
    # Define enhanced category-specific query templates with keyword banks
    query_templates = {
        "market_dynamics": {
            "template": "{primary_terms} market trends",
            "keyword_groups": ["contracts", "procurement", "strategy"]
        },
        "provider_landscape": {
            "template": "{primary_terms} vendors suppliers",
            "keyword_groups": ["suppliers", "rfp", "strategy"]
        },
        "technical_requirements": {
            "template": "{primary_terms} technical specifications",
            "keyword_groups": ["rfp", "procurement", "risk"]
        },
        "regulatory_landscape": {
            "template": "{primary_terms} regulations compliance",
            "keyword_groups": ["contracts", "risk", "process"]
        },
        "cost_considerations": {
            "template": "{primary_terms} pricing cost budget",
            "keyword_groups": ["pricing", "payment", "strategy"]
        },
        "best_practices": {
            "template": "{primary_terms} best practices",
            "keyword_groups": ["strategy", "risk", "process"]
        },
        "implementation_factors": {
            "template": "{primary_terms} implementation factors",
            "keyword_groups": ["procurement", "suppliers", "risk"]
        }
    }
    
    # Extract primary terms from the query (filter out common words)
    words = expanded_query.split()
    stop_words = ["help", "me", "research", "find", "information", "about", "on", "for", 
                  "the", "and", "or", "in", "to", "with", "by", "is", "are"]
    
    primary_terms = []
    for word in words:
        if word.lower() not in stop_words and len(word) > 3:
            primary_terms.append(word)
            # Limit to first 3-4 meaningful terms
            if len(primary_terms) >= 4:
                break
    
    # If no primary terms found, use the whole query up to a limit
    if not primary_terms:
        primary_terms = words[:3]
    
    # Get template for this category
    if category not in query_templates:
        return " ".join(primary_terms)
    
    template = query_templates[category]["template"]
    
    # Get procurement terms for this category (but use fewer to keep query simple)
    procurement_terms = []
    if include_all_keywords:
        # Include more keywords for comprehensive searches but still limit
        for group, terms in PROCUREMENT_TERMS.items():
            if group in query_templates[category]["keyword_groups"]:
                procurement_terms.append(terms[0])  # Just top 1 term from each group
    else:
        # Include only minimal keyword groups
        keyword_groups = query_templates[category]["keyword_groups"]
        if keyword_groups:
            top_group = keyword_groups[0]
            if top_group in PROCUREMENT_TERMS:
                procurement_terms.append(PROCUREMENT_TERMS[top_group][0])  # Just top term
    
    # Format the template with just essential terms
    primary_terms_str = " ".join(primary_terms)
    
    optimized_query = template.format(
        primary_terms=primary_terms_str
    )
    
    # Add at most one procurement term if we need to for context
    if procurement_terms:
        optimized_query += " " + procurement_terms[0]
        
    # Add at most one vertical term if needed
    if vertical != "general" and vertical_terms:
        optimized_query += " " + vertical_terms[0]
    
    # Ensure the query isn't too long for search engines
    if len(optimized_query) > 100:
        optimized_query = optimized_query[:100].rsplit(' ', 1)[0]
    
    return optimized_query
</file>

<file path="src/react_agent/prompts/reflection.py">
"""Reflection and critique prompts.

This module provides functionality for reflection and critique prompts
in the agent framework.
"""

from typing import Final

# Reflection prompt
# Parameters:
#   current_state: The current state of the agent
#   validation_targets: List of targets to validate
REFLECTION_PROMPT: Final[
    str
] = """You are a Reflection Agent responsible for validating research findings and preventing hallucinations.
Your tasks include:

1. Citation Validation
- Check all URLs for validity (no 404s)
- Verify source credibility
- Ensure citation dates are recent

2. Confidence Scoring
- Evaluate research findings confidence (threshold: 98%)
- Score market data reliability
- Assess source quality

3. Structured Output Validation
- Verify all required fields are populated
- Check data format consistency
- Validate numerical values

4. Quality Control
- Flag potential hallucinations
- Identify data gaps
- Request additional research if needed

Current state: {current_state}
Validation targets: {validation_targets}
"""
</file>

<file path="src/react_agent/prompts/research.py">
"""Enhanced research-specific prompts.

This module provides specialized prompts for different research categories
to improve extraction quality and relevance.
"""

from typing import Final, Dict, List, Any, Optional, Union
import json
from datetime import datetime

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight
from react_agent.utils.defaults import get_default_extraction_result

# Initialize logger
logger = get_logger(__name__)

# Base templates for common validation requirements
STRUCTURED_OUTPUT_VALIDATION: Final[str] = """CRITICAL: All responses MUST:
1. Be valid JSON only - no additional text or comments
2. Follow the exact schema provided
3. Never return empty or null values - use empty strings or arrays instead
4. Include all required fields
5. Use proper data types (strings, numbers, arrays)
6. Maintain proper JSON syntax
7. Include citations for all data points
8. Pass JSON schema validation

Any response that fails these requirements will be rejected."""

# Enhanced query analysis prompt with improved categorization and structure
QUERY_ANALYSIS_PROMPT: Final[str] = """Analyze the following research query to generate targeted search terms.

Query: {query}

TASK:
Break down this query into precise search components following these rules:
1. Use the UNSPSC taxonomy to identify relevant procurement categories
2. Extract no more than 3-5 focused keywords per category
3. Prioritize specificity over quantity
4. Identify the specific industry verticals, markets, and sectors
5. Determine geographical scope if relevant

FORMAT YOUR RESPONSE AS JSON:
{{
    "unspsc_categories": [
        {{"code": "code", "name": "category name", "relevance": 0.0-1.0}}
    ],
    "search_components": {{
        "primary_topic": "", 
        "industry": "",
        "product_type": "",
        "geographical_focus": ""
    }},
    "search_terms": {{
        "market_dynamics": [],
        "provider_landscape": [],
        "technical_requirements": [],
        "regulatory_landscape": [],
        "cost_considerations": [],
        "best_practices": [],
        "implementation_factors": []
    }},
    "boolean_query": "",
    "missing_context": []
}}

IMPORTANT: 
- Keep each category to a MAXIMUM of 5 focused search terms
- Only include truly essential items in "missing_context" - make reasonable assumptions
- For "boolean_query" create a precise search string using AND/OR operators
- Assign relevance scores (0.0-1.0) to each UNSPSC category
- Your response must be valid JSON with all fields present
- Do not include any comments or additional text in the JSON response
"""

# Specialized extraction prompts for different research categories
EXTRACTION_PROMPTS: Dict[str, str] = {
    "market_dynamics": """Extract factual information about MARKET DYNAMICS from this content about {query}.

URL: {url}

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about market size, growth rates, trends, forecasts, competitive dynamics, and procurement patterns
2. Format each fact with:
   - The fact statement
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If the document doesn't contain relevant market data, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_facts": [
    {{
      "fact": "Clear factual statement about market dynamics",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low",
      "data_type": "market_size/growth_rate/trend/forecast/competitive/procurement_pattern"
    }}
  ],
  "market_metrics": {{
    "market_size": null,  // Include if available with units
    "growth_rate": null,  // Include if available with time period
    "forecast_period": null,  // Include if available
    "procurement_volume": null,  // Include if available
    "contract_value": null  // Include if available
  }},
  "relevance_score": 0.0-1.0
}}
""",

    "provider_landscape": """Extract factual information about PROVIDERS/VENDORS from this content about {query}.

URL: {url}

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about vendors, suppliers, service providers, manufacturers, distributors, and market players
2. Format each fact with:
   - The vendor name and specific details
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no vendor information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_vendors": [
    {{
      "vendor_name": "Name of vendor",
      "description": "What they provide",
      "market_position": "leader/challenger/niche",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low",
      "contract_status": "active/expired/pending",
      "contract_terms": "Key contract terms if available",
      "pricing_model": "Pricing structure if available"
    }}
  ],
  "vendor_relationships": [
    {{
      "relationship_type": "partnership/competition/acquisition/contract",
      "entities": ["vendor1", "vendor2"],
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low",
      "contract_details": "Contract details if available"
    }}
  ],
  "relevance_score": 0.0-1.0
}}
""",

    "technical_requirements": """Extract factual information about TECHNICAL REQUIREMENTS from this content about {query}.

URL: {url}

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about specifications, standards, technologies, and requirements
2. Format each fact with:
   - The technical requirement or specification
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no technical information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_requirements": [
    {{
      "requirement": "Specific technical requirement",
      "category": "hardware/software/compliance/integration/performance",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "standards": [
    {{
      "standard_name": "Name of standard or protocol",
      "description": "Brief description",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}
""",

    "regulatory_landscape": """Extract factual information about REGULATIONS & COMPLIANCE from this content about {query}.

URL: {url}

CRITICAL: Your response must be a valid JSON object starting with '{{' and ending with '}}'. Do not include any additional text, explanations, or markdown formatting.

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about regulations, laws, compliance requirements, and standards
2. Format each regulation with:
   - The regulation name and jurisdiction
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no regulatory information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_regulations": [
    {{
      "regulation": "Name of regulation/law/standard",
      "jurisdiction": "Geographical or industry scope",
      "description": "Brief description of requirement",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "compliance_requirements": [
    {{
      "requirement": "Specific compliance requirement",
      "description": "What must be done",
      "source_text": "Direct quote from content", 
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}

CRITICAL REQUIREMENTS:
1. Response must be a valid JSON object
2. All fields must be present (even if empty)
3. Use proper data types (strings, numbers, arrays)
4. Never use null values - use empty arrays instead
5. Include source_text for every regulation and requirement
6. Assign confidence ratings for every item
7. Calculate relevance_score based on content quality
8. Do not include any text outside the JSON object
9. Do not include comments or trailing commas
10. Use double quotes for all strings
11. Ensure all JSON objects are properly closed with matching braces
12. Do not include any line breaks or whitespace outside the JSON structure
13. Do not include any markdown code block markers (```)
14. Do not include any explanatory text before or after the JSON
15. The response must start with '{{' and end with '}}' only
16. Do not include any text that would make the JSON invalid
17. Do not include any text that would make the JSON parsing fail
18. Do not include any text that would make the JSON validation fail
19. Do not include any text that would make the JSON schema validation fail
20. Do not include any text that would make the JSON structure invalid

EXAMPLE OF VALID RESPONSE:
{{
  "extracted_regulations": [
    {{
      "regulation": "Example Regulation",
      "jurisdiction": "Example Jurisdiction",
      "description": "Example description",
      "source_text": "Example quote",
      "confidence": "high"
    }}
  ],
  "compliance_requirements": [
    {{
      "requirement": "Example requirement",
      "description": "Example description",
      "source_text": "Example quote",
      "confidence": "high"
    }}
  ],
  "relevance_score": 0.8
}}

CRITICAL: Your response must be ONLY the JSON object above, with no additional text, comments, or formatting. The response must start with '{{' and end with '}}' only.""",

    "cost_considerations": """Extract factual information about COSTS & PRICING from this content about {query}.

URL: {url}

CRITICAL: Your response must be a valid JSON object starting with '{{' and ending with '}}'. Do not include any additional text, explanations, or markdown formatting.

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about pricing, costs, budgets, TCO, ROI, and financial considerations
2. Format each fact with:
   - The specific cost information
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no cost information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_costs": [
    {{
      "cost_item": "Specific cost element",
      "amount": null,  // Include if available with currency
      "context": "Description of pricing context",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "pricing_models": [
    {{
      "model_type": "subscription/one-time/usage-based/etc",
      "description": "How the pricing works",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}

CRITICAL REQUIREMENTS:
1. Response must be a valid JSON object
2. All fields must be present (even if empty)
3. Use proper data types (strings, numbers, arrays)
4. Never use null values - use empty arrays instead
5. Include source_text for every cost and pricing model
6. Assign confidence ratings for every item
7. Calculate relevance_score based on content quality
8. Do not include any text outside the JSON object
9. Do not include comments or trailing commas
10. Use double quotes for all strings
""",

    "best_practices": """Extract factual information about BEST PRACTICES from this content about {query}.

URL: {url}

CRITICAL: Your response must be a valid JSON object starting with '{{' and ending with '}}'. Do not include any additional text, explanations, or markdown formatting.

INSTRUCTIONS:
1. ONLY extract VERIFIED best practices, methodologies, and success factors
2. Format each practice with:
   - The specific practice or methodology
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no best practices are found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_practices": [
    {{
      "practice": "Specific best practice or methodology",
      "description": "Detailed description",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "methodologies": [
    {{
      "methodology": "Name of methodology",
      "description": "How it works",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}

CRITICAL REQUIREMENTS:
1. Response must be a valid JSON object
2. All fields must be present (even if empty)
3. Use proper data types (strings, numbers, arrays)
4. Never use null values - use empty arrays instead
5. Include source_text for every practice and methodology
6. Assign confidence ratings for every item
7. Calculate relevance_score based on content quality
8. Do not include any text outside the JSON object
9. Do not include comments or trailing commas
10. Use double quotes for all strings
""",

    "implementation_factors": """Extract factual information about IMPLEMENTATION FACTORS from this content about {query}.

URL: {url}

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about implementation challenges, success factors, and considerations
2. Format each factor with:
   - The specific implementation factor
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no implementation information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_factors": [
    {{
      "factor": "Specific implementation factor",
      "description": "Detailed description",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "challenges": [
    {{
      "challenge": "Specific implementation challenge",
      "description": "What makes it challenging",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}
"""
}

# Enhanced synthesis prompt with better structure
SYNTHESIS_PROMPT: Final[str] = """Create a comprehensive synthesis of research findings for: {query}

REQUIREMENTS:
1. Structure your synthesis with these EXACT sections:
   - Domain Overview: Essential context and background
   - Market Dynamics: Size, growth, trends, competition, procurement patterns
   - Provider Landscape: Key vendors, manufacturers, distributors, and their positioning
   - Technical Requirements: Specifications, standards, and procurement requirements
   - Regulatory Landscape: Compliance, legal requirements, and procurement regulations
   - Implementation Factors: Resources, process, challenges, and procurement considerations
   - Cost Analysis: Pricing, ROI, financial factors, volume discounts, and contract terms
   - Best Practices: Recommended approaches for procurement and sourcing
   - Contract & Procurement Strategy: Contract terms, negotiation strategies, and procurement processes

2. For EACH section:
   - Synthesize insights from multiple sources when available
   - Address inconsistencies and note knowledge gaps
   - Include SPECIFIC facts with proper citations
   - Present balanced perspectives where there are differences
   - Prioritize VERIFIED information from authoritative sources
   - If information is missing for any section, explicitly note this

3. For "Confidence Assessment":
   - Evaluate information completeness for each section
   - Identify potential biases in the sources
   - Note limitations in the research
   - Assign justified confidence scores by section

AVAILABLE RESEARCH:
{research_json}

FORMAT RESPONSE AS JSON:
{{
  "synthesis": {{
    "domain_overview": {{ "content": "", "citations": [] }},
    "market_dynamics": {{ "content": "", "citations": [] }},
    "provider_landscape": {{ "content": "", "citations": [] }},
    "technical_requirements": {{ "content": "", "citations": [] }},
    "regulatory_landscape": {{ "content": "", "citations": [] }},
    "implementation_factors": {{ "content": "", "citations": [] }},
    "cost_considerations": {{ "content": "", "citations": [] }},
    "best_practices": {{ "content": "", "citations": [] }},
    "contract_procurement_strategy": {{ "content": "", "citations": [] }}
  }},
  "confidence_assessment": {{
    "overall_score": 0.0-1.0,
    "section_scores": {{
      "domain_overview": 0.0-1.0,
      "market_dynamics": 0.0-1.0,
      "provider_landscape": 0.0-1.0,
      "technical_requirements": 0.0-1.0,
      "regulatory_landscape": 0.0-1.0,
      "implementation_factors": 0.0-1.0,
      "cost_considerations": 0.0-1.0,
      "best_practices": 0.0-1.0,
      "contract_procurement_strategy": 0.0-1.0
    }},
    "limitations": [],
    "knowledge_gaps": []
  }}
}}

REMEMBER:
- Prioritize factual accuracy over comprehensiveness
- Only include claims that are supported by the research
- Use clear, concise language focused on business impact
- Highlight conflicting information when present
- Pay special attention to procurement and sourcing-related insights
"""

# Enhanced validation prompt with adaptive thresholds
VALIDATION_PROMPT: Final[str] = """Validate the research synthesis against these criteria:

VALIDATION CRITERIA:
1. Factual Accuracy
   - Does each claim have proper citation?
   - Are the citations from credible sources?
   - Are claims consistent with the source material?

2. Comprehensive Coverage
   - Are all required sections populated?
   - Is the depth appropriate for each section?
   - Are there any significant knowledge gaps?

3. Source Quality
   - Are sources diverse and authoritative?
   - Are recent sources used where appropriate?
   - Is there over-reliance on any single source?

4. Overall Quality
   - Is confidence assessment realistic?
   - Are limitations properly acknowledged?
   - Is the synthesis balanced and objective?

RESEARCH SYNTHESIS TO VALIDATE:
{synthesis_json}

FORMAT RESPONSE AS JSON:
{{
  "validation_results": {{
    "is_valid": true/false,
    "validation_score": 0.0-1.0,
    "section_validations": {{
      "domain_overview": {{ "is_valid": true/false, "issues": [] }},
      "market_dynamics": {{ "is_valid": true/false, "issues": [] }},
      "provider_landscape": {{ "is_valid": true/false, "issues": [] }},
      "technical_requirements": {{ "is_valid": true/false, "issues": [] }},
      "regulatory_landscape": {{ "is_valid": true/false, "issues": [] }},
      "implementation_factors": {{ "is_valid": true/false, "issues": [] }},
      "cost_considerations": {{ "is_valid": true/false, "issues": [] }},
      "best_practices": {{ "is_valid": true/false, "issues": [] }}
    }},
    "critical_issues": [],
    "improvement_suggestions": []
  }},
  "adaptive_threshold": {{
    "minimum_valid_sections": 0-8,
    "required_sections": [],
    "section_weights": {{
      "domain_overview": 0.0-1.0,
      "market_dynamics": 0.0-1.0,
      "provider_landscape": 0.0-1.0,
      "technical_requirements": 0.0-1.0,
      "regulatory_landscape": 0.0-1.0,
      "implementation_factors": 0.0-1.0,
      "cost_considerations": 0.0-1.0,
      "best_practices": 0.0-1.0
    }}
  }}
}}

IMPORTANT:
- Calculate "minimum_valid_sections" based on query complexity and available data
- Identify critical sections as "required_sections" based on query intent
- Assign weights to sections based on importance to the query
- A synthesis can be valid even with some sections incomplete if priority sections are solid
- Flag fabricated or unsupported claims as critical issues
"""

# Enhanced report template with executive summary format
REPORT_TEMPLATE: Final[str] = """
# Research Report: {query}

## Executive Summary
{executive_summary}

## Key Findings
{key_findings}

## Detailed Analysis

### Market Dynamics
{market_dynamics}

### Provider Landscape
{provider_landscape}

### Technical Requirements
{technical_requirements}

### Regulatory Landscape
{regulatory_landscape}

### Implementation Considerations
{implementation_factors}

### Cost Analysis
{cost_considerations}

### Best Practices
{best_practices}

### Contract & Procurement Strategy
{contract_procurement_strategy}

## Recommendations
{recommendations}

## Sources and Citations
{sources}

---
Confidence Score: {confidence_score}
Generated: {generation_date}
"""

# Enhanced clarity request prompt
CLARIFICATION_PROMPT: Final[str] = """I'm analyzing your research request: "{query}"

Based on my initial analysis, I need some additional context to provide you with the most relevant research.

What I understand so far:
- Product/Service Focus: {product_vs_service}
- Industry Context: {industry_context}
- Geographical Scope: {geographical_focus}

To deliver more precise and comprehensive research, I need clarification on:

{missing_sections}

Could you please provide these additional details? This will help me focus the research on your specific needs rather than making assumptions.

Even with partial clarification, I can begin the research process and refine as we go."""

# Category-specific search quality thresholds
SEARCH_QUALITY_THRESHOLDS: Dict[str, Dict[str, float]] = {
    "market_dynamics": {
        "min_sources": 3,
        "min_facts": 5,
        "recency_threshold_days": 180,  # Market data needs to be recent
        "authoritative_source_ratio": 0.5  # At least half from authoritative sources
    },
    "provider_landscape": {
        "min_sources": 3,
        "min_facts": 3,
        "recency_threshold_days": 365,
        "authoritative_source_ratio": 0.3
    },
    "technical_requirements": {
        "min_sources": 2,
        "min_facts": 3,
        "recency_threshold_days": 730,  # Technical specs can be older
        "authoritative_source_ratio": 0.7  # Need highly authoritative sources
    },
    "regulatory_landscape": {
        "min_sources": 2,
        "min_facts": 2,
        "recency_threshold_days": 730,
        "authoritative_source_ratio": 0.8  # Regulatory info needs official sources
    },
    "cost_considerations": {
        "min_sources": 2,
        "min_facts": 3,
        "recency_threshold_days": 365,  # Pricing should be recent
        "authoritative_source_ratio": 0.4
    },
    "best_practices": {
        "min_sources": 2,
        "min_facts": 3,
        "recency_threshold_days": 730,
        "authoritative_source_ratio": 0.5
    },
    "implementation_factors": {
        "min_sources": 2,
        "min_facts": 3,
        "recency_threshold_days": 730,
        "authoritative_source_ratio": 0.4
    }
}

# Helper function for creating category-specific search prompts
def get_extraction_prompt(category: str, query: str, url: str, content: str) -> str:
    """Get the appropriate extraction prompt for a specific category."""
    if category in EXTRACTION_PROMPTS:
        prompt = EXTRACTION_PROMPTS[category]
        return prompt.format(
            query=query,
            url=url,
            content=content
        )
    else:
        # Fallback to general extraction prompt
        prompt = EXTRACTION_PROMPTS["market_dynamics"]
        return prompt.format(
            query=query,
            url=url,
            content=content
        )

# Prompt for identifying additional research topics
ADDITIONAL_TOPICS_PROMPT: Final[str] = """Based on the current research findings, identify additional topics that would enhance the analysis.

Current Research:
{current_research}

Consider:
1. Related market segments or industries
2. Emerging technologies or trends
3. Regulatory or compliance areas
4. Implementation considerations
5. Cost factors
6. Best practices

Format your response as JSON:
{
    "additional_topics": [
        {
            "topic": "Topic name",
            "relevance": 0.0-1.0,
            "rationale": "Why this topic is important"
        }
    ],
    "priority_order": ["topic1", "topic2", ...],
    "estimated_effort": {
        "topic1": "high/medium/low",
        "topic2": "high/medium/low",
        ...
    }
}"""

# Base research prompt
RESEARCH_BASE_PROMPT: Final[str] = """You are a Research Agent focused on gathering comprehensive market intelligence.

Your task is to analyze the following query and provide detailed research findings.

Query: {query}

INSTRUCTIONS:
1. Break down the query into research components
2. Identify key areas for investigation
3. Gather relevant market data
4. Analyze trends and patterns
5. Synthesize findings

RESPONSE FORMAT:
{
    "research_components": ["Component 1", "Component 2"],
    "key_findings": ["Finding 1", "Finding 2"],
    "sources": ["Source 1", "Source 2"],
    "confidence_score": 0.0
}"""

# Research agent prompt
RESEARCH_AGENT_PROMPT: Final[str] = """You are an advanced Research Agent specialized in market analysis.

Your task is to conduct comprehensive research on the following topic.

Topic: {topic}

INSTRUCTIONS:
1. Identify key research areas
2. Gather market intelligence
3. Analyze trends and patterns
4. Evaluate sources and credibility
5. Synthesize findings

RESPONSE FORMAT:
{
    "research_areas": ["Area 1", "Area 2"],
    "findings": ["Finding 1", "Finding 2"],
    "sources": ["Source 1", "Source 2"],
    "confidence_score": 0.0
}"""

# Topics prompt
TOPICS_PROMPT: Final[str] = """Analyze the following query to identify key research topics.

Query: {query}

INSTRUCTIONS:
1. Break down the query into main topics
2. Identify subtopics for each main topic
3. Prioritize topics by relevance
4. Consider industry context
5. Note any specialized areas

RESPONSE FORMAT:
{
    "main_topics": ["Topic 1", "Topic 2"],
    "subtopics": {
        "Topic 1": ["Subtopic 1", "Subtopic 2"],
        "Topic 2": ["Subtopic 1", "Subtopic 2"]
    },
    "priority_order": ["Topic 1", "Topic 2"],
    "specialized_areas": ["Area 1", "Area 2"]
}"""

# Export all prompts and utilities
__all__ = [
    "STRUCTURED_OUTPUT_VALIDATION",
    "QUERY_ANALYSIS_PROMPT",
    "EXTRACTION_PROMPTS",
    "SYNTHESIS_PROMPT",
    "VALIDATION_PROMPT",
    "REPORT_TEMPLATE",
    "CLARIFICATION_PROMPT",
    "SEARCH_QUALITY_THRESHOLDS",
    "get_extraction_prompt",
    "ADDITIONAL_TOPICS_PROMPT",
    "RESEARCH_BASE_PROMPT",
    "RESEARCH_AGENT_PROMPT",
    "TOPICS_PROMPT"
]
</file>

<file path="src/react_agent/prompts/synthesis.py">
"""Enhanced synthesis and output module.

This module provides improved synthesis and output formatting capabilities
to create more thorough, insightful and verbose research reports with 
better statistics integration and citation handling.
"""

from typing import Dict, List, Any, Optional, Union, Tuple
import json
from datetime import datetime, timezone
import re

from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableConfig

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight, error_highlight
from react_agent.utils.llm import call_model_json
from react_agent.utils.extraction import extract_statistics
from react_agent.utils.defaults import get_default_extraction_result

# Initialize logger
logger = get_logger(__name__)

# Enhanced synthesis prompt template
ENHANCED_SYNTHESIS_PROMPT = """Create a comprehensive synthesis of research findings for: {query}

REQUIREMENTS:
1. Structure your synthesis with these EXACT sections:
   - Executive Summary: Concise overview of key findings with critical statistics
   - Domain Overview: Essential context and background with industry statistics
   - Market Dynamics: Size, growth, trends, competition, procurement patterns with market statistics
   - Provider Landscape: Key vendors, manufacturers, distributors, and their positioning with market share data
   - Technical Requirements: Specifications, standards, and procurement requirements with technical statistics
   - Regulatory Landscape: Compliance, legal requirements, and procurement regulations with compliance data
   - Implementation Factors: Resources, process, challenges, and procurement considerations with implementation statistics
   - Cost Analysis: Pricing, ROI, financial factors, volume discounts, and contract terms with financial metrics
   - Best Practices: Recommended approaches for procurement and sourcing with adoption statistics
   - Contract & Procurement Strategy: Contract terms, negotiation strategies, and procurement processes with benchmarks

2. For EACH section:
   - Synthesize insights from multiple sources when available
   - Prioritize STATISTICAL information and NUMERICAL data
   - Include specific PERCENTAGES, AMOUNTS, and METRICS
   - Address inconsistencies and note knowledge gaps
   - Include SPECIFIC facts with proper citations
   - Present balanced perspectives where there are differences
   - Prioritize VERIFIED information from authoritative sources
   - If information is missing for any section, explicitly note this

3. For "Confidence Assessment":
   - Evaluate information completeness for each section
   - Identify potential biases in the sources
   - Note limitations in the research
   - Assign justified confidence scores by section
   - Provide specific reasons for confidence ratings

AVAILABLE RESEARCH:
{research_json}

FORMAT RESPONSE AS JSON:
{{
  "synthesis": {{
    "executive_summary": {{ "content": "", "citations": [], "statistics": [] }},
    "domain_overview": {{ "content": "", "citations": [], "statistics": [] }},
    "market_dynamics": {{ "content": "", "citations": [], "statistics": [] }},
    "provider_landscape": {{ "content": "", "citations": [], "statistics": [] }},
    "technical_requirements": {{ "content": "", "citations": [], "statistics": [] }},
    "regulatory_landscape": {{ "content": "", "citations": [], "statistics": [] }},
    "implementation_factors": {{ "content": "", "citations": [], "statistics": [] }},
    "cost_considerations": {{ "content": "", "citations": [], "statistics": [] }},
    "best_practices": {{ "content": "", "citations": [], "statistics": [] }},
    "contract_procurement_strategy": {{ "content": "", "citations": [], "statistics": [] }}
  }},
  "confidence_assessment": {{
    "overall_score": 0.0-1.0,
    "section_scores": {{
      "executive_summary": 0.0-1.0,
      "domain_overview": 0.0-1.0,
      "market_dynamics": 0.0-1.0,
      "provider_landscape": 0.0-1.0,
      "technical_requirements": 0.0-1.0,
      "regulatory_landscape": 0.0-1.0,
      "implementation_factors": 0.0-1.0,
      "cost_considerations": 0.0-1.0,
      "best_practices": 0.0-1.0,
      "contract_procurement_strategy": 0.0-1.0
    }},
    "limitations": [],
    "knowledge_gaps": [],
    "confidence_justifications": {{
      "executive_summary": "",
      "domain_overview": "",
      "market_dynamics": "",
      "provider_landscape": "",
      "technical_requirements": "",
      "regulatory_landscape": "",
      "implementation_factors": "",
      "cost_considerations": "",
      "best_practices": "",
      "contract_procurement_strategy": ""
    }}
  }}
}}

REMEMBER:
- Prioritize statistical data and numerical findings
- Include specific numbers, percentages, and metrics
- Emphasize recent studies, surveys, and market reports
- Highlight data from industry-leading sources
- Only include claims that are supported by the research
- Use clear, concise language focused on business impact
- Highlight conflicting information when present
- Pay special attention to procurement and sourcing-related insights
"""

# Enhanced report template with better statistics and citations
ENHANCED_REPORT_TEMPLATE = """
# {title}

## Executive Summary
{executive_summary}

## Key Findings & Statistics
{key_statistics}

## Domain Overview
{domain_overview}

## Market Analysis
### Market Size & Growth
{market_size}

### Competitive Landscape
{competitive_landscape}

### Trends & Developments
{market_trends}

## Provider Landscape
### Key Vendors
{key_vendors}

### Vendor Comparison
{vendor_comparison}

## Technical Requirements
{technical_requirements}

## Regulatory Considerations
{regulatory_landscape}

## Implementation Strategy
{implementation_factors}

## Cost Analysis
### Cost Structure
{cost_structure}

### Pricing Models
{pricing_models}

### ROI Considerations
{roi_considerations}

## Best Practices
{best_practices}

## Procurement Strategy
{procurement_strategy}

## Recommendations
{recommendations}

## Sources & Citations
{sources}

---
**Research Confidence:** {confidence_score}/1.0  
**Date Generated:** {generation_date}  
{confidence_notes}
"""

def extract_all_statistics(synthesis: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Extract all statistics from synthesis results."""
    all_stats = []
    
    for section_name, section_data in synthesis.items():
        if isinstance(section_data, dict) and "statistics" in section_data:
            stats = section_data.get("statistics", [])
            if stats and isinstance(stats, list):
                for stat in stats:
                    if isinstance(stat, dict):
                        # Add section name to statistic
                        stat["section"] = section_name
                        all_stats.append(stat)
    
    # Sort by quality score if available
    sorted_stats = sorted(
        all_stats,
        key=lambda x: x.get("quality_score", 0),
        reverse=True
    )
    
    return sorted_stats

def format_citation(citation: Dict[str, Any]) -> str:
    """Format a citation for inclusion in the report."""
    if not citation or not isinstance(citation, dict):
        return ""
        
    title = citation.get("title", "")
    source = citation.get("source", "")
    url = citation.get("url", "")
    date = citation.get("date", "")
    
    if title and source:
        return f"{title} ({source}{', ' + date if date else ''})"
    elif title:
        return title
    elif source:
        return source
    elif url:
        return url
    else:
        return "Unnamed source"

def format_statistic(stat: Dict[str, Any]) -> str:
    """Format a statistic for inclusion in the report."""
    if not stat or not isinstance(stat, dict):
        return ""
        
    text = stat.get("text", "")
    citation = ""
    
    # Add citation if available
    citations = stat.get("citations", [])
    if citations and isinstance(citations, list) and len(citations) > 0:
        first_citation = citations[0]
        if isinstance(first_citation, dict):
            source = first_citation.get("source", "")
            if source:
                citation = f" ({source})"
    
    return f"{text}{citation}"

def highlight_statistics_in_content(content: str, statistics: List[Dict[str, Any]]) -> str:
    """Highlight statistics in content with bold formatting."""
    if not content or not statistics:
        return content
        
    highlighted_content = content
    
    for stat in statistics:
        if isinstance(stat, dict) and "text" in stat:
            text = stat.get("text", "")
            if text and text in highlighted_content:
                # Highlight the statistic with bold formatting
                highlighted_content = highlighted_content.replace(text, f"**{text}**")
    
    return highlighted_content

async def synthesize_research(
    state: Dict[str, Any],
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Synthesize all research data into a comprehensive result with enhanced statistics focus."""
    info_highlight("Synthesizing research results with enhanced statistics focus")
    
    categories = state["categories"]
    original_query = state["original_query"]
    
    # Prepare research data for synthesis
    research_data = {}
    for category, category_state in categories.items():
        # Extract statistics from facts
        statistics = []
        for fact in category_state.get("extracted_facts", []):
            if "statistics" in fact:
                statistics.extend(fact["statistics"])
            elif "source_text" in fact:
                # Extract statistics from source text
                extracted_stats = extract_statistics(fact["source_text"])
                statistics.extend(extracted_stats)
        
        research_data[category] = {
            "facts": category_state["extracted_facts"],
            "sources": category_state["sources"],
            "quality_score": category_state["quality_score"],
            "statistics": statistics  # Add extracted statistics
        }
    
    # Generate prompt
    synthesis_prompt = ENHANCED_SYNTHESIS_PROMPT.format(
        query=original_query,
        research_json=json.dumps(research_data, indent=2)
    )
    
    try:
        synthesis_result = await call_model_json(
            messages=[{"role": "human", "content": synthesis_prompt}],
            config=config
        )
        
        # Extract key statistics for reference
        synthesis_sections = synthesis_result.get("synthesis", {})
        all_statistics = extract_all_statistics(synthesis_sections)
        synthesis_result["key_statistics"] = all_statistics[:10]  # Top 10 statistics
        
        overall_score = synthesis_result.get("confidence_assessment", {}).get("overall_score", 0.0)
        info_highlight(f"Research synthesis complete with confidence score: {overall_score:.2f}")
        
        return {
            "synthesis": synthesis_result,
            "status": "synthesized"
        }
    except Exception as e:
        error_highlight(f"Error in research synthesis: {str(e)}")
        return {"error": {"message": f"Error in research synthesis: {str(e)}", "phase": "synthesis"}}

async def prepare_enhanced_response(
    state: Dict[str, Any],
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Prepare an enhanced final response with detailed statistics and citations."""
    info_highlight("Preparing enhanced final response")
    
    synthesis = state.get("synthesis", {})
    validation = state.get("validation_result", {})
    original_query = state["original_query"]
    
    # Get synthesis content
    synthesis_content = synthesis.get("synthesis", {}) if synthesis else {}
    confidence = synthesis.get("confidence_assessment", {}) if synthesis else {}
    
    # Get all statistics
    all_statistics = synthesis.get("key_statistics", []) if synthesis else []
    
    # Format the title with proper capitalization
    title = "Research Results: " + original_query.capitalize()
    
    # Format each section with highlighted statistics
    sections = {}
    for section_name, section_data in synthesis_content.items():
        if isinstance(section_data, dict) and "content" in section_data:
            content = section_data.get("content", "")
            statistics = section_data.get("statistics", [])
            
            # Highlight statistics in content
            highlighted_content = highlight_statistics_in_content(content, statistics)
            
            # Add citations
            citations = section_data.get("citations", [])
            if citations and isinstance(citations, list) and len(citations) > 0:
                citation_text = "\n\n**Sources:** " + ", ".join(
                    format_citation(citation) for citation in citations if citation
                )
                sections[section_name] = highlighted_content + citation_text
            else:
                sections[section_name] = highlighted_content
    
    # Format key statistics section
    key_stats_formatted = []
    for stat in all_statistics:
        formatted_stat = format_statistic(stat)
        if formatted_stat:
            key_stats_formatted.append(f"- {formatted_stat}")
    
    key_statistics_section = "\n".join(key_stats_formatted) if key_stats_formatted else "No key statistics available."
    
    # Format sources section
    sources_set = set()
    for section_data in synthesis_content.values():
        if isinstance(section_data, dict) and "citations" in section_data:
            citations = section_data.get("citations", [])
            for citation in citations:
                if isinstance(citation, dict):
                    formatted = format_citation(citation)
                    if formatted:
                        sources_set.add(formatted)
    
    sources_list = sorted(list(sources_set))
    sources_section = "\n".join(f"- {source}" for source in sources_list) if sources_list else "No sources available."
    
    # Get confidence information
    confidence_score = confidence.get("overall_score", 0.0)
    limitations = confidence.get("limitations", [])
    knowledge_gaps = confidence.get("knowledge_gaps", [])
    
    # Format confidence notes
    confidence_notes = []
    if limitations:
        confidence_notes.append("**Limitations:** " + ", ".join(limitations))
    if knowledge_gaps:
        confidence_notes.append("**Knowledge Gaps:** " + ", ".join(knowledge_gaps))
    
    confidence_notes_text = "\n".join(confidence_notes)
    
    # Generate recommendations based on synthesis
    recommendations = await generate_recommendations(synthesis_content, original_query, config)
    
    # Fill in the template
    report_content = ENHANCED_REPORT_TEMPLATE.format(
        title=title,
        executive_summary=sections.get("executive_summary", "No executive summary available."),
        key_statistics=key_statistics_section,
        domain_overview=sections.get("domain_overview", "No domain overview available."),
        market_size=sections.get("market_dynamics", "No market dynamics information available."),
        competitive_landscape=sections.get("provider_landscape", "No provider landscape information available."),
        market_trends=sections.get("market_dynamics", "No market trends information available."),
        key_vendors=sections.get("provider_landscape", "No vendor information available."),
        vendor_comparison=sections.get("provider_landscape", "No vendor comparison available."),
        technical_requirements=sections.get("technical_requirements", "No technical requirements information available."),
        regulatory_landscape=sections.get("regulatory_landscape", "No regulatory information available."),
        implementation_factors=sections.get("implementation_factors", "No implementation information available."),
        cost_structure=sections.get("cost_considerations", "No cost structure information available."),
        pricing_models=sections.get("cost_considerations", "No pricing models information available."),
        roi_considerations=sections.get("cost_considerations", "No ROI considerations available."),
        best_practices=sections.get("best_practices", "No best practices information available."),
        procurement_strategy=sections.get("contract_procurement_strategy", "No procurement strategy information available."),
        recommendations=recommendations,
        sources=sources_section,
        confidence_score=f"{confidence_score:.2f}",
        generation_date=datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC"),
        confidence_notes=confidence_notes_text
    )
    
    # Create the response message
    response_message = AIMessage(content=report_content)
    
    return {
        "messages": [response_message],
        "status": "complete",
        "complete": True
    }

async def generate_recommendations(
    synthesis_content: Dict[str, Any],
    query: str,
    config: Optional[RunnableConfig] = None
) -> str:
    """Generate recommendations based on synthesis content."""
    info_highlight("Generating recommendations based on synthesis")
    
    if not synthesis_content:
        return "No recommendations available due to insufficient data."
    
    # Create a prompt for recommendations
    recommendations_prompt = f"""
    Based on the research synthesis for "{query}", generate 5-7 specific, actionable recommendations.
    Each recommendation should:
    1. Be specific and actionable
    2. Reference relevant statistics or findings when available
    3. Address a key need or gap identified in the research
    4. Be practical and implementable
    5. Include expected benefits or outcomes
    
    Synthesis data:
    {json.dumps(synthesis_content, indent=2)}
    
    FORMAT:
    Return a markdown list of recommendations with brief explanations.
    """
    
    try:
        response = await call_model_json(
            messages=[{"role": "human", "content": recommendations_prompt}],
            config=config
        )
        
        if isinstance(response, dict) and "recommendations" in response:
            return response["recommendations"]
        elif isinstance(response, dict) and "content" in response:
            return response["content"]
        elif isinstance(response, str):
            return response
        else:
            # Default format if response structure is unexpected
            return "Recommendations could not be generated due to unexpected response format."
    except Exception as e:
        error_highlight(f"Error generating recommendations: {str(e)}")
        return "Recommendations could not be generated due to an error."

__all__ = [
    "chunk_text",
    "preprocess_content",
    "estimate_tokens",
    "should_skip_content",
    "merge_chunk_results",
    "validate_content",
    "detect_content_type"
]
</file>

<file path="src/react_agent/prompts/templates.py">
"""Main prompt templates.

This module provides functionality for main prompt templates
in the agent framework.
"""

from typing import Final

# Common validation template used across multiple prompts
STRUCTURED_OUTPUT_VALIDATION: Final[str] = """CRITICAL: All responses MUST:
1. Be valid JSON only - no additional text or comments
2. Follow the exact schema provided
3. Never return empty or null values
4. Include all required fields
5. Use proper data types (strings, numbers, arrays)
6. Maintain proper JSON syntax
7. Include citations for all data points
8. Pass JSON schema validation

Any response that fails these requirements will be rejected."""

# Validation requirements component - reusable across prompts
VALIDATION_REQUIREMENTS: Final[str] = """VALIDATION REQUIREMENTS:
1. Structural Validation
   - Verify JSON syntax is valid
   - Check all required fields are present
   - Ensure no empty or null values
   - Validate data types match schema
   - Check array elements follow required format

2. Citation Validation
   - Verify each citation URL exists and is accessible
   - Ensure at least 2 citations per analysis section
   - Validate source credibility and relevance
   - Cross-reference data points across sources"""

# Main prompt for the primary agent
MAIN_PROMPT: Final[
    str
] = """You are conducting web research for RFP category analysis and market basket development.
Your goal is to produce a structured JSON response following this exact schema:
{
    "rfp_analysis": {
        "analysis": {
            "porters_5_forces": {
                "competitive_rivalry": "",
                "threat_of_new_entrants": "",
                "threat_of_substitutes": "",
                "bargaining_power_buyers": "",
                "bargaining_power_suppliers": ""
            },
            "swot": {
                "strengths": [],
                "weaknesses": [],
                "opportunities": [],
                "threats": []
            },
            "recent_breakthroughs_and_disruptors": "",
            "cost_trends_and_projections": "",
            "typical_contract_clauses_and_pricing_nuances": "",
            "competitive_landscape": ""
        },
        "market_basket": [
            {
                "manufacturer_or_distributor": "",
                "item_number": "",
                "item_description": "",
                "uom": "",
                "estimated_qty_per_uom": 0.0,
                "unit_cost": 0.0
            }
        ]
    },
    "confidence_score": 0.0
}
Category to analyze: {topic}
IMPORTANT INSTRUCTIONS:
1. Your response must be ONLY valid JSON - no additional text, comments or explanations
2. Every field must be populated - no empty strings or null values
3. If you cannot structure some information, include it under a "raw_findings" key
4. Do not truncate or leave responses incomplete
5. Ensure all JSON syntax is valid (quotes, commas, brackets)
Available tools:
1. Search: Query search engines for industry and market information
2. ScrapeWebsite: Extract structured data from industry sources
3. SummarizeResearch: Generate AI-powered summaries for complex topics
4. SearchNews: Find recent news articles and industry developments
5. Info: Compile and format final findings
"""

# Tool descriptions
WEB_SEARCH_DESC: Final[str] = """Search the web for information about a topic.
Input should be a search query string.
Returns up to 3 search results with titles, URLs, and snippets."""

SCRAPE_DESC: Final[str] = """Scrape content from a website URL.
Input should be a valid URL.
Returns the scraped content and metadata."""

# New tool descriptions for Brave Summarizer and News APIs
SUMMARIZER_DESC: Final[
    str
] = """Generate an AI-powered summary of search results for a topic.
Input should be a search query string.
Returns a comprehensive summary along with key topics and 5 source articles."""

NEWS_SEARCH_DESC: Final[str] = """Search for recent news articles related to a topic.
Input should be a search query string.
Returns 5 news articles with titles, URLs, descriptions, and sources."""

# Tool instructions for reuse across agent nodes
TOOL_INSTRUCTIONS: Final[str] = """
IMPORTANT:
1. Use the search_web tool to find relevant information (returns 3 results per query)
2. Use the search_news tool for recent developments and news (returns 5 results per query)
3. Use the scrape_website tool to extract detailed content from websites
4. Use the summarize_research tool to get AI-powered summaries of complex topics (returns 5 sources per query)
5. Always include proper citations for all information
6. Follow all research requirements in the prompt
"""

# Evaluation prompt template for content evaluation
EVALUATION_PROMPT_TEMPLATE: Final[
    str
] = """You are an evaluation system that assesses the quality of AI responses.
Review the following response and provide scores and feedback.

Task description: {task_description}

Response to evaluate:
{response}

Please evaluate this response on these criteria: {criteria}.
For each criterion, provide a score from 0.0 to 1.0 and brief feedback."""

# Reflection prompt templates
FEEDBACK_PROMPT_TEMPLATE: Final[str] = """You are an AI improvement coach.
Based on the critique and evaluation of a previous response, generate actionable feedback 
to help improve future responses.

Original task: {task}

Previous response: {response}

Critique: {critique}

Evaluation scores: {scores}

Generate specific, actionable feedback with examples of how to improve."""

CRITIQUE_PROMPT_TEMPLATE: Final[str] = """You are an expert evaluator providing critique.
Review the following response and provide detailed feedback.

Task: {task}
Response: {response}
Evaluation criteria: {criteria}

Provide specific critique points and actionable suggestions for improvement."""

ANALOGICAL_REASONING_PROMPT: Final[str] = """You are an expert at improving solutions through analogical reasoning.

Current task: {task}
Current response: {response}
Similar examples:
{examples}

Based on these examples, suggest improvements to the current response."""

COUNTERFACTUAL_PROMPT: Final[str] = """You are an expert at generating counterfactual improvements.
Consider 'what if' scenarios that could lead to better outcomes.

Current response: {response}
Areas for improvement: {areas}

Generate counterfactual scenarios and corresponding improvements."""

METACOGNITION_PROMPT: Final[str] = """You are an expert at analyzing thinking processes and cognitive patterns.
Identify patterns, biases, and potential improvements in the reasoning process.

Conversation history: {history}
Current scores: {scores}
Improvement areas: {areas}

Analyze the thinking process and suggest meta-level improvements."""

# Detailed feedback prompt templates
DETAILED_FEEDBACK_PROMPT: Final[str] = """You are an AI improvement coach providing detailed feedback.
Review the following response and generate specific, actionable feedback.

CONTEXT:
Original task: {task}
Previous response: {response}
Critique points: {critique}
Current scores: {scores}

REQUIREMENTS:
1. Provide specific examples of what could be improved
2. Suggest concrete implementation steps
3. Reference similar successful approaches
4. Highlight both strengths and areas for improvement
5. Maintain constructive and actionable tone

Generate detailed, actionable feedback that addresses:
1. Content quality and accuracy
2. Structure and organization
3. Completeness and depth
4. Implementation and practicality
5. Overall effectiveness"""

REFLECTION_FEEDBACK_PROMPT: Final[str] = """You are an AI reflection coach.
Help improve responses through structured reflection and feedback.

CONTEXT:
Task description: {task}
Current response: {response}
Evaluation scores: {scores}
Areas for improvement: {areas}

REFLECTION POINTS:
1. What worked well in the current approach?
2. What could have been done differently?
3. How can we apply lessons from similar successful cases?
4. What specific steps would lead to better outcomes?

Provide actionable feedback focusing on:
1. Strategic improvements
2. Tactical adjustments
3. Process refinements
4. Quality enhancements"""

STRUCTURED_SYSTEM_PROMPT: Final[str] = """You are a helpful assistant that can answer questions and help with tasks."""

SYSTEM_PROMPT: Final[str] = """You are a helpful assistant that can answer questions and help with tasks."""
</file>

<file path="src/react_agent/prompts/validation.py">
"""Validation-specific prompts.

This module provides functionality for validation-specific prompts
in the agent framework.
"""

from typing import Final

# Validation base prompt
VALIDATION_BASE_PROMPT: Final[
    str
] = """You are a Validation Agent for RFP market analysis.
Your goal is to prevent hallucinations and ensure data quality.

{VALIDATION_REQUIREMENTS}

3. Content Validation
   - Verify all required fields are populated
   - Check for data consistency across sections
   - Validate numerical data and calculations
   - Ensure analysis conclusions are supported by data

4. Market Basket Validation
   - Verify product information accuracy
   - Cross-check pricing against multiple sources
   - Validate manufacturer/distributor details
   - Ensure proper unit of measure conversions

5. Analysis Quality
   - Verify PESTEL factors are comprehensive
   - Check GAP analysis identifies clear needs
   - Validate cost-benefit calculations
   - Review risk assessment completeness
   - Cross-check TCO components
   - Verify vendor analysis objectivity
   - Check benchmarking methodology
   - Validate stakeholder identification
   - Ensure compliance requirements are current
   - Verify business impact assessments

CONFIDENCE SCORING:
- Start with base score of 0.4
- Add 0.1 for each validated section with 2+ citations
- Add 0.1 for each verified market basket item
- Add 0.1 for comprehensive analysis coverage
- Subtract 0.1 for each validation failure
- Reject if final score < 0.98

RESPONSE_FORMAT:
{
    "validation_results": {
        "is_valid": false,
        "errors": [],
        "warnings": [],
        "confidence_score": 0.0,
        "section_scores": {
            "structural": 0.0,
            "citations": 0.0,
            "content": 0.0,
            "market_basket": 0.0
        },
        "failed_validations": [],
        "required_fixes": []
    }
}

Current state: {state}
"""

# Validation agent prompt with structured output validation
VALIDATION_AGENT_PROMPT: Final[str] = VALIDATION_BASE_PROMPT.replace(
    "Your goal is to prevent hallucinations and ensure data quality.\n",
    "Your goal is to prevent hallucinations and ensure data quality.\n\n{STRUCTURED_OUTPUT_VALIDATION}\n",
)

# Prompt for generating validation criteria
VALIDATION_CRITERIA_PROMPT: Final[str] = """
Content Type: {content_type}
Generate appropriate validation criteria for content of this type.
The criteria should be comprehensive and tailored to the specific content type.
For example:
- For research content: factual accuracy, source credibility, logical consistency
- For analysis content: methodological soundness, statistical validity, interpretative accuracy
- For code: functional correctness, efficiency, security, readability

Format your response as a JSON object with these fields:
- primary_criteria: List of primary validation criteria (string[])
- secondary_criteria: List of secondary validation criteria (string[])
- critical_requirements: List of must-have elements (string[])
- disqualifying_factors: List of automatic disqualifiers (string[])
- scoring_weights: Dictionary mapping criteria to weights (0.0 to 1.0)
"""

# Prompt for fact checking
FACT_CHECK_CLAIMS_PROMPT: Final[str] = """
Content Type: {content_type}
Analyze the following content for factual accuracy of claims:
{content}

1. Identify claims that are factual, opinion-based, unclear, or contradictory.
2. Provide source citations for each claim.
3. Evaluate the credibility of sources.
4. Verify the accuracy of each claim.
5. Determine if the content as a whole is factually accurate.
6. Identify any potential biases or conflicts of interest.
7. Note any areas where more research is needed.

Respond in JSON format with these fields:
- factually_accurate_claims: string[] (list of factual claims)
- opinion_based_claims: string[] (list of opinion-based claims)
- unclear_claims: string[] (list of unclear claims)
- source_citations: string[] (list of source URLs for each claim)
- source_credibility: string[] (list of source credibility scores)
- verification_results: string[] (list of verification results)
- overall_accuracy: number from 0-10
- potential_biases: string[] (list of potential biases)
- areas_for_future_research: string[] (list of areas for future research)
- issues: string[] (summarizing all critical issues)
"""

# Prompt for validating individual claims
VALIDATE_CLAIM_PROMPT: Final[str] = """
Fact check the following claim:
CLAIM: {claim}

Respond in JSON format with these fields:
- accuracy: number from 0-10
- confidence: number from 0-10
- issues: string[] (empty if no issues)
- verification_notes: string
"""

# Prompt for logic validation
LOGIC_VALIDATION_PROMPT: Final[str] = """
Content Type: {content_type}
Validate the logical consistency, reasoning quality, and argument structure of the following content:
{content}

Analyze for:
1. Valid argument structure (premises, conclusions)
2. Logical fallacies (e.g., circular reasoning, false cause)
3. Consistency between claims
4. Quality of evidence and reasoning
5. Appropriate conclusions

Respond in JSON format with these fields:
- logical_structure_score: number from 0-10
- fallacies_found: string[] (empty if none)
- consistency_issues: string[] (empty if none)
- reasoning_quality: number from 0-10
- conclusion_validity: number from 0-10
- overall_score: number from 0-10
- issues: string[] (summarizing all critical issues)
"""

# Prompt for consistency checking
CONSISTENCY_CHECK_PROMPT: Final[str] = """
Content Type: {content_type}
Check the internal consistency and coherence of the following content:
{content}

Analyze for:
1. Consistency between different sections
2. Coherence of narrative or explanation
3. Presence of contradictions
4. Logical flow and structure
5. Completeness (no missing pieces in the reasoning)

Respond in JSON format with these fields:
- section_consistency: number from 0-10
- coherence_score: number from 0-10
- contradictions: string[] (empty if none)
- flow_quality: number from 0-10
- completeness: number from 0-10
- overall_score: number from 0-10
- issues: string[] (summarizing all critical issues)
- needs_human_review: boolean (true if human review is recommended)
"""

# Prompt for human feedback request
HUMAN_FEEDBACK_PROMPT: Final[str] = """
Content Type: {content_type}
Based on automated validation, the following issues were identified:
{issues}

Generate 3-5 specific questions for human reviewers to address these issues.
Questions should be clear, focused, and help improve the quality of the content.
Additionally, suggest specific sections or aspects that need human attention.

Format your response as JSON with these fields:
- questions: string[] (list of questions)
- focus_areas: string[] (specific aspects needing review)
- content_summary: string (brief summary of the content)
"""
</file>

<file path="src/react_agent/tools/jina.py">
"""Enhanced Jina AI Search Integration.

This module provides a more robust integration with Jina AI's search API
with improved error handling, result validation, and search strategies.
"""


from typing import Dict, List, Optional, Any, Union, cast, Tuple, Literal
import json
import time
import aiohttp
import asyncio
import contextlib
from urllib.parse import urljoin, quote, urlparse
import random
from datetime import datetime
import re
import os

from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import InjectedToolArg
from typing_extensions import Annotated
from pydantic import BaseModel, Field
from langchain.tools import BaseTool
from langchain_core.tools import ToolException

from react_agent.configuration import Configuration
from react_agent.utils.logging import get_logger, log_dict, info_highlight, warning_highlight, error_highlight
from react_agent.utils.validations import is_valid_url
from react_agent.prompts.query import optimize_query, detect_vertical, expand_acronyms
from react_agent.utils.extraction import safe_json_parse

# Initialize logger
logger = get_logger(__name__)

# Define search types for specialized search strategies
SearchType = Literal["general", "authoritative", "recent", "comprehensive", "technical"]

# Add at module level after imports
_query_cache: Dict[str, Tuple[List[Document], datetime]] = {}

class RetryConfig(BaseModel):
    """Configuration for retry behavior."""
    max_retries: int = Field(default=3, description="Maximum number of retries")
    base_delay: float = Field(default=1.0, description="Base delay between retries in seconds")
    max_delay: float = Field(default=10.0, description="Maximum delay between retries in seconds")
    
    def get_delay(self, attempt: int) -> float:
        """Calculate delay with exponential backoff."""
        delay = min(self.max_delay, self.base_delay * (2 ** (attempt - 1)))
        return delay

class SearchParams(BaseModel):
    """Parameters for search operations."""
    query: str = Field(..., description="Search query")
    search_type: SearchType = Field(default="general", description="Type of search to perform")
    max_results: Optional[int] = Field(default=None, description="Maximum number of results to return")
    min_quality_score: Optional[float] = Field(default=0.5, description="Minimum quality score for results")
    recency_days: Optional[int] = Field(default=None, description="Maximum age of results in days")
    domains: Optional[List[str]] = Field(default=None, description="List of domains to search")
    category: Optional[str] = Field(default=None, description="Category to search in")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for API request."""
        result = {
            "q": self.query,
            "limit": self.max_results or 10,
            "min_score": self.min_quality_score or 0.5
        }
        
        if self.recency_days:
            result["recency_days"] = self.recency_days
            
        if self.domains:
            result["domains"] = ",".join(self.domains)
            
        if self.category:
            result["category"] = self.category
            
        return result

class JinaSearchClient:
    """Enhanced client for Jina AI search with retry and validation."""
    
    def __init__(
        self,
        api_key: str,
        base_url: Optional[str] = None,
        retry_config: Optional[RetryConfig] = None,
    ):
        """Initialize Jina search client.
        
        Args:
            api_key: Jina AI API key
            base_url: Optional base URL for self-hosted instances
            retry_config: Configuration for retry behavior
        """
        self.api_key = api_key
        self.base_url = base_url.rstrip('/') if base_url else "https://s.jina.ai"
        self.retry_config = retry_config or RetryConfig()
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        """Create aiohttp session."""
        self.session = aiohttp.ClientSession(
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Close aiohttp session."""
        if self.session:
            await self.session.close()
            self.session = None

    def _get_endpoint(self, endpoint: str) -> str:
        """Get endpoint URL."""
        base = self.base_url.rstrip('/')
        if not endpoint.startswith('/'):
            endpoint = f'/{endpoint}'
        return f"{base}{endpoint}"

    async def _make_request_with_retry(
        self,
        method: str,
        endpoint: str,
        params: Optional[Dict[str, Any]] = None,
        json_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Make HTTP request to Jina API with retry logic and better error handling."""
        if not self.session:
            raise ValueError("Session not initialized")

        last_exception = None
        for attempt in range(1, self.retry_config.max_retries + 1):
            try:
                url = self._get_endpoint(endpoint)
                info_highlight(f"Making request to: {url} with params: {params}")
                
                async with self.session.request(
                    method=method,
                    url=url,
                    params=params,
                    json=json_data
                ) as response:
                    # Check for no results error (422)
                    if response.status == 422:
                        error_text = await response.text()
                        if "No search results available" in error_text:
                            warning_highlight("No search results available for this query")
                            return {"results": []}  # Return empty results rather than raising an error
                    
                    # Handle other errors
                    if response.status != 200:
                        error_text = await response.text()
                        error_highlight(f"Request failed with status {response.status}: {error_text}")
                        raise aiohttp.ClientError(f"Request failed with status {response.status}: {error_text}")
                    
                    return await response.json()
            except Exception as e:
                last_exception = e
                if attempt < self.retry_config.max_retries:
                    delay = self.retry_config.get_delay(attempt)
                    warning_highlight(
                        f"Request failed (attempt {attempt}/{self.retry_config.max_retries}): {str(last_exception)}. Retrying in {delay:.2f}s"
                    )
                    await asyncio.sleep(delay)
                else:
                    error_highlight(
                        f"Request failed after {self.retry_config.max_retries} attempts: {str(last_exception)}"
                    )
                    raise

        # This should never be reached as the last failure should raise
        assert last_exception is not None
        raise last_exception

    async def search(
        self,
        params: SearchParams
    ) -> List[Document]:
        """Search with improved parameters and result validation.
        
        Args:
            params: Search parameters
            
        Returns:
            List of documents from search results
        """
        request_params = params.to_dict()
        info_highlight(f"Executing {params.search_type} search with query: {params.query}")
        
        try:
            # Use the search endpoint
            data = await self._make_request_with_retry(
                method="GET",
                endpoint="/search",  # Use the search endpoint
                params=request_params
            )
            
            results = self._parse_search_results(data)
            info_highlight(f"Retrieved {len(results)} search results")
            
            # If no results, try simplified query
            if not results:
                # Extract main keywords (first 3 words or up to 30 chars)
                simplified_query = " ".join(params.query.split()[:3])
                if len(simplified_query) > 30:
                    simplified_query = simplified_query[:30]
                
                info_highlight(f"No results found, trying simplified query: {simplified_query}")
                request_params["query"] = simplified_query
                
                data = await self._make_request_with_retry(
                    method="GET",
                    endpoint="/search",
                    params=request_params
                )
                
                results = self._parse_search_results(data)
                info_highlight(f"Retrieved {len(results)} results with simplified query")
            
            # Convert results to Documents
            documents = self._convert_to_documents(results)
            
            # Apply quality filtering
            min_score: float = float(params.min_quality_score or 0.5)  # Explicit type conversion
            filtered_docs = self._filter_documents(documents, min_score)
            info_highlight(f"Filtered to {len(filtered_docs)} high-quality results")
            
            return filtered_docs
        except Exception as e:
            error_highlight(f"Search failed: {str(e)}")
            return []

    def _parse_search_results(self, raw_results: Union[str, List[Dict], Dict]) -> List[Dict]:
        """Parse raw results with improved error handling."""
        try:
            # Convert raw_results to string if it's not already
            if isinstance(raw_results, (list, dict)):
                raw_results = json.dumps(raw_results)
            parsed = safe_json_parse(raw_results, "search_results")
            return self._extract_results_list(parsed)
        except Exception as e:
            error_highlight(f"Error parsing search results: {str(e)}")
            return []

    def _extract_results_list(self, data: Union[List[Dict], Dict]) -> List[Dict]:
        """Extract results list from various response formats."""
        try:
            if isinstance(data, list):
                return data
            elif isinstance(data, dict):
                # Try common response formats
                for key in ['results', 'data', 'items', 'hits', 'matches', 'documents', 'response']:
                    if key in data:
                        if isinstance(data[key], list):
                            return data[key]
                        elif isinstance(data[key], dict):
                            # Try to extract from nested structure
                            for nested_key in ['results', 'data', 'items', 'hits', 'matches']:
                                if nested_key in data[key] and isinstance(data[key][nested_key], list):
                                    return data[key][nested_key]

                # If no list found, try to extract single result
                if 'result' in data:
                    return [data['result']]

                # If still no results, try to extract from nested structure
                for value in data.values():
                    if isinstance(value, list):
                        return value
                    elif isinstance(value, dict):
                        if nested_results := self._extract_results_list(value):
                            return nested_results

                # If still no results, return empty list
                return []
            return []
        except Exception as e:
            error_highlight(f"Error extracting results list: {str(e)}")
            return []

    def _extract_content(self, result: Dict) -> Optional[str]:
        """Extract content from result with fallbacks."""
        content_fields = ['snippet', 'content', 'text', 'description', 'summary', 'body', 'raw']
        for field in content_fields:
            if content := result.get(field):
                return content.strip().strip('```json').strip('```')
        return None

    def _build_metadata(self, result: Dict, content: str) -> Dict:
        """Build metadata dictionary from result."""
        field_mapping = {
            'url': ['url', 'link', 'href', 'source_url', 'web_url'],
            'title': ['title', 'name', 'heading', 'subject', 'headline'],
            'source': ['source', 'domain', 'site', 'provider', 'publisher'],
            'published_date': ['published_date', 'date', 'timestamp', 'published', 'created_at', 'publication_date']
        }
        
        metadata = {
            'quality_score': self._calculate_quality_score(result, content),
            'extraction_status': 'success',
            'extraction_timestamp': datetime.now().isoformat(),
            'original_result': result
        }
        
        for field_type, fields in field_mapping.items():
            for field in fields:
                if value := result.get(field):
                    metadata[field_type] = value
                    break
        
        if url := metadata.get('url'):
            try:
                metadata['domain'] = urlparse(url).netloc
            except Exception:
                metadata['domain'] = ""
                
        return metadata

    def _convert_to_documents(self, results: List[Dict]) -> List[Document]:
        """Convert search results to Document objects with improved metadata."""
        documents = []
        for idx, result in enumerate(results, 1):
            if not isinstance(result, dict):
                continue
                
            if not (content := self._extract_content(result)):
                warning_highlight(f"No content found for result {idx}")
                continue
                
            try:
                metadata = self._build_metadata(result, content)
                documents.append(Document(page_content=content, metadata=metadata))
                info_highlight(f"Successfully converted result {idx} to Document")
            except Exception as e:
                warning_highlight(f"Error converting result {idx} to Document: {str(e)}")
                continue
                
        return documents

    def _calculate_quality_score(self, result: Dict[str, Any], content: str) -> float:
        """Calculate quality score for a search result."""
        score = 0.5  # Base score

        # Add points for authoritative domains
        authoritative_domains = [
            '.gov', '.edu', '.org', 'wikipedia.org', 
            'research', 'journal', 'university', 'association'
        ]
        url = result.get('url', '')
        if any(domain in url.lower() for domain in authoritative_domains):
            score += 0.2

        # Add points for content length (substantive content)
        if len(content) > 500:
            score += 0.1

        # Add points for having title/publication date
        if result.get('title'):
            score += 0.05
        if result.get('published_date') or result.get('date'):
            score += 0.05

        if date_field := result.get('published_date', result.get('date', '')):
            with contextlib.suppress(Exception):
                # Try to parse date
                from dateutil import parser
                from datetime import datetime, timezone
                published_date = parser.parse(date_field)
                current_date = datetime.now(timezone.utc)
                days_old = (current_date - published_date).days

                # Fresher content gets higher score
                if days_old < 30:  # Last month
                    score += 0.1
                elif days_old < 180:  # Last 6 months
                    score += 0.05
        return min(1.0, score)  # Cap at 1.0

    def _filter_documents(self, documents: List[Document], min_score: float) -> List[Document]:
        """Filter documents based on quality score."""
        return [
            doc for doc in documents 
            if doc.metadata.get('quality_score', 0) >= min_score
        ]

class JinaSearchTool(BaseTool):
    """Enhanced Jina AI search integration with caching and parallel processing."""
    
    name: str = "jina_search"
    description: str = "Search for information using Jina AI's search engine with enhanced caching and parallel processing"
    
    config: Configuration = Field(default_factory=Configuration)
    _query_cache: Dict[str, Tuple[List[Document], datetime]] = Field(default_factory=dict)
    
    def __init__(self, config: Optional[Configuration] = None):
        """Initialize the Jina search tool.
        
        Args:
            config: Optional configuration for the search tool
        """
        super().__init__()
        if config:
            self.config = config
        if not self.config.jina_api_key:
            raise ValueError("Jina API key is required")
            
    async def _arun(
        self,
        query: str,
        search_type: Optional[SearchType] = None,
        max_results: Optional[int] = None,
        min_quality_score: Optional[float] = None,
        recency_days: Optional[int] = None,
        domains: Optional[List[str]] = None,
        category: Optional[str] = None
    ) -> List[Document]:
        """Execute search with caching and parallel processing.
        
        Args:
            query: Search query
            search_type: Type of search to perform
            max_results: Maximum number of results to return
            min_quality_score: Minimum quality score for results
            recency_days: Maximum age of results in days
            domains: List of domains to search
            category: Category to search in
            
        Returns:
            List of search results as Documents
        """
        # Generate cache key
        cache_key = f"{query}_{search_type}_{max_results}_{min_quality_score}_{recency_days}_{domains}_{category}"
        
        # Check cache with TTL
        if cache_key in self._query_cache:
            cached_results, timestamp = self._query_cache[cache_key]
            if (datetime.now() - timestamp).total_seconds() < 3600:  # 1 hour TTL
                return cached_results
            else:
                del self._query_cache[cache_key]
                
        try:
            # Create search parameters
            params = SearchParams(
                query=query,
                search_type=search_type or "general",
                max_results=max_results or self.config.max_search_results,
                min_quality_score=min_quality_score or 0.5,
                recency_days=recency_days,
                domains=domains,
                category=category
            )
            
            retry_config = RetryConfig(max_retries=3, base_delay=1.0, max_delay=10.0)
            async with JinaSearchClient(
                api_key=str(self.config.jina_api_key),  # Ensure API key is string
                base_url=self.config.jina_url,
                retry_config=retry_config
            ) as client:
                # Create tasks for different search strategies
                search_tasks = []
                
                # Main search task
                search_tasks.append(client.search(params))
                
                # If no category specified, try category-specific search
                if not params.category:
                    # Detect vertical from query
                    vertical = detect_vertical(params.query)
                    if vertical:
                        category_params = SearchParams(
                            query=params.query,
                            search_type=params.search_type,
                            max_results=params.max_results or self.config.max_search_results,
                            min_quality_score=params.min_quality_score or 0.5,
                            recency_days=params.recency_days,
                            domains=params.domains,
                            category=vertical
                        )
                        search_tasks.append(client.search(category_params))
                
                # Execute all search tasks in parallel
                results_list = await asyncio.gather(*search_tasks)
                
                # Merge and deduplicate results
                all_results = []
                seen_urls = set()
                
                for results in results_list:
                    for doc in results:
                        url = doc.metadata.get("url", "")
                        if url not in seen_urls:
                            seen_urls.add(url)
                            all_results.append(doc)
                
                # Sort by quality score
                all_results.sort(
                    key=lambda x: x.metadata.get("quality_score", 0),
                    reverse=True
                )
                
                # Filter by minimum quality score
                min_score = min_quality_score or 0.5
                all_results = [doc for doc in all_results if doc.metadata.get("quality_score", 0) >= min_score]
                
                # Cache results with timestamp
                self._query_cache[cache_key] = (all_results, datetime.now())
                
                return all_results
                
        except Exception as e:
            error_highlight(f"Error in Jina search: {str(e)}")
            raise ToolException(f"Failed to execute Jina search: {str(e)}")
            
    def _run(
        self,
        query: str,
        search_type: Optional[SearchType] = None,
        max_results: Optional[int] = None,
        min_quality_score: Optional[float] = None,
        recency_days: Optional[int] = None,
        domains: Optional[List[str]] = None,
        category: Optional[str] = None
    ) -> List[Document]:
        """Synchronous wrapper for the async search method."""
        return asyncio.run(self._arun(
            query=query,
            search_type=search_type,
            max_results=max_results,
            min_quality_score=min_quality_score,
            recency_days=recency_days,
            domains=domains,
            category=category
        ))

async def search(
    query: str,
    search_type: Optional[SearchType] = None,
    max_results: Optional[int] = None,
    min_quality: Optional[float] = None,
    recency_days: Optional[int] = None,
    domains: Optional[List[str]] = None,
    category: Optional[str] = None,
    *,
    config: Annotated[RunnableConfig, InjectedToolArg]
) -> List[Document]:
    """Enhanced search with multiple strategies, quality filters, and improved caching."""
    configuration = Configuration.from_runnable_config(config)
    if not configuration.jina_api_key:
        error_highlight("Jina API key is required")
        return []

    # Set environment variables for Jina (used by some libraries)
    os.environ["JINA_API_KEY"] = configuration.jina_api_key
    if configuration.jina_url:
        os.environ["JINA_URL"] = configuration.jina_url

    # Use provided params or defaults
    params = SearchParams(
        query=query,
        search_type=search_type or "general",
        max_results=max_results or configuration.max_search_results,
        min_quality_score=min_quality or 0.5,
        recency_days=recency_days,
        domains=domains,
        category=category
    )

    # Generate cache key from search parameters
    cache_key = str(f"{params.query}_{params.search_type}_{params.max_results}")

    # Check cache first with TTL
    if cache_key in _query_cache:
        cached_results, timestamp = _query_cache[cache_key]
        # Check if cache is still valid (24 hour TTL)
        if (datetime.now() - timestamp).total_seconds() < 86400:
            info_highlight("Using cached search results")
            return cached_results
        else:
            info_highlight("Cache expired, performing fresh search")
            del _query_cache[cache_key]

    # Add default domains for educational/authoritative content if not provided
    if not domains and search_type in ["authoritative", None]:
        params.domains = ['.edu', '.gov', '.org']

    try:
        retry_config = RetryConfig(max_retries=3, base_delay=1.0, max_delay=10.0)
        async with JinaSearchClient(
            api_key=configuration.jina_api_key,
            base_url=configuration.jina_url,
            retry_config=retry_config
        ) as client:
            # Create tasks for different search strategies
            search_tasks = []
            
            # Main search task
            search_tasks.append(client.search(params))
            
            # If no category specified, try category-specific search
            if not category:
                # Detect vertical from query
                vertical = detect_vertical(query)
                if vertical:
                    category_params = SearchParams(
                        query=query,
                        search_type=search_type or "general",
                        max_results=max_results or configuration.max_search_results,
                        min_quality_score=min_quality or 0.5,
                        recency_days=recency_days,
                        domains=domains,
                        category=vertical
                    )
                    search_tasks.append(client.search(category_params))
            
            # Execute all search tasks in parallel
            results_list = await asyncio.gather(*search_tasks)
            
            # Merge and deduplicate results
            all_results = []
            seen_urls = set()
            
            for results in results_list:
                for doc in results:
                    url = doc.metadata.get("url", "")
                    if url not in seen_urls:
                        seen_urls.add(url)
                        all_results.append(doc)
            
            # Sort by quality score
            all_results.sort(key=lambda x: x.metadata.get("quality_score", 0), reverse=True)
            
            # Take top results up to max_results
            final_results = all_results[:params.max_results]
            
            # Cache the results with timestamp
            if final_results:
                _query_cache[cache_key] = (final_results, datetime.now())
            
            return final_results
            
    except Exception as e:
        error_highlight(f"Search failed: {str(e)}")
        return []

# Export available tools
TOOLS = [search]
</file>

<file path="src/react_agent/utils/__init__.py">
from react_agent.utils.validations import is_valid_url
from react_agent.utils.logging import (
    get_logger,
    log_dict,
    info_highlight,
    warning_highlight,
    error_highlight,
    log_step
)

__all__ = [
    "is_valid_url",
    "get_logger",
    "log_dict",
    "info_highlight",
    "warning_highlight",
    "error_highlight",
    "log_step"
]
</file>

<file path="src/react_agent/utils/content.py">
"""Content processing utilities for the research agent.

This module provides utilities for processing and validating content,
including chunking, preprocessing, and content type detection.
"""

from typing import List, Dict, Any, Optional, Union, Tuple
import re
from urllib.parse import urlparse
import logging
from datetime import datetime
import json

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight, error_highlight
from react_agent.utils.extraction import safe_json_parse
from react_agent.utils.defaults import ChunkConfig, get_default_extraction_result, get_category_merge_mapping

# Initialize logger
logger = get_logger(__name__)

# Constants
DEFAULT_CHUNK_SIZE: int = 40000
DEFAULT_OVERLAP: int = 5000
MAX_CONTENT_LENGTH: int = 100000
TOKEN_CHAR_RATIO: float = 4.0

# Problematic content patterns
PROBLEMATIC_PATTERNS: List[str] = [
    r'\.pdf$',
    r'\.docx?$',
    r'\.xlsx?$',
    r'\.ppt$',
    r'\.zip$',
    r'\.rar$',
    r'\.exe$',
    r'\.dmg$',
    r'\.iso$',
    r'\.tar$',
    r'\.gz$'
]

# Known problematic sites
PROBLEMATIC_SITES: List[str] = [
    'iaeme.com',
    'scribd.com',
    'slideshare.net',
    'academia.edu'
]

# Cache for chunked content
_chunk_cache: Dict[int, Tuple[List[str], datetime]] = {}
# Cache for preprocessed content
_preprocess_cache: Dict[int, Tuple[str, datetime]] = {}

def chunk_text(
    text: str,
    chunk_size: Optional[int] = None,
    overlap: Optional[int] = None,
    use_large_chunks: bool = False,
    min_chunk_size: int = 100  # Minimum chunk size to avoid too small chunks
) -> List[str]:
    """Split text into overlapping chunks with enhanced robustness and caching.
    
    Args:
        text: Text to chunk
        chunk_size: Size of each chunk (defaults to ChunkConfig values)
        overlap: Overlap between chunks (defaults to ChunkConfig values)
        use_large_chunks: Whether to use large chunk sizes
        min_chunk_size: Minimum size for a chunk to avoid too small chunks
        
    Returns:
        List of text chunks
    """
    if not text or text.isspace():
        return []
    
    # Check cache first with TTL
    cache_key = hash(f"{text}_{chunk_size}_{overlap}_{use_large_chunks}_{min_chunk_size}")
    if cache_key in _chunk_cache:
        cached_chunks, timestamp = _chunk_cache[cache_key]
        # Check if cache is still valid (1 hour TTL)
        if (datetime.now() - timestamp).total_seconds() < 3600:
            return cached_chunks
        else:
            del _chunk_cache[cache_key]
        
    # Use appropriate chunk size and overlap based on configuration
    if use_large_chunks:
        chunk_size = chunk_size or ChunkConfig.LARGE_CHUNK_SIZE
        overlap = overlap or ChunkConfig.LARGE_OVERLAP
    else:
        chunk_size = chunk_size or ChunkConfig.DEFAULT_CHUNK_SIZE
        overlap = overlap or ChunkConfig.DEFAULT_OVERLAP
        
    # Ensure chunk size is positive and overlap is less than chunk size
    chunk_size = max(min_chunk_size, chunk_size)
    overlap = min(chunk_size - 1, max(0, overlap))
    
    # Pre-calculate text length and create list with estimated size
    text_length = len(text)
    estimated_chunks = (text_length // (chunk_size - overlap)) + 1
    chunks: List[str] = []  # Initialize as empty list instead of pre-allocating with None
    
    start = 0
    while start < text_length:
        end = start + chunk_size
        
        if end >= text_length:
            # If the remaining text is too small, append it to the last chunk
            if chunks and len(text[start:]) < min_chunk_size:
                chunks[-1] = chunks[-1] + text[start:]
            else:
                chunks.append(text[start:])
            break
            
        # Find the last space before the chunk end
        last_space = text.rfind(' ', start, end)
        if last_space > start:
            end = last_space
            
        # Ensure we don't create chunks smaller than min_chunk_size
        if end - start < min_chunk_size and chunks:
            # Append to previous chunk instead of creating a new one
            chunks[-1] = chunks[-1] + text[start:end]
        else:
            chunks.append(text[start:end])
            
        start = end - overlap
    
    # Cache the result with timestamp
    _chunk_cache[cache_key] = (chunks, datetime.now())
    return chunks

def preprocess_content(content: str, url: str) -> str:
    """Clean and preprocess content before sending to model with improved performance.
    
    Args:
        content: Content to preprocess
        url: URL of the content
        
    Returns:
        Preprocessed content string
    """
    if not content:
        return ""

    logger.info(f"Preprocessing content from {url}")
    logger.debug(f"Initial content length: {len(content)}")

    # Cache key for preprocessed content
    cache_key = hash(f"{content}_{url}")
    if cache_key in _preprocess_cache:
        cached_content, timestamp = _preprocess_cache[cache_key]
        # Check if cache is still valid (1 hour TTL)
        if (datetime.now() - timestamp).total_seconds() < 3600:
            return cached_content
        else:
            del _preprocess_cache[cache_key]

    # Compile regex patterns once
    boilerplate_patterns = [
        (re.compile(r'Copyright © \d{4}.*?reserved\.', re.IGNORECASE | re.DOTALL), ''),
        (re.compile(r'Terms of Service.*?Privacy Policy', re.IGNORECASE | re.DOTALL), ''),
        (re.compile(r'Please enable JavaScript.*?continue', re.IGNORECASE | re.DOTALL), '')
    ]
    
    # Apply boilerplate removal patterns
    for pattern, replacement in boilerplate_patterns:
        content = pattern.sub(replacement, content)

    # Remove redundant whitespace efficiently
    content = ' '.join(content.split())

    # Site-specific cleaning with compiled pattern
    domain = urlparse(url).netloc.lower()
    if 'iaeme.com' in domain:
        iaeme_pattern = re.compile(r'International Journal.*?Indexing', re.IGNORECASE | re.DOTALL)
        content = iaeme_pattern.sub('', content)

    # Truncate if too long
    if len(content) > MAX_CONTENT_LENGTH:
        warning_highlight(f"Content exceeds {MAX_CONTENT_LENGTH} characters, truncating")
        content = f"{content[:MAX_CONTENT_LENGTH]}..."

    # Cache the result with timestamp
    _preprocess_cache[cache_key] = (content, datetime.now())
    
    logger.debug(f"Final content length: {len(content)}")
    return content

def estimate_tokens(text: str) -> int:
    """Estimate number of tokens in text.
    
    Args:
        text: Text to estimate tokens for
        
    Returns:
        Estimated number of tokens
    """
    return int(len(text) / TOKEN_CHAR_RATIO) if text else 0

def should_skip_content(url: str) -> bool:
    """Check if content should be skipped based on URL patterns.
    
    Args:
        url: URL to check
        
    Returns:
        True if content should be skipped, False otherwise
    """
    if not url:
        return True
        
    url_lower = url.lower()
    
    # Check for problematic file types
    for pattern in PROBLEMATIC_PATTERNS:
        if re.search(pattern, url_lower):
            info_highlight(f"Skipping content with pattern {pattern}: {url}")
            return True
            
    # Check for problematic sites
    domain = urlparse(url).netloc.lower()
    for site in PROBLEMATIC_SITES:
        if site in domain:
            info_highlight(f"Skipping content from problematic site {site}: {url}")
            return True
            
    return False

def merge_chunk_results(
    results: List[Dict[str, Any]], 
    category: str,
    merge_strategy: Optional[Dict[str, str]] = None
) -> Dict[str, Any]:
    """Merge results from multiple chunks into a single result with enhanced merging strategies.
    
    Args:
        results: List of chunk results to merge
        category: Research category being processed
        merge_strategy: Optional custom merge strategy for specific fields
        
    Returns:
        Merged result dictionary
    """
    if not results:
        return get_default_extraction_result(category)

    logger.info(f"Merging {len(results)} chunk results for category {category}")

    # Initialize merged result
    merged: Dict[str, Any] = get_default_extraction_result(category)

    # Get merge mappings for the category or use provided strategy
    merge_mappings = merge_strategy or get_category_merge_mapping(category)

    # Track unique items to avoid duplicates
    seen_items = set()

    # Merge results based on category mappings
    for result in results:
        # Handle the new response format with content field
        if "content" in result:
            try:
                parsed_content = safe_json_parse(result["content"], category)
                result = parsed_content
            except Exception as e:
                error_highlight(f"Error parsing content in merge_chunk_results: {str(e)}")
                continue

        for field, operation in merge_mappings.items():
            if field in result:
                if operation == "extend":
                    if field not in merged:
                        merged[field] = []
                    # Add only unique items
                    for item in result[field]:
                        item_key = json.dumps(item, sort_keys=True)
                        if item_key not in seen_items:
                            merged[field].append(item)
                            seen_items.add(item_key)
                elif operation == "update":
                    if field not in merged:
                        merged[field] = {}
                    merged[field].update(result[field])
                elif operation == "max":
                    # Take the maximum value for numeric fields
                    if field not in merged or result[field] > merged[field]:
                        merged[field] = result[field]
                elif operation == "min":
                    # Take the minimum value for numeric fields
                    if field not in merged or result[field] < merged[field]:
                        merged[field] = result[field]
                elif operation == "avg":
                    # Calculate average for numeric fields
                    if field not in merged:
                        merged[field] = []
                    merged[field].append(result[field])
                    if len(merged[field]) == len(results):
                        merged[field] = sum(merged[field]) / len(merged[field])

    return merged

def validate_content(content: str) -> bool:
    """Validate content before processing.
    
    Args:
        content: Content to validate
        
    Returns:
        True if content is valid, False otherwise
    """
    if not content or not isinstance(content, str):
        warning_highlight("Invalid content type or empty content")
        return False
        
    if len(content) < 10:  # Minimum content length
        warning_highlight(f"Content too short: {len(content)} characters")
        return False
        
    return True

def detect_content_type(url: str, content: str) -> str:
    """Detect content type from URL and content.
    
    Args:
        url: URL of the content
        content: Content to analyze
        
    Returns:
        Detected content type
    """
    if not url:
        return "unknown"
        
    url_lower = url.lower()
    
    if url_lower.endswith('.pdf'):
        return 'pdf'
    elif url_lower.endswith(('.doc', '.docx')):
        return 'doc'
    elif url_lower.endswith(('.xls', '.xlsx')):
        return 'excel'
    elif url_lower.endswith(('.ppt', '.pptx')):
        return 'presentation'
    elif url_lower.endswith(('.txt', '.md', '.rst')):
        return 'text'
    elif url_lower.endswith(('.html', '.htm')):
        return 'html'
    elif url_lower.endswith(('.json', '.xml')):
        return 'data'
    else:
        return 'unknown'

__all__ = [
    "chunk_text",
    "preprocess_content",
    "estimate_tokens",
    "should_skip_content",
    "merge_chunk_results",
    "validate_content",
    "detect_content_type"
]
</file>

<file path="src/react_agent/utils/defaults.py">
"""Default values and configurations for the research agent.

This module consolidates all default values, configurations, and common structures
used across the research agent to maintain consistency and reduce duplication.
"""

from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass

# Default chunking configurations
@dataclass
class ChunkConfig:
    """Configuration for text chunking operations."""
    DEFAULT_CHUNK_SIZE: int = 4000
    DEFAULT_OVERLAP: int = 500
    LARGE_CHUNK_SIZE: int = 40000
    LARGE_OVERLAP: int = 5000

# Default extraction result structure
DEFAULT_EXTRACTION_RESULTS = {
    "market_dynamics": {
        "extracted_facts": [],
        "market_metrics": {
            "market_size": None,
            "growth_rate": None,
            "forecast_period": None
        },
        "relevance_score": 0.0
    },
    "provider_landscape": {
        "extracted_vendors": [],
        "vendor_relationships": [],
        "relevance_score": 0.0
    },
    "technical_requirements": {
        "extracted_requirements": [],
        "standards": [],
        "relevance_score": 0.0
    },
    "regulatory_landscape": {
        "extracted_regulations": [],
        "compliance_requirements": [],
        "relevance_score": 0.0
    },
    "cost_considerations": {
        "extracted_costs": [],
        "pricing_models": [],
        "relevance_score": 0.0
    },
    "best_practices": {
        "extracted_practices": [],
        "methodologies": [],
        "relevance_score": 0.0
    },
    "implementation_factors": {
        "extracted_factors": [],
        "challenges": [],
        "relevance_score": 0.0
    }
}

# Category-specific merge mappings
CATEGORY_MERGE_MAPPINGS = {
    "market_dynamics": {
        "extracted_facts": "extend",
        "market_metrics": "update"
    },
    "provider_landscape": {
        "extracted_vendors": "extend",
        "vendor_relationships": "extend"
    },
    "technical_requirements": {
        "extracted_requirements": "extend",
        "standards": "extend"
    },
    "regulatory_landscape": {
        "extracted_regulations": "extend",
        "compliance_requirements": "extend"
    },
    "cost_considerations": {
        "extracted_costs": "extend",
        "pricing_models": "extend"
    },
    "best_practices": {
        "extracted_practices": "extend",
        "methodologies": "extend"
    },
    "implementation_factors": {
        "extracted_factors": "extend",
        "challenges": "extend"
    }
}

def get_default_extraction_result(category: str) -> Dict[str, Any]:
    """Get a default empty extraction result when parsing fails.
    
    Args:
        category: Research category
        
    Returns:
        Default empty result dictionary
    """
    return DEFAULT_EXTRACTION_RESULTS.get(category, {"extracted_facts": [], "relevance_score": 0.0})

def get_category_merge_mapping(category: str) -> Dict[str, str]:
    """Get the merge mapping for a specific category.
    
    Args:
        category: Research category
        
    Returns:
        Dictionary mapping field names to merge operations
    """
    return CATEGORY_MERGE_MAPPINGS.get(category, {})

# Export all defaults
__all__ = [
    "ChunkConfig",
    "DEFAULT_EXTRACTION_RESULTS",
    "CATEGORY_MERGE_MAPPINGS",
    "get_default_extraction_result",
    "get_category_merge_mapping"
]
</file>

<file path="src/react_agent/utils/extraction.py">
"""Enhanced extraction module for research categories with statistics focus.

This module improves the extraction of facts and statistics from search results,
with a particular emphasis on numerical data, trends, and statistical information.
"""


import contextlib
from typing import Dict, List, Any, Optional, Union, Tuple
import re
import json
from datetime import datetime
from urllib.parse import urlparse

from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight, error_highlight
from react_agent.utils.validations import is_valid_url
from react_agent.utils.content import chunk_text, preprocess_content, merge_chunk_results
from react_agent.utils.defaults import get_default_extraction_result

# Initialize logger
logger = get_logger(__name__)

# Regular expressions for identifying statistical content
STAT_PATTERNS = [
    r'\d+%',  # Percentage
    r'\$\d+(?:,\d+)*(?:\.\d+)?(?:\s?(?:million|billion|trillion))?',  # Currency
    r'\d+(?:\.\d+)?(?:\s?(?:million|billion|trillion))?',  # Numbers with scale
    r'increased by|decreased by|grew by|reduced by|rose|fell',  # Trend language
    r'majority|minority|fraction|proportion|ratio',  # Proportion language
    r'survey|respondents|participants|study found',  # Research language
    r'statistics show|data indicates|report reveals',  # Statistical citation
    r'market share|growth rate|adoption rate|satisfaction score',  # Business metrics
    r'average|mean|median|mode|range|standard deviation'  # Statistical terms
]

COMPILED_STAT_PATTERNS = [re.compile(pattern, re.IGNORECASE) for pattern in STAT_PATTERNS]

def extract_citations(text: str) -> List[Dict[str, str]]:
    """Extract citation information from text."""
    citations = []
    
    # Find patterns like (Source: X), [X], cited from X, etc.
    citation_patterns = [
        r'\(Source:?\s+([^)]+)\)',
        r'\[([^]]+)\]',
        r'cited\s+from\s+([^,.;]+)',
        r'according\s+to\s+([^,.;]+)',
        r'reported\s+by\s+([^,.;]+)',
        r'([^,.;]+)\s+reports'
    ]
    
    for pattern in citation_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            citation = match.group(1).strip()
            # Skip dates that might be captured in brackets like [2022]
            if not re.match(r'^(19|20)\d{2}$', citation):
                citations.append({
                    "source": citation,
                    "context": text[max(0, match.start() - 50):min(len(text), match.end() + 50)]
                })
    
    return citations

def extract_statistics(text: str) -> List[Dict[str, Any]]:
    """Extract statistics and numerical data from text."""
    statistics = []
    sentences = re.split(r'(?<=[.!?])\s+', text)

    for sentence in sentences:
        for pattern in COMPILED_STAT_PATTERNS:
            if pattern.search(sentence):
                # Avoid duplicate statistics
                stat_text = sentence.strip()
                if all(s["text"] != stat_text for s in statistics):
                    # Attempt to rate the quality of the statistic
                    quality = rate_statistic_quality(stat_text)

                    statistics.append({
                        "text": stat_text,
                        "type": infer_statistic_type(stat_text),
                        "citations": extract_citations(stat_text),
                        "quality_score": quality,
                        "year_mentioned": extract_year(stat_text)
                    })
                break

    return statistics

def rate_statistic_quality(stat_text: str) -> float:
    """Rate the quality of a statistic on a scale of 0.0 to 1.0."""
    score = 0.5  # Base score
    
    # Higher quality if includes specific numbers
    if re.search(r'\d+(?:\.\d+)?%', stat_text):
        score += 0.15  # Specific percentage
    elif re.search(r'\$\d+(?:,\d+)*(?:\.\d+)?', stat_text):
        score += 0.15  # Specific currency amount
    
    # Higher quality if cites source
    if re.search(r'according to|reported by|cited from|source|study|survey', stat_text, re.IGNORECASE):
        score += 0.2
    
    # Higher quality if includes year
    if extract_year(stat_text):
        score += 0.1
    
    # Lower quality if uses vague language
    if re.search(r'may|might|could|possibly|potentially|estimated', stat_text, re.IGNORECASE):
        score -= 0.1
    
    # Cap between 0.0 and 1.0
    return max(0.0, min(1.0, score))

def extract_year(text: str) -> Optional[int]:
    """Extract year mentioned in text, if any."""
    year_match = re.search(r'\b(19\d{2}|20\d{2})\b', text)
    return int(year_match[1]) if year_match else None

def infer_statistic_type(text: str) -> str:
    """Infer the type of statistic mentioned."""
    if re.search(r'%|percent|percentage', text, re.IGNORECASE):
        return "percentage"
    elif re.search(r'\$|\beuro\b|\beur\b|\bgbp\b|\bjpy\b|cost|price|spend|budget', text, re.IGNORECASE):
        return "financial"
    elif re.search(r'time|duration|period|year|month|week|day|hour', text, re.IGNORECASE):
        return "temporal"
    elif re.search(r'ratio|proportion|fraction', text, re.IGNORECASE):
        return "ratio"
    elif re.search(r'increase|decrease|growth|decline|trend', text, re.IGNORECASE):
        return "trend"
    elif re.search(r'survey|respondent|participant', text, re.IGNORECASE):
        return "survey"
    elif re.search(r'market share|market size', text, re.IGNORECASE):
        return "market"
    else:
        return "general"

def enrich_extracted_fact(fact: Dict[str, Any], url: str, source_title: str) -> Dict[str, Any]:
    """Enrich an extracted fact with additional context and metadata."""
    # Add source information
    fact["source_url"] = url
    fact["source_title"] = source_title

    # Extract domain from URL
    try:
        domain = urlparse(url).netloc
        fact["source_domain"] = domain
    except Exception:
        fact["source_domain"] = ""

    # Add timestamp
    fact["extraction_timestamp"] = datetime.now().isoformat()

    # Extract statistics if present in source_text
    if "source_text" in fact and isinstance(fact["source_text"], str):
        if statistics := extract_statistics(fact["source_text"]):
            fact["statistics"] = statistics

    # Extract citations if present in source_text
    if "source_text" in fact and isinstance(fact["source_text"], str):
        if citations := extract_citations(fact["source_text"]):
            fact["additional_citations"] = citations

    # Add confidence score based on enriched data
    confidence_score = fact.get("confidence", 0.5)
    if isinstance(confidence_score, str):
        # Convert string confidence to float
        if confidence_score.lower() == "high":
            confidence_score = 0.9
        elif confidence_score.lower() == "medium":
            confidence_score = 0.7
        elif confidence_score.lower() == "low":
            confidence_score = 0.4
        else:
            confidence_score = 0.5

    # Adjust confidence based on statistics and citations
    if "statistics" in fact and len(fact["statistics"]) > 0:
        confidence_score = min(1.0, confidence_score + 0.1)

    if "additional_citations" in fact and len(fact["additional_citations"]) > 0:
        confidence_score = min(1.0, confidence_score + 0.1)

    fact["confidence_score"] = confidence_score

    return fact

def find_json_object(text: str) -> Optional[str]:
    """Find a JSON object in text using balanced brace matching.
    
    This is more robust than simple regex for finding JSON objects as it:
    1. Handles nested braces correctly
    2. Supports both objects and arrays as root elements
    3. Finds the longest valid JSON-like structure
    
    Args:
        text: Text that may contain a JSON object
        
    Returns:
        Extracted JSON-like text or None if not found
    """
    # Look for both object and array patterns
    for start_char, end_char in [('{', '}'), ('[', ']')]:
        # Find potential starting positions
        start_positions = [pos for pos, char in enumerate(text) if char == start_char]
        
        for start_pos in start_positions:
            # Track nesting level
            level = 0
            # Track position
            pos = start_pos
            
            # Scan through text tracking brace/bracket balance
            while pos < len(text):
                char = text[pos]
                if char == start_char:
                    level += 1
                elif char == end_char:
                    level -= 1
                    # If we've found a balanced structure, extract it
                    if level == 0:
                        return text[start_pos:pos+1]
                pos += 1
    
    # No balanced JSON-like structure found
    return None

def safe_json_parse(response: Union[str, Dict[str, Any]], category: str) -> Dict[str, Any]:
    """Safely parse JSON response with enhanced error handling and cleanup.
    
    Args:
        response: Response to parse (string or dict)
        category: Category for default structure
        
    Returns:
        Parsed JSON dictionary
    """
    try:
        # If already a dict, return it
        if isinstance(response, dict):
            return response
            
        # Clean up the response string
        if isinstance(response, str):
            # Remove any markdown code blocks
            response = re.sub(r'```(?:json)?\s*|\s*```', '', response)
            
            # Remove any leading/trailing whitespace
            response = response.strip()
            
            # Handle empty or invalid responses
            if not response or response in ['{}', '[]', 'null']:
                return get_default_extraction_result(category)
                
            # Handle single-quoted strings
            response = response.replace("'", '"')
            
            # Handle trailing commas
            response = re.sub(r',(\s*[}\]])', r'\1', response)
            
            # Handle missing braces
            if not response.startswith('{'):
                response = '{' + response
            if not response.endswith('}'):
                response = response + '}'
                
            # Handle quoted keys without values
            response = re.sub(r'"(\w+)":\s*"', r'"\1": []', response)
            
            # Parse the cleaned JSON
            parsed = json.loads(response)
            
            # Ensure we have a dictionary
            if not isinstance(parsed, dict):
                return get_default_extraction_result(category)
                
            # Validate and clean up the parsed structure
            result = get_default_extraction_result(category)
            
            # Copy valid fields from parsed result
            for key, value in parsed.items():
                if key in result:
                    if isinstance(value, list) and isinstance(result[key], list):
                        result[key] = value
                    elif isinstance(value, dict) and isinstance(result[key], dict):
                        result[key].update(value)
                    elif isinstance(value, (int, float)) and key in ['relevance_score', 'confidence_score']:
                        result[key] = float(value)
                        
            return result
            
    except json.JSONDecodeError as e:
        error_highlight(f"JSON parsing error: {str(e)}")
        return get_default_extraction_result(category)
    except Exception as e:
        error_highlight(f"Error in safe_json_parse: {str(e)}")
        return get_default_extraction_result(category)

async def extract_category_information(
    content: str,
    url: str,
    title: str,
    category: str,
    original_query: str,
    prompt_template: str,
    extraction_model: Any,
    config: Optional[RunnableConfig] = None
) -> Tuple[List[Dict[str, Any]], float]:
    """Extract information from content for a specific category with enhanced statistics focus.
    
    Args:
        content: The content to extract information from
        url: URL of the content
        title: Title of the content
        category: Research category
        original_query: Original search query
        prompt_template: Template for extraction prompt
        extraction_model: Model to use for extraction
        config: Optional configuration
        
    Returns:
        Tuple of (extracted facts, relevance score)
    """
    if not content or not url or not is_valid_url(url):
        warning_highlight(f"Invalid content or URL for extraction: {url}")
        return [], 0.0

    info_highlight(f"Extracting from {url} for {category}")

    try:
        # Preprocess content to reduce size
        content = preprocess_content(content, url)

        # Apply extraction prompt template
        extraction_prompt = prompt_template.format(
            query=original_query,
            url=url,
            content=content
        )

        # If content is too large, chunk it and process each chunk
        if len(content) > 40000:  # Threshold for chunking
            info_highlight(f"Content too large ({len(content)} chars), chunking...")
            chunks = chunk_text(content)

            all_statistics = []
            chunk_results = []

            for chunk_idx, chunk in enumerate(chunks):
                info_highlight(f"Processing chunk {chunk_idx + 1}/{len(chunks)}")
                chunk_prompt = prompt_template.format(
                    query=original_query,
                    url=url,
                    content=chunk
                )

                # Extract from this chunk
                chunk_response = await extraction_model(
                    messages=[{"role": "human", "content": chunk_prompt}],
                    config=config
                )

                if chunk_result := safe_json_parse(chunk_response, category):
                    # Extract statistics from this chunk directly
                    chunk_statistics = extract_statistics(chunk)
                    all_statistics.extend(chunk_statistics)

                    # Add to chunk results for merging
                    chunk_results.append(chunk_result)

            # Merge results from all chunks
            extraction_result = merge_chunk_results(chunk_results, category)

            # Add all statistics to the merged result
            if all_statistics:
                extraction_result["statistics"] = all_statistics
        else:
            # Process single chunk
            model_response = await extraction_model(
                messages=[{"role": "human", "content": extraction_prompt}],
                config=config
            )

            # Handle model response
            if isinstance(model_response, str):
                # Clean up the response
                model_response = model_response.strip()
                
                # If it starts with a quoted key, wrap it in braces
                if model_response.startswith('"') or model_response.startswith('\n  "'):
                    # Extract the key name
                    match = re.search(r'"(extracted_[^"]+)"', model_response)
                    if match:
                        key = match.group(1)
                        model_response = f'{{"{key}": []}}'
                    else:
                        model_response = f'{{"extracted_facts": []}}'
                
                # If it's an error message, use default
                if model_response == "' and ending with '" or model_response == '" and ending with "':
                    model_response = '{"extracted_facts": []}'

            # Safely parse the model response
            extraction_result = safe_json_parse(model_response, category)

            if statistics := extract_statistics(content):
                extraction_result["statistics"] = statistics

        # Get relevance score
        relevance_score = extraction_result.get("relevance_score", 0.0)

        # Get extracted facts based on category
        facts = _get_category_facts(category, extraction_result)

        # Enrich each fact with additional context
        enriched_facts = [
            enrich_extracted_fact(fact, url, title)
            for fact in facts
        ]

        # Sort facts by confidence score
        sorted_facts = sorted(
            enriched_facts, 
            key=lambda x: x.get("confidence_score", 0), 
            reverse=True
        )

        return sorted_facts, relevance_score

    except Exception as e:
        error_highlight(f"Error extracting from {url}: {str(e)}")
        return [], 0.0

def _get_category_facts(category: str, extraction_result: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Extract facts from extraction result based on category structure."""
    if not extraction_result or not isinstance(extraction_result, dict):
        return []

    # Mapping of categories to their corresponding fact types and keys
    category_mapping = {
        "market_dynamics": [("fact", "extracted_facts")],
        "provider_landscape": [
            ("vendor", "extracted_vendors"),
            ("relationship", "vendor_relationships")
        ],
        "technical_requirements": [
            ("requirement", "extracted_requirements"),
            ("standard", "standards")
        ],
        "regulatory_landscape": [
            ("regulation", "extracted_regulations"),
            ("compliance", "compliance_requirements")
        ],
        "cost_considerations": [
            ("cost", "extracted_costs"),
            ("pricing_model", "pricing_models")
        ],
        "best_practices": [
            ("practice", "extracted_practices"),
            ("methodology", "methodologies")
        ],
        "implementation_factors": [
            ("factor", "extracted_factors"),
            ("challenge", "challenges")
        ]
    }

    facts = []
    for fact_type, key in category_mapping.get(category, [("fact", "extracted_facts")]):
        items = extraction_result.get(key, [])
        facts.extend([{"type": fact_type, "data": item} for item in items])
    
    return facts
</file>

<file path="src/react_agent/utils/llm.py">
"""LLM utility functions for handling model calls and content processing.

This module provides utilities for interacting with language models,
including content length management and error handling.
"""

from typing import List, Dict, Any, Optional, Union, cast
import json
import logging
from datetime import datetime, timezone
import os
import asyncio

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig, ensure_config
from openai import AsyncClient
from openai.types.chat import ChatCompletionMessageParam
from anthropic import AsyncAnthropic

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight, error_highlight
from react_agent.utils.content import (
    chunk_text,
    preprocess_content,
    estimate_tokens,
    validate_content,
    detect_content_type,
    merge_chunk_results
)
from react_agent.configuration import Configuration
from react_agent.utils.extraction import safe_json_parse
from react_agent.utils.defaults import ChunkConfig

# Initialize logger
logger = get_logger(__name__)

# Constants
MAX_TOKENS: int = 16000  # Reduced from 100000 to stay within model limits
MAX_SUMMARY_TOKENS: int = 2000  # New constant for summary model

# Initialize API clients
openai_client = AsyncClient(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_BASE")
)
anthropic_client = AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

async def summarize_content(input_content: str, max_tokens: int = MAX_SUMMARY_TOKENS) -> str:
    """Summarize content using a more efficient model.
    
    Args:
        input_content: The content to summarize
        max_tokens: Maximum tokens for the summary
        
    Returns:
        Summarized content
    """
    try:
        # Use a more efficient model for summarization
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",  # Use a more efficient model for summarization
            messages=[
                {"role": "system", "content": "You are a helpful assistant that creates concise summaries. Focus on key points and maintain factual accuracy."},
                {"role": "user", "content": f"Please summarize the following content concisely:\n\n{input_content}"}
            ],
            max_tokens=max_tokens,
            temperature=0.3  # Lower temperature for more focused summaries
        )
        response_content = cast(Optional[str], response.choices[0].message.content)
        return response_content if response_content is not None else ""
    except Exception as e:
        error_highlight(f"Error in summarize_content: {str(e)}")
        return input_content  # Return original content if summarization fails

async def _format_openai_messages(
    messages: List[Dict[str, str]], 
    system_prompt: str,
    max_tokens: Optional[int] = None
) -> List[ChatCompletionMessageParam]:
    """Format messages for OpenAI API with enhanced content handling.
    
    Args:
        messages: List of message dictionaries
        system_prompt: System prompt to use
        max_tokens: Optional maximum tokens per message
        
    Returns:
        Formatted messages for OpenAI API
    """
    if not messages:
        return [{"role": "system", "content": system_prompt}]
        
    formatted_messages: List[ChatCompletionMessageParam] = []
    max_tokens = max_tokens or MAX_TOKENS
    
    for msg in messages:
        if msg["role"] == "system":
            formatted_messages.append({"role": "system", "content": msg["content"]})
        else:
            content = msg["content"]
            if estimate_tokens(content) > max_tokens:
                info_highlight("Content too long, summarizing...")
                content = await summarize_content(content, max_tokens)
            formatted_messages.append({"role": "user", "content": content})

    if all(msg["role"] != "system" for msg in formatted_messages):
        formatted_messages.insert(0, {"role": "system", "content": system_prompt})
    
    return formatted_messages

async def _call_openai_api(model: str, messages: List[ChatCompletionMessageParam]) -> Dict[str, Any]:
    """Call OpenAI API with formatted messages."""
    try:
        response = await openai_client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=MAX_TOKENS,
            temperature=0.7
        )
        content = response.choices[0].message.content
        return {"content": content} if content else {}
    except Exception as e:
        error_highlight(f"Error in _call_openai_api: {str(e)}")
        return {}

async def call_model(
    messages: List[Dict[str, str]],
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Call the language model with the given messages."""
    if not messages:
        error_highlight("No messages provided to call_model")
        return {}

    try:
        config = config or {}
        configurable = config.get("configurable", {})
        configurable["timestamp"] = datetime.now(timezone.utc).isoformat()
        config = {**config, "configurable": configurable}

        configuration = Configuration.from_runnable_config(config)
        logger.info(f"Calling model with {len(messages)} messages")
        logger.debug(f"Config: {config}")

        provider, model = configuration.model.split("/", 1)

        if provider == "openai":
            formatted_messages = await _format_openai_messages(messages, configuration.system_prompt)
            return await _call_openai_api(model, formatted_messages)
        elif provider == "anthropic":
            formatted_messages = [
                {"role": "user", "content": configuration.system_prompt} if messages[0]["role"] != "system" else None,
                *[{"role": "user" if msg["role"] == "system" else msg["role"], "content": msg["content"]} for msg in messages]
            ]
            formatted_messages = [msg for msg in formatted_messages if msg is not None]
            
            response = await anthropic_client.messages.create(
                model=model,
                messages=formatted_messages,
                max_tokens=MAX_TOKENS,
                temperature=0.7
            )
            return {"content": str(response.content[0])}
        else:
            error_highlight(f"Unsupported model provider: {provider}")
            return {}

    except Exception as e:
        error_highlight(f"Error in call_model: {str(e)}")
        return {}

async def _process_chunk(
    chunk: str,
    previous_messages: List[Dict[str, str]],
    config: Optional[RunnableConfig] = None,
    model: Optional[str] = None
) -> Dict[str, Any]:
    """Process a single chunk of content with enhanced error handling.
    
    Args:
        chunk: Content chunk to process
        previous_messages: Previous messages in the conversation
        config: Optional configuration
        model: Optional model to use for processing
        
    Returns:
        Processed chunk result
    """
    if not chunk or not previous_messages:
        return {}

    try:
        # Create messages with chunk
        messages = previous_messages + [{"role": "human", "content": chunk}]
        
        # Call model with retries
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                response = await call_model(messages, config)
                if not response or not response.get("content"):
                    error_highlight("Empty response from model")
                    return {}
                    
                # Parse JSON response with enhanced error handling
                parsed = safe_json_parse(response["content"], "model_response")
                return parsed if parsed else {}
                
            except Exception as e:
                if attempt < max_retries - 1:
                    warning_highlight(f"Attempt {attempt + 1} failed: {str(e)}. Retrying...")
                    await asyncio.sleep(retry_delay * (attempt + 1))
                else:
                    error_highlight(f"All retry attempts failed: {str(e)}")
                    return {}
                    
    except Exception as e:
        error_highlight(f"Error processing chunk: {str(e)}")
        return {}
        
    return {}  # Fallback return to satisfy type checker

async def call_model_json(
    messages: List[Dict[str, str]],
    config: Optional[RunnableConfig] = None,
    chunk_size: Optional[int] = None,
    overlap: Optional[int] = None
) -> Dict[str, Any]:
    """Call the model with JSON output format and enhanced chunking.
    
    Args:
        messages: List of message dictionaries
        config: Optional configuration
        chunk_size: Optional custom chunk size
        overlap: Optional custom overlap size
        
    Returns:
        JSON response from the model
    """
    try:
        if not messages:
            error_highlight("No messages provided to call_model_json")
            return {}
            
        # Process content in chunks if needed
        content = messages[-1]["content"]
        
        # Check if content needs chunking
        if estimate_tokens(content) > MAX_TOKENS:
            info_highlight(f"Content too large ({estimate_tokens(content)} tokens), chunking...")
            chunks = chunk_text(
                content,
                chunk_size=chunk_size,
                overlap=overlap,
                use_large_chunks=True
            )
            
            if len(chunks) > 1:
                # Process chunks in parallel with rate limiting
                chunk_results = []
                for chunk in chunks:
                    result = await _process_chunk(chunk, messages[:-1], config)
                    if result:
                        chunk_results.append(result)
                        
                if not chunk_results:
                    error_highlight("No valid results from chunks")
                    return {}
                    
                # Merge results with enhanced merging
                return merge_chunk_results(chunk_results, "model_response")
            else:
                return await _process_chunk(content, messages[:-1], config)
        else:
            return await _process_chunk(content, messages[:-1], config)
            
    except Exception as e:
        error_highlight(f"Error in call_model_json: {str(e)}")
        return {}

async def _process_chunked_content(
    messages: List[Dict[str, str]],
    content: str,
    config: Optional[RunnableConfig]
) -> Dict[str, Any]:
    """Process content that exceeds token limit by chunking."""
    try:
        chunks = chunk_text(
            content,
            chunk_size=ChunkConfig.DEFAULT_CHUNK_SIZE,
            overlap=ChunkConfig.DEFAULT_OVERLAP
        )
        chunk_results = []
        
        for i, chunk in enumerate(chunks):
            chunk_messages = messages[:-1] + [{"role": "human", "content": chunk}]
            try:
                chunk_response = await call_model(chunk_messages, config)
                if chunk_response and chunk_response.get("content"):
                    chunk_results.append(chunk_response)
            except Exception as e:
                error_highlight(f"Error processing chunk {i+1}: {str(e)}")
                continue
        
        if not chunk_results:
            error_highlight("No valid results from chunks")
            return {}
            
        return merge_chunk_results(chunk_results, "general")
    except Exception as e:
        error_highlight(f"Error in _process_chunked_content: {str(e)}")
        return {}

def _parse_json_response(response: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
    """Parse and clean JSON response from model."""
    try:
        if isinstance(response, dict):
            return response
        return safe_json_parse(response, "model_response")
    except Exception as e:
        error_highlight(f"Error in _parse_json_response: {str(e)}")
        return {}

__all__ = ["call_model_json", "call_model"]
</file>

<file path="src/react_agent/utils/logging.py">
"""Logging utilities.

This module provides enhanced logging utilities and convenience methods
for the agent framework. It builds upon the basic logging configuration
defined in log_config.py.
"""

import logging
from typing import Any, Mapping, Optional, Dict


"""Module: log_config.py. This module provides logging configuration for the enrichment agent.

-------------------------
It includes functions for setting up and configuring loggers with rich formatting.
"""

# Standard library imports
import logging
import threading
from typing import Optional  # noqa: F401

# Third-party imports
from rich.console import Console
from rich.logging import RichHandler

# Create console for rich output - use stderr for logs
console = Console(stderr=True)

# Logging configuration
LOG_FORMAT = "%(message)s"  # Added back for backward compatibility with tests
DATE_FORMAT = "[%X]"

# Logger lock for thread safety
_logger_lock = threading.Lock()


def setup_logger(
    name: str = "enrichment_agent", level: int = logging.INFO
) -> logging.Logger:
    """Set up a logger with rich formatting for beautiful console output.

    Args:
        name: Name of the logger. Defaults to "enrichment_agent".
        level: Logging level. Defaults to logging.INFO.

    Returns:
        Configured logger instance with rich formatting.
    """
    with _logger_lock:
        logger = logging.getLogger(name)

        # Only configure the logger if it hasn't been configured yet
        if not logger.handlers:
            logger.setLevel(level)

            # Create rich handler with proper parameter passing
            handler = RichHandler(
                console=console,
                rich_tracebacks=True,
                tracebacks_show_locals=True,
                show_time=True,
                show_path=True,
                markup=True,
                log_time_format=DATE_FORMAT,
                omit_repeated_times=False,
                level=level,
            )

            # Add handler and disable propagation to prevent duplicate logs
            logger.addHandler(handler)
            logger.propagate = False

        return logger


# Create a default logger instance
logger = setup_logger()

def set_level(level: int) -> None:
    """Set the logging level for the default logger and root logger.
    
    Args:
        level: The logging level to set (e.g., logging.DEBUG, logging.INFO)
    """
    with _logger_lock:
        # Set level for the default logger
        logger.setLevel(level)
        
        # Update handler level
        for handler in logger.handlers:
            handler.setLevel(level)
        
        # Also set for root logger to affect other loggers in hierarchy
        root_logger = logging.getLogger()
        root_logger.setLevel(level)


def get_logger(name: str) -> logging.Logger:
    """Get a logger instance with the specified name.

    This function returns a logger configured with rich formatting and proper
    log levels. It uses the setup_logger function from log_config to ensure
    consistent logging configuration across the application.

    Args:
        name (str): The name for the logger, typically __name__ from the calling module

    Returns:
        logging.Logger: A configured logger instance with rich formatting
    """
    return setup_logger(name)


# Convenience methods for formatted logging with Rich markup
def info_success(message: str, exc_info: bool | BaseException | None = None) -> None:
    """Log a success message with green formatting.

    Args:
        message (str): The message to log
        exc_info (bool | BaseException | None): Exception information to include in the log
    """
    logger.info("[bold green]✓ %s[/bold green]", message, exc_info=exc_info)


def info_highlight(message: str, category: Optional[str] = None, progress: Optional[str] = None, exc_info: bool | BaseException | None = None) -> None:
    """Log a highlighted informational message with optional category and progress tracking.
    
    Args:
        message: The message to log
        category: Optional category for the message
        progress: Optional progress indicator
        exc_info: Exception information to include in the log
    """
    if progress:
        message = f"[{progress}] {message}"
    if category:
        message = f"[{category}] {message}"
    logger.info("[bold blue]ℹ %s[/bold blue]", message, exc_info=exc_info)


def warning_highlight(message: str, category: Optional[str] = None, exc_info: bool | BaseException | None = None) -> None:
    """Log a highlighted warning message with optional category.
    
    Args:
        message: The message to log
        category: Optional category for the message
        exc_info: Exception information to include in the log
    """
    if category:
        message = f"[{category}] {message}"
    logger.warning("[bold yellow]⚠ %s[/bold yellow]", message, exc_info=exc_info)


def error_highlight(message: str, category: Optional[str] = None, exc_info: bool | BaseException | None = None) -> None:
    """Log a highlighted error message with optional category.
    
    Args:
        message: The message to log
        category: Optional category for the message
        exc_info: Exception information to include in the log
    """
    if category:
        message = f"[{category}] {message}"
    logger.error("[bold red]✗ %s[/bold red]", message, exc_info=exc_info)


def log_dict(
    data: Mapping[str, Any], level: int = logging.INFO, title: str | None = None
) -> None:
    """Log a dictionary with pretty formatting.

    Args:
        data (Mapping[str, Any]): The dictionary to log
        level (int): The logging level to use. Defaults to logging.INFO
        title (str | None): Optional title to display before the dictionary

    Raises:
        ValueError: If level is not a valid logging level
    """
    # Validate logging level
    if level not in (
        logging.DEBUG,
        logging.INFO,
        logging.WARNING,
        logging.ERROR,
        logging.CRITICAL,
    ):
        raise ValueError(f"Invalid logging level: {level}")

    if title:
        logger.log(level, "[bold]%s[/bold]", title)

    # Use rich's pretty printing through the logger
    for key, value in data.items():
        logger.log(level, "  [cyan]%s[/cyan]: %s", key, value)


def log_step(
    step_name: str, step_number: int | None = None, total_steps: int | None = None
) -> None:
    """Log a processing step with optional step counter.

    Args:
        step_name (str): The name of the step
        step_number (int | None): Current step number if part of a sequence
        total_steps (int | None): Total number of steps in the sequence

    Raises:
        ValueError: If step_number is provided without total_steps or vice versa
        ValueError: If step_number is greater than total_steps
    """
    # Validate step numbers
    if (step_number is None) != (total_steps is None):
        raise ValueError("Both step_number and total_steps must be provided together")

    if step_number is None or total_steps is None:
        logger.info("[bold magenta]Step:[/bold magenta] %s", step_name)

    elif not 1 <= step_number <= total_steps:
        raise ValueError(f"Invalid step numbers: {step_number}/{total_steps}")
    else:
        logger.info(
            "[bold magenta]Step %s/%s:[/bold magenta] %s",
            step_number,
            total_steps,
            step_name,
        )


def log_progress(current: int, total: int, category: str, operation: str):
    """Log progress for long-running operations.
    
    Args:
        current: Current progress value
        total: Total number of items
        category: Category being processed
        operation: Type of operation (e.g., "processing", "extracting")
    """
    if total > 0:
        percentage = (current / total) * 100
        info_highlight(
            f"{operation} {current}/{total} ({percentage:.1f}%)",
            category=category
        )


def log_performance_metrics(
    operation: str,
    start_time: float,
    end_time: float,
    category: Optional[str] = None,
    additional_info: Optional[Dict[str, Any]] = None
) -> None:
    """Log performance metrics for operations.
    
    Args:
        operation: Name of the operation being measured
        start_time: Start time of the operation
        end_time: End time of the operation
        category: Optional category for the operation
        additional_info: Optional dictionary of additional metrics
    """
    duration = end_time - start_time
    message = f"{operation} completed in {duration:.2f}s"
    
    if additional_info:
        info_parts = [f"{k}: {v}" for k, v in additional_info.items()]
        message += f" ({', '.join(info_parts)})"
    
    info_highlight(message, category=category)
</file>

<file path="src/react_agent/utils/statistics.py">
"""Improved confidence scoring with statistical validation.

This module enhances the confidence scoring logic to focus on statistical
validation, source quality assessment, and cross-validation to achieve
confidence scores above 80%.
"""

from typing import Dict, List, Any, Optional, Set
from datetime import datetime, timezone
import re
from urllib.parse import urlparse
from collections import Counter

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight, error_highlight

# Initialize logger
logger = get_logger(__name__)

# Authority domain patterns
AUTHORITY_DOMAINS = [
    r'\.gov($|/)',  # Government domains
    r'\.edu($|/)',  # Educational institutions
    r'\.org($|/)',  # Non-profit organizations
    r'research\.',  # Research organizations
    r'\.ac\.($|/)',  # Academic institutions
    r'journal\.',   # Academic journals
    r'university\.',  # Universities
    r'institute\.',  # Research institutes
    r'association\.'  # Professional associations
]

# Compiled patterns for efficiency
COMPILED_AUTHORITY_PATTERNS = [re.compile(pattern) for pattern in AUTHORITY_DOMAINS]

# High-credibility source terms
HIGH_CREDIBILITY_TERMS = [
    'study', 'research', 'survey', 'report', 'analysis', 
    'journal', 'publication', 'paper', 'review', 'assessment',
    'statistics', 'data', 'findings', 'results', 'evidence'
]

def calculate_category_quality_score(
    category: str,
    extracted_facts: List[Dict[str, Any]],
    sources: List[Dict[str, Any]],
    thresholds: Dict[str, Any]
) -> float:
    """Calculate enhanced quality score for a category based on extracted data.
    
    Args:
        category: Research category
        extracted_facts: List of extracted facts
        sources: List of source information
        thresholds: Quality thresholds for this category
        
    Returns:
        Quality score (0.0-1.0)
    """
    # Start with a slightly higher base score
    score = 0.35
    
    # Get category-specific thresholds
    min_facts = thresholds.get("min_facts", 3)
    min_sources = thresholds.get("min_sources", 2)
    auth_ratio = thresholds.get("authoritative_source_ratio", 0.5)
    recency_threshold = thresholds.get("recency_threshold_days", 365)
    
    # 1. QUANTITY ASSESSMENT (0.25 max)
    # Add points for number of facts (nonlinear scale to reward comprehensiveness)
    if len(extracted_facts) >= min_facts * 3:  # Excellent quantity
        score += 0.15
    elif len(extracted_facts) >= min_facts * 2:  # Very good quantity
        score += 0.12
    elif len(extracted_facts) >= min_facts:  # Good quantity
        score += 0.08
    else:  # Below threshold
        fact_ratio = len(extracted_facts) / min_facts
        score += fact_ratio * 0.06
    
    # Add points for number of sources (reward diversity)
    if len(sources) >= min_sources * 3:  # Excellent source diversity
        score += 0.10
    elif len(sources) >= min_sources * 2:  # Very good source diversity
        score += 0.08
    elif len(sources) >= min_sources:  # Good source diversity
        score += 0.05
    else:  # Below threshold
        source_ratio = len(sources) / min_sources
        score += source_ratio * 0.03
    
    # 2. STATISTICAL CONTENT (0.20 max)
    # Reward statistical content
    stat_facts = [
        f for f in extracted_facts 
        if "statistics" in f or 
        (isinstance(f.get("source_text"), str) and 
         re.search(r'\d+', f.get("source_text", "")) is not None)
    ]
    
    if stat_facts:
        stat_ratio = len(stat_facts) / len(extracted_facts) if extracted_facts else 0
        if stat_ratio >= 0.5:  # Excellent statistical content
            score += 0.20
        elif stat_ratio >= 0.3:  # Good statistical content
            score += 0.15
        elif stat_ratio >= 0.1:  # Some statistical content
            score += 0.10
        else:  # Minimal statistical content
            score += 0.05
    
    # 3. SOURCE QUALITY (0.25 max)
    # Assess authoritative sources
    authoritative_sources = assess_authoritative_sources(sources)
    if sources:
        auth_source_ratio = len(authoritative_sources) / len(sources)
        
        if auth_source_ratio >= auth_ratio * 1.5:  # Excellent authority
            score += 0.25
        elif auth_source_ratio >= auth_ratio:  # Good authority
            score += 0.20
        elif auth_source_ratio >= auth_ratio * 0.7:  # Adequate authority
            score += 0.15
        elif auth_source_ratio >= auth_ratio * 0.5:  # Minimal authority
            score += 0.10
        else:  # Poor authority
            score += 0.05
    
    # 4. RECENCY (0.15 max)
    # Assess source recency
    recent_sources = count_recent_sources(sources, recency_threshold)
    if sources:
        recency_ratio = recent_sources / len(sources)
        
        if recency_ratio >= 0.8:  # Very recent sources
            score += 0.15
        elif recency_ratio >= 0.6:  # Mostly recent sources
            score += 0.12
        elif recency_ratio >= 0.4:  # Some recent sources
            score += 0.08
        elif recency_ratio >= 0.2:  # Few recent sources
            score += 0.05
        else:  # Outdated sources
            score += 0.02
    
    # 5. CONSISTENCY AND CROSS-VALIDATION (0.15 max)
    # Assess fact consistency and cross-validation
    consistency_score = assess_fact_consistency(extracted_facts)
    score += consistency_score * 0.15
    
    # Log detailed scoring breakdown
    info_highlight(f"Category {category} quality score breakdown:")
    info_highlight(f"  - Facts: {len(extracted_facts)}/{min_facts} min")
    info_highlight(f"  - Sources: {len(sources)}/{min_sources} min")
    info_highlight(f"  - Statistical content: {len(stat_facts)}/{len(extracted_facts)} facts")
    info_highlight(f"  - Authoritative sources: {len(authoritative_sources)}/{len(sources)} sources")
    info_highlight(f"  - Recent sources: {recent_sources}/{len(sources)} sources")
    info_highlight(f"  - Consistency score: {consistency_score:.2f}")
    info_highlight(f"  - Final score: {min(1.0, score):.2f}")
    
    return min(1.0, score)

def assess_authoritative_sources(sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Assess which sources are from authoritative domains."""
    authoritative_sources = []
    
    for source in sources:
        url = source.get("url", "")
        quality_score = source.get("quality_score", 0)
        
        # Check domain patterns
        is_authoritative_domain = any(
            pattern.search(url) for pattern in COMPILED_AUTHORITY_PATTERNS
        )
        
        # Check title and source for credibility terms
        title = source.get("title", "").lower()
        source_name = source.get("source", "").lower() 
        
        credibility_term_count = sum(
            1 for term in HIGH_CREDIBILITY_TERMS 
            if term in title or term in source_name
        )
        
        # Consider authoritative if domain matches or high quality score or credibility terms
        if (
            is_authoritative_domain or 
            quality_score >= 0.8 or 
            credibility_term_count >= 2
        ):
            authoritative_sources.append(source)
    
    return authoritative_sources

def count_recent_sources(sources: List[Dict[str, Any]], recency_threshold: int) -> int:
    """Count how many sources are recent according to threshold."""
    recent_count = 0
    current_time = datetime.now().replace(tzinfo=timezone.utc)
    
    for source in sources:
        published_date = source.get("published_date")
        if not published_date:
            continue
            
        try:
            # First try direct datetime parsing
            try:
                date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
            except (ValueError, AttributeError):
                # Try other common formats
                from dateutil import parser
                date = parser.parse(published_date)
                
            # Calculate days difference
            if hasattr(date, 'tzinfo') and date.tzinfo is None:
                # Make naive datetime timezone-aware
                date = date.replace(tzinfo=timezone.utc)
                
            days_old = (current_time - date).days
            if days_old <= recency_threshold:
                recent_count += 1
        except Exception:
            # If unparseable, don't count as recent
            pass
    
    return recent_count

def assess_fact_consistency(facts: List[Dict[str, Any]]) -> float:
    """Assess consistency among extracted facts.
    
    Returns:
        Score from 0.0-1.0 representing consistency
    """
    if not facts or len(facts) < 2:
        return 0.5  # Neutral score if insufficient facts
    
    # Extract key topics/entities mentioned across facts
    topics = extract_topics_from_facts(facts)
    
    # Count topic occurrences
    topic_counts = Counter(topics)
    
    # Calculate consistency based on topic distribution
    if not topic_counts:
        return 0.5
    
    # Get top topics (those mentioned multiple times)
    recurring_topics = {topic for topic, count in topic_counts.items() if count > 1}
    
    if not recurring_topics:
        return 0.5
    
    # Calculate what percentage of facts mention recurring topics
    facts_with_recurring = 0
    for fact in facts:
        fact_topics = get_topics_in_fact(fact)
        if any(topic in recurring_topics for topic in fact_topics):
            facts_with_recurring += 1
    
    return min(1.0, facts_with_recurring / len(facts))

def extract_topics_from_facts(facts: List[Dict[str, Any]]) -> List[str]:
    """Extract key topics/entities from facts."""
    all_topics = []
    
    for fact in facts:
        fact_topics = get_topics_in_fact(fact)
        all_topics.extend(fact_topics)
    
    return all_topics

def get_topics_in_fact(fact: Dict[str, Any]) -> Set[str]:
    """Extract topics from a single fact."""
    topics = set()
    
    # Extract from different fact formats
    if "data" in fact and isinstance(fact["data"], dict):
        data = fact["data"]
        # Handle different types of facts
        if fact.get("type") == "vendor":
            if "vendor_name" in data:
                topics.add(data["vendor_name"].lower())
        elif fact.get("type") == "relationship":
            entities = data.get("entities", [])
            for entity in entities:
                if isinstance(entity, str):
                    topics.add(entity.lower())
        elif fact.get("type") in ["requirement", "standard", "regulation", "compliance"]:
            if "description" in data:
                extract_noun_phrases(data["description"], topics)
    
    # Extract from source text if available
    if "source_text" in fact and isinstance(fact["source_text"], str):
        extract_noun_phrases(fact["source_text"], topics)
    
    return topics

def extract_noun_phrases(text: str, topics: Set[str]) -> None:
    """Extract potential noun phrases from text and add to topics set."""
    if not text:
        return
    
    # Simple noun phrase extraction (can be enhanced with NLP)
    # Currently just extracts capitalized multi-word sequences
    for match in re.finditer(r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)', text):
        topics.add(match.group(1).lower())
    
    # Also extract technical terms and acronyms
    for match in re.finditer(r'\b([A-Z]{2,})\b', text):
        topics.add(match.group(1).lower())

def calculate_overall_confidence(
    category_scores: Dict[str, float],
    synthesis_quality: float,
    validation_score: float
) -> float:
    """Calculate overall confidence score from multiple inputs.
    
    Args:
        category_scores: Quality scores for each research category
        synthesis_quality: Quality score for synthesis
        validation_score: Score from validation process
        
    Returns:
        Overall confidence score (0.0-1.0)
    """
    # If no scores, return low confidence
    if not category_scores:
        return 0.3
    
    # Calculate average category score
    avg_category_score = sum(category_scores.values()) / len(category_scores)
    
    # Base confidence on weighted components
    base_score = (
        avg_category_score * 0.5 +  # Category quality is 50% of score
        synthesis_quality * 0.3 +    # Synthesis quality is 30% of score
        validation_score * 0.2       # Validation score is 20% of score
    )
    
    # Apply modifiers based on coverage
    # Full coverage of all categories gets a boost
    if len(category_scores) >= 7 and all(score >= 0.6 for score in category_scores.values()):
        base_score += 0.1
    
    # Strong statistical content gets a boost
    stats_categories = sum(1 for cat, score in category_scores.items() 
                         if cat in ['market_dynamics', 'cost_considerations'] and score >= 0.7)
    if stats_categories >= 2:
        base_score += 0.05
    
    # Ensure cap at 1.0
    return min(1.0, base_score)

def assess_synthesis_quality(synthesis: Dict[str, Any]) -> float:
    """Assess the quality of synthesis output.
    
    Args:
        synthesis: Synthesis output
        
    Returns:
        Quality score (0.0-1.0)
    """
    if not synthesis:
        return 0.3
    
    # Base score
    score = 0.5
    
    # Check for presence of synthesis sections
    synthesis_content = synthesis.get("synthesis", {})
    if not synthesis_content:
        return 0.3
    
    # Count sections with content
    sections_with_content = sum(
        1 for section in synthesis_content.values()
        if isinstance(section, dict) and 
        section.get("content") and 
        len(section.get("content", "")) > 50
    )
    
    # Add points for section coverage
    section_ratio = sections_with_content / max(1, len(synthesis_content))
    score += section_ratio * 0.2
    
    # Count sections with citations
    sections_with_citations = sum(
        1 for section in synthesis_content.values()
        if isinstance(section, dict) and 
        section.get("citations") and
        len(section.get("citations", [])) > 0
    )
    
    # Add points for citation coverage
    citation_ratio = sections_with_citations / max(1, len(synthesis_content))
    score += citation_ratio * 0.15
    
    # Count sections with statistics
    sections_with_stats = sum(
        1 for section in synthesis_content.values()
        if isinstance(section, dict) and 
        section.get("statistics") and
        len(section.get("statistics", [])) > 0
    )
    
    # Add points for statistics coverage
    stats_ratio = sections_with_stats / max(1, len(synthesis_content))
    score += stats_ratio * 0.15
    
    return min(1.0, score)
</file>

<file path="src/react_agent/utils/validations.py">
import re
from urllib.parse import urlparse

def is_valid_url(url: str) -> bool:
    """
    Validate if a URL is properly formatted and not a fabricated example URL.
    """
    if not url:
        return False

    # Check for example/fake URLs
    fake_url_patterns = [
        r'example\.com',
        r'sample\.org',
        r'test\.net',
        r'domain\.com',
        r'yourcompany\.com',
        r'acme\.com',
        r'widget\.com',
        r'placeholder\.net',
        r'company\.org'
    ]

    for pattern in fake_url_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            return False

    # Basic URL validation
    try:
        result = urlparse(url)
        return all([result.scheme in ('http', 'https'), result.netloc])
    except Exception:
        return False
</file>

<file path="src/react_agent/__init__.py">
"""React Agent.

This module defines a custom reasoning and action agent graph.
It invokes tools in a simple loop.
"""

from react_agent.graphs.graph import graph

__all__ = ["graph"]
</file>

<file path="src/react_agent/configuration.py">
"""Define the configurable parameters for the agent."""

from __future__ import annotations

import os
from dataclasses import dataclass, field, fields
from typing import Annotated, Optional

from langchain_core.runnables import RunnableConfig, ensure_config

from react_agent.prompts import templates


@dataclass(kw_only=True)
class Configuration:
    """The configuration for the agent."""

    system_prompt: str = field(
        default=templates.SYSTEM_PROMPT,
        metadata={
            "description": "The system prompt to use for the agent's interactions. "
            "This prompt sets the context and behavior for the agent."
        },
    )

    model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
        default="openai/gpt-4o",
        metadata={
            "description": "The name of the language model to use for the agent's main interactions. "
            "Should be in the form: provider/model-name."
        },
    )

    max_search_results: int = field(
        default=10,
        metadata={
            "description": "The maximum number of search results to return for each search query."
        },
    )

    firecrawl_api_key: Optional[str] = field(
        default_factory=lambda: os.getenv("FIRECRAWL_API_KEY"),
        metadata={
            "description": "API key for the FireCrawl service. Required for web scraping and crawling."
        },
    )

    firecrawl_url: Optional[str] = field(
        default_factory=lambda: os.getenv("FIRECRAWL_URL"),
        metadata={
            "description": "Base URL for the FireCrawl service. Use this for self-hosted instances."
        },
    )

    jina_api_key: Optional[str] = field(
        default_factory=lambda: os.getenv("JINA_API_KEY"),
        metadata={
            "description": "API key for the Jina AI service. Required for web search and summarization."
        },
    )

    jina_url: Optional[str] = field(
        default_factory=lambda: os.getenv("JINA_URL", "https://s.jina.ai"),
        metadata={
            "description": "Base URL for the Jina AI service. Use this for self-hosted instances."
        },
    )

    @classmethod
    def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> Configuration:
        """Create a Configuration instance from a RunnableConfig object."""
        config = ensure_config(config)
        configurable = config.get("configurable") or {}
        
        # Get environment variables for FireCrawl
        env_config = {
            "firecrawl_api_key": os.getenv("FIRECRAWL_API_KEY"),
            "firecrawl_url": os.getenv("FIRECRAWL_URL"),
        }
        
        # Merge environment variables with configurable, giving priority to configurable
        merged_config = {**env_config, **configurable}
        
        _fields = {f.name for f in fields(cls) if f.init}
        return cls(**{k: v for k, v in merged_config.items() if k in _fields})
</file>

<file path="src/react_agent/state.py">
"""Define the state structures for the agent."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Sequence

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from langgraph.managed import IsLastStep
from typing_extensions import Annotated


@dataclass
class InputState:
    """Defines the input state for the agent, representing a narrower interface to the outside world.

    This class is used to define the initial state and structure of incoming data.
    """

    messages: Annotated[Sequence[AnyMessage], add_messages] = field(
        default_factory=list
    )
    """
    Messages tracking the primary execution state of the agent.

    Typically accumulates a pattern of:
    1. HumanMessage - user input
    2. AIMessage with .tool_calls - agent picking tool(s) to use to collect information
    3. ToolMessage(s) - the responses (or errors) from the executed tools
    4. AIMessage without .tool_calls - agent responding in unstructured format to the user
    5. HumanMessage - user responds with the next conversational turn

    Steps 2-5 may repeat as needed.

    The `add_messages` annotation ensures that new messages are merged with existing ones,
    updating by ID to maintain an "append-only" state unless a message with the same ID is provided.
    """


@dataclass
class State(InputState):
    """Represents the complete state of the agent, extending InputState with additional attributes.

    This class can be used to store any information needed throughout the agent's lifecycle.
    """

    is_last_step: IsLastStep = field(default=False)
    """
    Indicates whether the current step is the last one before the graph raises an error.

    This is a 'managed' variable, controlled by the state machine rather than user code.
    It is set to 'True' when the step count reaches recursion_limit - 1.
    """

    # Additional attributes can be added here as needed.
    # Common examples include:
    # retrieved_documents: List[Document] = field(default_factory=list)
    # extracted_entities: Dict[str, Any] = field(default_factory=dict)
    # api_connections: Dict[str, Any] = field(default_factory=dict)
</file>

<file path="tests/integration_tests/__init__.py">
"""Define any integration tests you want in this directory."""
</file>

<file path="tests/integration_tests/test_graph.py">
import pytest
from langsmith import unit

from react_agent import graph


@pytest.mark.asyncio
@unit
async def test_react_agent_simple_passthrough() -> None:
    res = await graph.ainvoke(
        {"messages": [("user", "Who is the founder of LangChain?")]},
        {"configurable": {"system_prompt": "You are a helpful AI assistant."}},
    )

    assert "harrison" in str(res["messages"][-1].content).lower()
</file>

<file path="tests/unit_tests/__init__.py">
"""Define any unit tests you may want in this directory."""
</file>

<file path="tests/unit_tests/test_configuration.py">
from react_agent.configuration import Configuration


def test_configuration_empty() -> None:
    Configuration.from_runnable_config({})
</file>

<file path="tests/unit_tests/test_extraction.py">
import json
import pytest
import sys
from react_agent.utils.extraction import safe_json_parse, find_json_object


def test_find_json_object_basic():
    """Test basic JSON object extraction."""
    text = "Some text before {\"key\": \"value\"} and after"
    result = find_json_object(text)
    assert result == "{\"key\": \"value\"}"
    
    # Test with JSON array
    text = "Some text before [1, 2, 3] and after"
    result = find_json_object(text)
    assert result == "[1, 2, 3]"


def test_find_json_object_nested():
    """Test nested JSON object extraction."""
    text = "Text {\"outer\": {\"inner\": \"value\"}} more"
    result = find_json_object(text)
    assert result == "{\"outer\": {\"inner\": \"value\"}}"
    
    # Test with nested arrays
    text = "Text {\"items\": [1, [2, 3], 4]} more"
    result = find_json_object(text)
    assert result == "{\"items\": [1, [2, 3], 4]}"


def test_find_json_object_multiple():
    """Test extraction with multiple JSON objects."""
    text = "{\"first\": 1} then {\"second\": 2}"
    result = find_json_object(text)
    # Should find the first complete object
    assert result == "{\"first\": 1}"


def test_find_json_object_unbalanced():
    """Test with unbalanced braces."""
    text = "Unbalanced {\"key\": \"value\"} and {\"broken\": \"value"
    result = find_json_object(text)
    assert result == "{\"key\": \"value\"}"


def test_safe_json_parse_direct():
    """Test direct JSON parsing."""
    json_str = json.dumps({"key": "value"})
    result = safe_json_parse(json_str, "test_category")
    assert result == {"key": "value"}


def test_safe_json_parse_embedded():
    """Test parsing JSON embedded in text."""
    text = "Some text before {\"key\": \"value\"} and after"
    result = safe_json_parse(text, "test_category")
    assert result == {"key": "value"}


def test_safe_json_parse_array():
    """Test parsing JSON array."""
    text = "Array: [1, 2, 3]"
    result = safe_json_parse(text, "test_category")
    assert result == [1, 2, 3]


def test_safe_json_parse_single_quotes():
    """Test parsing with single quotes."""
    text = "{'key': 'value'}"
    result = safe_json_parse(text, "test_category")
    assert result == {"key": "value"}


def test_safe_json_parse_trailing_comma():
    """Test parsing with trailing comma."""
    text = "{\"items\": [1, 2, 3,]}"
    result = safe_json_parse(text, "test_category")
    assert result == {"items": [1, 2, 3]}


def test_safe_json_parse_markdown():
    """Test parsing JSON in markdown code block."""
    text = "```json\n{\"key\": \"value\"}\n```"
    result = safe_json_parse(text, "test_category")
    assert result == {"key": "value"}



def test_safe_json_parse_unquoted_keys():
    """Test parsing with unquoted keys."""
    text = "{key: \"value\"}"
    result = safe_json_parse(text, "test_category")
    assert result == {"key": "value"}


def test_safe_json_parse_quoted_json_object():
    """Test parsing JSON object with quotes around the entire object."""
    text = "'{\"key\": \"value\"}'"
    result = safe_json_parse(text, "test_category")
    assert result == {"key": "value"}
    
    # Test with single quotes around the entire object
    text = "'{ \"key\": \"value\" }'"
    result = safe_json_parse(text, "test_category")
    assert result == {"key": "value"}
    
    # Test with double quotes around the entire object
    text = "\"{ \\\"key\\\": \\\"value\\\" }\""
    
    # Print the exact string for debugging
    print(f"\nExact string: {repr(text)}", file=sys.stderr)
    
    result = safe_json_parse(text, "test_category")
    assert result == {"key": "value"}, f"Expected {{'key': 'value'}} but got {repr(result)}"
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
uv.lock

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST
.langgraph_api/
# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/
.qodo
</file>

<file path=".repomixignore">
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
.venv/
src/react_agent/tools/tavily.py
src/react_agent/tools/firecrawl.py
src/react_agent/graphs/error.py
src/react_agent/graphs/graph.py
src/react_agent/graphs/analysis.py
tests/cassettes/**
</file>

<file path="langgraph.json">
{
  "dockerfile_lines": [],
  "graphs": {
    "agent": "src/react_agent/graphs/graph.py:graph",
    "analysis": "src/react_agent/graphs/analysis.py:graph",
    "research": "src/react_agent/graphs/research.py:research_graph"
  },
  "env": ".env",
  "python_version": "3.11",
  "dependencies": [
    "."
  ]
}
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 LangChain

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="Makefile">
.PHONY: all format lint test tests test_watch integration_tests docker_tests help extended_tests

# Default target executed when no arguments are given to make.
all: help

# Define a variable for the test file path.
TEST_FILE ?= tests/unit_tests/

test:
	python -m pytest $(TEST_FILE)

test_watch:
	python -m ptw --snapshot-update --now . -- -vv tests/unit_tests

test_profile:
	python -m pytest -vv tests/unit_tests/ --profile-svg

extended_tests:
	python -m pytest --only-extended $(TEST_FILE)


######################
# LINTING AND FORMATTING
######################

# Define a variable for Python and notebook files.
PYTHON_FILES=src/
MYPY_CACHE=.mypy_cache
lint format: PYTHON_FILES=.
lint_diff format_diff: PYTHON_FILES=$(shell git diff --name-only --diff-filter=d main | grep -E '\.py$$|\.ipynb$$')
lint_package: PYTHON_FILES=src
lint_tests: PYTHON_FILES=tests
lint_tests: MYPY_CACHE=.mypy_cache_test

lint lint_diff lint_package lint_tests:
	python -m ruff check .
	[ "$(PYTHON_FILES)" = "" ] || python -m ruff format $(PYTHON_FILES) --diff
	[ "$(PYTHON_FILES)" = "" ] || python -m ruff check --select I $(PYTHON_FILES)
	[ "$(PYTHON_FILES)" = "" ] || python -m mypy --strict $(PYTHON_FILES)
	[ "$(PYTHON_FILES)" = "" ] || mkdir -p $(MYPY_CACHE) && python -m mypy --strict $(PYTHON_FILES) --cache-dir $(MYPY_CACHE)

format format_diff:
	ruff format $(PYTHON_FILES)
	ruff check --select I --fix $(PYTHON_FILES)

spell_check:
	codespell --toml pyproject.toml

spell_fix:
	codespell --toml pyproject.toml -w

######################
# HELP
######################

help:
	@echo '----'
	@echo 'format                       - run code formatters'
	@echo 'lint                         - run linters'
	@echo 'test                         - run unit tests'
	@echo 'tests                        - run unit tests'
	@echo 'test TEST_FILE=<test_file>   - run all tests in file'
	@echo 'test_watch                   - run unit tests in watch mode'
</file>

<file path="pyproject.toml">
[project]
name = "react-agent"
version = "0.0.1"
description = "Starter template for making a custom Reasoning and Action agent (using tool calling) in LangGraph."
authors = [
    { name = "William Fu-Hinthorn", email = "13333726+hinthornw@users.noreply.github.com" },
]
readme = "README.md"
license = { text = "MIT" }
requires-python = ">=3.11,<4.0"
dependencies = [
    "langgraph>=0.2.6",
    "langchain-openai>=0.1.22",
    "langchain-anthropic>=0.1.23",
    "langchain>=0.2.14",
    "langchain-fireworks>=0.1.7",
    "python-dotenv>=1.0.1",
    "langchain-community>=0.2.17",
    "tavily-python>=0.4.0",
    "pandas",
    "firecrawl-py",
    "rich"
]


[project.optional-dependencies]
dev = ["mypy>=1.11.1", "ruff>=0.6.1"]

[build-system]
requires = ["setuptools>=73.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = ["langgraph.templates.react_agent", "react_agent"]
[tool.setuptools.package-dir]
"langgraph.templates.react_agent" = "src/react_agent"
"react_agent" = "src/react_agent"


[tool.setuptools.package-data]
"*" = ["py.typed"]

[tool.ruff]
lint.select = [
    "E",    # pycodestyle
    "F",    # pyflakes
    "I",    # isort
    "D",    # pydocstyle
    "D401", # First line should be in imperative mood
    "T201",
    "UP",
]
lint.ignore = [
    "UP006",
    "UP007",
    # We actually do want to import from typing_extensions
    "UP035",
    # Relax the convention by _not_ requiring documentation for every function parameter.
    "D417",
    "E501",
]
[tool.ruff.lint.per-file-ignores]
"tests/*" = ["D", "UP"]
[tool.ruff.lint.pydocstyle]
convention = "google"

[dependency-groups]
dev = [
    "langgraph-cli[inmem]>=0.1.71",
]
</file>

<file path="README.md">
# LangGraph ReAct Agent Template

[![CI](https://github.com/langchain-ai/react-agent/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/langchain-ai/react-agent/actions/workflows/unit-tests.yml)
[![Integration Tests](https://github.com/langchain-ai/react-agent/actions/workflows/integration-tests.yml/badge.svg)](https://github.com/langchain-ai/react-agent/actions/workflows/integration-tests.yml)
[![Open in - LangGraph Studio](https://img.shields.io/badge/Open_in-LangGraph_Studio-00324d.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4NS4zMzMiIGhlaWdodD0iODUuMzMzIiB2ZXJzaW9uPSIxLjAiIHZpZXdCb3g9IjAgMCA2NCA2NCI+PHBhdGggZD0iTTEzIDcuOGMtNi4zIDMuMS03LjEgNi4zLTYuOCAyNS43LjQgMjQuNi4zIDI0LjUgMjUuOSAyNC41QzU3LjUgNTggNTggNTcuNSA1OCAzMi4zIDU4IDcuMyA1Ni43IDYgMzIgNmMtMTIuOCAwLTE2LjEuMy0xOSAxLjhtMzcuNiAxNi42YzIuOCAyLjggMy40IDQuMiAzLjQgNy42cy0uNiA0LjgtMy40IDcuNkw0Ny4yIDQzSDE2LjhsLTMuNC0zLjRjLTQuOC00LjgtNC44LTEwLjQgMC0xNS4ybDMuNC0zLjRoMzAuNHoiLz48cGF0aCBkPSJNMTguOSAyNS42Yy0xLjEgMS4zLTEgMS43LjQgMi41LjkuNiAxLjcgMS44IDEuNyAyLjcgMCAxIC43IDIuOCAxLjYgNC4xIDEuNCAxLjkgMS40IDIuNS4zIDMuMi0xIC42LS42LjkgMS40LjkgMS41IDAgMi43LS41IDIuNy0xIDAtLjYgMS4xLS44IDIuNi0uNGwyLjYuNy0xLjgtMi45Yy01LjktOS4zLTkuNC0xMi4zLTExLjUtOS44TTM5IDI2YzAgMS4xLS45IDIuNS0yIDMuMi0yLjQgMS41LTIuNiAzLjQtLjUgNC4yLjguMyAyIDEuNyAyLjUgMy4xLjYgMS41IDEuNCAyLjMgMiAyIDEuNS0uOSAxLjItMy41LS40LTMuNS0yLjEgMC0yLjgtMi44LS44LTMuMyAxLjYtLjQgMS42LS41IDAtLjYtMS4xLS4xLTEuNS0uNi0xLjItMS42LjctMS43IDMuMy0yLjEgMy41LS41LjEuNS4yIDEuNi4zIDIuMiAwIC43LjkgMS40IDEuOSAxLjYgMi4xLjQgMi4zLTIuMy4yLTMuMi0uOC0uMy0yLTEuNy0yLjUtMy4xLTEuMS0zLTMtMy4zLTMtLjUiLz48L3N2Zz4=)](https://langgraph-studio.vercel.app/templates/open?githubUrl=https://github.com/langchain-ai/react-agent)

This template showcases a [ReAct agent](https://arxiv.org/abs/2210.03629) implemented using [LangGraph](https://github.com/langchain-ai/langgraph), designed for [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio). ReAct agents are uncomplicated, prototypical agents that can be flexibly extended to many tools.

![Graph view in LangGraph studio UI](./static/studio_ui.png)

The core logic, defined in `src/react_agent/graph.py`, demonstrates a flexible ReAct agent that iteratively reasons about user queries and executes actions, showcasing the power of this approach for complex problem-solving tasks.

## What it does

The ReAct agent:

1. Takes a user **query** as input
2. Reasons about the query and decides on an action
3. Executes the chosen action using available tools
4. Observes the result of the action
5. Repeats steps 2-4 until it can provide a final answer

By default, it's set up with a basic set of tools, but can be easily extended with custom tools to suit various use cases.

## Getting Started

Assuming you have already [installed LangGraph Studio](https://github.com/langchain-ai/langgraph-studio?tab=readme-ov-file#download), to set up:

1. Create a `.env` file.

```bash
cp .env.example .env
```

2. Define required API keys in your `.env` file.

The primary [search tool](./src/react_agent/tools.py) [^1] used is [Tavily](https://tavily.com/). Create an API key [here](https://app.tavily.com/sign-in).

<!--
Setup instruction auto-generated by `langgraph template lock`. DO NOT EDIT MANUALLY.
-->

### Setup Model

The defaults values for `model` are shown below:

```yaml
model: anthropic/claude-3-5-sonnet-20240620
```

Follow the instructions below to get set up, or pick one of the additional options.

#### Anthropic

To use Anthropic's chat models:

1. Sign up for an [Anthropic API key](https://console.anthropic.com/) if you haven't already.
2. Once you have your API key, add it to your `.env` file:

```
ANTHROPIC_API_KEY=your-api-key
```
#### OpenAI

To use OpenAI's chat models:

1. Sign up for an [OpenAI API key](https://platform.openai.com/signup).
2. Once you have your API key, add it to your `.env` file:
```
OPENAI_API_KEY=your-api-key
```





<!--
End setup instructions
-->


3. Customize whatever you'd like in the code.
4. Open the folder LangGraph Studio!

## How to customize

1. **Add new tools**: Extend the agent's capabilities by adding new tools in [tools.py](./src/react_agent/tools.py). These can be any Python functions that perform specific tasks.
2. **Select a different model**: We default to Anthropic's Claude 3 Sonnet. You can select a compatible chat model using `provider/model-name` via configuration. Example: `openai/gpt-4-turbo-preview`.
3. **Customize the prompt**: We provide a default system prompt in [prompts.py](./src/react_agent/prompts.py). You can easily update this via configuration in the studio.

You can also quickly extend this template by:

- Modifying the agent's reasoning process in [graph.py](./src/react_agent/graph.py).
- Adjusting the ReAct loop or adding additional steps to the agent's decision-making process.

## Development

While iterating on your graph, you can edit past state and rerun your app from past states to debug specific nodes. Local changes will be automatically applied via hot reload. Try adding an interrupt before the agent calls tools, updating the default system message in `src/react_agent/configuration.py` to take on a persona, or adding additional nodes and edges!

Follow up requests will be appended to the same thread. You can create an entirely new thread, clearing previous history, using the `+` button in the top right.

You can find the latest (under construction) docs on [LangGraph](https://github.com/langchain-ai/langgraph) here, including examples and other references. Using those guides can help you pick the right patterns to adapt here for your use case.

LangGraph Studio also integrates with [LangSmith](https://smith.langchain.com/) for more in-depth tracing and collaboration with teammates.

[^1]: https://python.langchain.com/docs/concepts/#tools

<!--
Configuration auto-generated by `langgraph template lock`. DO NOT EDIT MANUALLY.
{
  "config_schemas": {
    "agent": {
      "type": "object",
      "properties": {
        "model": {
          "type": "string",
          "default": "anthropic/claude-3-5-sonnet-20240620",
          "description": "The name of the language model to use for the agent's main interactions. Should be in the form: provider/model-name.",
          "environment": [
            {
              "value": "anthropic/claude-1.2",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-2.0",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-2.1",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-3-5-sonnet-20240620",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-3-haiku-20240307",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-3-opus-20240229",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-3-sonnet-20240229",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-instant-1.2",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-0125",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-0301",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-0613",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-1106",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-16k",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-16k-0613",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-0125-preview",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-0314",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-0613",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-1106-preview",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-32k",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-32k-0314",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-32k-0613",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-turbo",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-turbo-preview",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-vision-preview",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4o",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4o-mini",
              "variables": "OPENAI_API_KEY"
            }
          ]
        }
      },
      "environment": [
        "TAVILY_API_KEY"
      ]
    }
  }
}
-->
</file>

<file path="repomix.config.json">
{
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

</files>
