This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.cursor/
  backups/
    rules_backup_20250324_131312/
      default.mdc
  rules/
    aiohttp.mdc
    click.mdc
    default.mdc
    langchain.mdc
    langgraph.mdc
    mypy.mdc
    numpy.mdc
    openai.mdc
    pandas.mdc
    pillow.mdc
    pytest.mdc
    rich.mdc
    tqdm.mdc
.github/
  workflows/
    integration-tests.yml
    unit-tests.yml
src/
  react_agent/
    graphs/
      research.py
    prompts/
      __init__.py
      analysis.py
      market.py
      query.py
      reflection.py
      research.py
      synthesis.py
      templates.py
      validation.py
    tools/
      jina.py
    utils/
      __init__.py
      cache.py
      content.py
      defaults.py
      extraction.py
      llm.py
      logging.py
      statistics.py
      validations.py
    __init__.py
    configuration.py
    state.py
.gitignore
.repomixignore
langgraph.json
LICENSE
Makefile
pyproject.toml
README.md
rules.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/backups/rules_backup_20250324_131312/default.mdc">
---
description:
globs:
alwaysApply: true
---

# Your rule content

- You can @ files here
- You can use markdown but dont have to
</file>

<file path=".cursor/rules/aiohttp.mdc">
---
description: Comprehensive guide for aiohttp development covering code organization, performance, security, testing, and deployment best practices. Provides actionable guidance for developers to build robust and maintainable aiohttp applications.
globs: **/*.py
---
# Aiohttp Best Practices

This document provides a comprehensive guide to aiohttp development, covering code organization, performance, security, testing, and deployment.

Library Information:
- Name: aiohttp
- Tags: web, python, http-client, async

## 1. Code Organization and Structure

### 1.1. Directory Structure Best Practices:

*   **Project Root:**
    *   `src/`: Contains the main application code.
        *   `main.py`: Entry point of the application.
        *   `app.py`: Application factory and setup.
        *   `routes.py`: Defines application routes.
        *   `handlers/`: Contains request handlers.
            *   `user_handlers.py`: User-related handlers.
            *   `product_handlers.py`: Product-related handlers.
        *   `middlewares/`: Custom middleware components.
            *   `logging_middleware.py`: Logging middleware.
            *   `auth_middleware.py`: Authentication middleware.
        *   `utils/`: Utility modules.
            *   `db.py`: Database connection and utilities.
            *   `config.py`: Configuration management.
    *   `tests/`: Contains unit and integration tests.
        *   `conftest.py`: Pytest configuration file.
        *   `unit/`: Unit tests.
        *   `integration/`: Integration tests.
    *   `static/`: Static files (CSS, JavaScript, images).
    *   `templates/`: Jinja2 or other template files.
    *   `docs/`: Project documentation.
    *   `requirements.txt`: Python dependencies.
    *   `Dockerfile`: Docker configuration file.
    *   `docker-compose.yml`: Docker Compose configuration.
    *   `.env`: Environment variables.
    *   `README.md`: Project description and instructions.
    *   `.gitignore`: Specifies intentionally untracked files that Git should ignore.
    *   `.cursor/rules/`: Project specific Cursor AI rules.

### 1.2. File Naming Conventions:

*   Python files: `snake_case.py` (e.g., `user_handlers.py`, `database_utils.py`).
*   Class names: `CamelCase` (e.g., `UserHandler`, `DatabaseConnection`).
*   Function names: `snake_case` (e.g., `get_user`, `create_product`).
*   Variables: `snake_case` (e.g., `user_id`, `product_name`).
*   Constants: `UPPER_SNAKE_CASE` (e.g., `DEFAULT_PORT`, `MAX_CONNECTIONS`).

### 1.3. Module Organization:

*   Group related functionality into modules.
*   Use clear and descriptive module names.
*   Avoid circular dependencies.
*   Keep modules focused and concise.
*   Use packages to organize modules into a hierarchical structure.

### 1.4. Component Architecture:

*   **Layered Architecture:** Separate the application into distinct layers (e.g., presentation, business logic, data access).
*   **Microservices Architecture:** Decompose the application into small, independent services.
*   **Hexagonal Architecture (Ports and Adapters):** Decouple the application core from external dependencies.
*   **MVC (Model-View-Controller):** Organize the application into models (data), views (presentation), and controllers (logic).

### 1.5. Code Splitting Strategies:

*   **Route-based splitting:** Load modules based on the requested route.
*   **Feature-based splitting:** Divide the application into feature modules.
*   **Component-based splitting:** Split the application into reusable components.
*   **On-demand loading:** Load modules only when they are needed.
*   **Asynchronous loading:** Use `asyncio.gather` or similar techniques to load modules concurrently.

## 2. Common Patterns and Anti-patterns

### 2.1. Design Patterns:

*   **Singleton:** For managing shared resources like database connections or configuration objects.
*   **Factory:** For creating instances of classes with complex initialization logic.
*   **Strategy:** For implementing different algorithms or behaviors.
*   **Observer:** For implementing event-driven systems.
*   **Middleware:** For handling cross-cutting concerns like logging, authentication, and error handling.

### 2.2. Recommended Approaches for Common Tasks:

*   **Request Handling:** Use request handlers to process incoming requests.
*   **Routing:** Use `aiohttp.web.RouteTableDef` for defining routes.
*   **Middleware:** Implement middleware for request pre-processing and response post-processing.
*   **Data Serialization:** Use `aiohttp.web.json_response` for serializing data to JSON.
*   **Error Handling:** Implement custom error handlers to handle exceptions gracefully.
*   **Session Management:** Use `aiohttp-session` for managing user sessions.
*   **WebSockets:** Utilize `aiohttp.web.WebSocketResponse` for handling WebSocket connections.

### 2.3. Anti-patterns and Code Smells:

*   **Creating a new `ClientSession` for each request:** This is a performance bottleneck. Reuse a single `ClientSession`.
*   **Blocking operations in asynchronous code:** Avoid using blocking operations (e.g., `time.sleep`) in asynchronous code.
*   **Ignoring exceptions:** Always handle exceptions properly to prevent unexpected behavior.
*   **Overusing global variables:** Avoid using global variables as much as possible to maintain code clarity and testability.
*   **Tight coupling:** Decouple components to improve maintainability and reusability.
*   **Hardcoding configuration:** Use environment variables or configuration files to manage configuration settings.

### 2.4. State Management:

*   **Application State:** Store application-level state in the `aiohttp.web.Application` instance.
*   **Request State:** Store request-specific state in the `aiohttp.web.Request` instance.
*   **Session State:** Use `aiohttp-session` to manage user session data.
*   **Database:** Use a database like PostgreSQL, MySQL, or MongoDB to store persistent state.
*   **Redis/Memcached:** Use in-memory data stores for caching frequently accessed data.

### 2.5. Error Handling:

*   **Use `try-except` blocks:** Wrap code that may raise exceptions in `try-except` blocks.
*   **Handle specific exceptions:** Catch specific exception types instead of using a generic `except Exception`.
*   **Log exceptions:** Log exceptions with detailed information for debugging.
*   **Return informative error responses:** Return appropriate HTTP status codes and error messages to the client.
*   **Implement custom error handlers:** Create custom error handlers to handle specific exception types.
*   **Use `aiohttp.web.HTTPException`:** Raise `aiohttp.web.HTTPException` to return HTTP error responses.

## 3. Performance Considerations

### 3.1. Optimization Techniques:

*   **Reuse `ClientSession`:** Always reuse a single `ClientSession` instance for making multiple requests.
*   **Connection Pooling:** aiohttp automatically uses connection pooling, so reuse your session.
*   **Keep-Alive Connections:** Keep-alive connections are enabled by default, reducing connection overhead.
*   **Gzip Compression:** Enable Gzip compression for responses to reduce bandwidth usage.
*   **Caching:** Implement caching for frequently accessed data to reduce database load.
*   **Optimize Database Queries:** Optimize database queries to improve response times.
*   **Use Indexes:** Use indexes in your database tables to speed up queries.
*   **Limit Payload Size:** Keep request and response payloads as small as possible.
*   **Background Tasks:** Use `asyncio.create_task` to offload long-running tasks to the background.
*   **Profiling:** Use profiling tools to identify performance bottlenecks.

### 3.2. Memory Management:

*   **Avoid Memory Leaks:** Ensure that all resources are properly released to prevent memory leaks.
*   **Use Generators:** Use generators to process large datasets in chunks.
*   **Limit Object Creation:** Minimize the creation of objects to reduce memory overhead.
*   **Use Data Structures Efficiently:** Choose appropriate data structures to optimize memory usage.
*   **Garbage Collection:** Understand how Python's garbage collection works and optimize your code accordingly.

### 3.3. Rendering Optimization:

*   **Template Caching:** Cache templates to reduce rendering time.
*   **Minimize Template Logic:** Keep template logic simple and move complex logic to request handlers.
*   **Use Asynchronous Templates:** Use asynchronous template engines like `aiohttp-jinja2`.
*   **Optimize Static Files:** Optimize static files (CSS, JavaScript, images) to reduce page load times.

### 3.4. Bundle Size Optimization:

*   **Minimize Dependencies:** Reduce the number of dependencies in your project.
*   **Tree Shaking:** Use tree shaking to remove unused code from your bundles.
*   **Code Minification:** Minify your code to reduce bundle sizes.
*   **Code Compression:** Compress your code to further reduce bundle sizes.

### 3.5. Lazy Loading:

*   **Lazy Load Modules:** Load modules only when they are needed.
*   **Lazy Load Images:** Load images only when they are visible in the viewport.
*   **Use Asynchronous Loading:** Use `asyncio.gather` or similar techniques to load resources concurrently.

## 4. Security Best Practices

### 4.1. Common Vulnerabilities:

*   **SQL Injection:** Prevent SQL injection by using parameterized queries or an ORM.
*   **Cross-Site Scripting (XSS):** Prevent XSS by escaping user input in templates.
*   **Cross-Site Request Forgery (CSRF):** Prevent CSRF by using CSRF tokens.
*   **Authentication and Authorization Issues:** Implement secure authentication and authorization mechanisms.
*   **Denial-of-Service (DoS) Attacks:** Implement rate limiting and other measures to prevent DoS attacks.
*   **Insecure Dependencies:** Keep your dependencies up to date to prevent vulnerabilities.

### 4.2. Input Validation:

*   **Validate all user input:** Validate all user input to prevent malicious data from entering your application.
*   **Use a validation library:** Use a validation library like `marshmallow` or `voluptuous` to simplify input validation.
*   **Sanitize user input:** Sanitize user input to remove potentially harmful characters.
*   **Limit input length:** Limit the length of input fields to prevent buffer overflows.
*   **Use regular expressions:** Use regular expressions to validate input patterns.

### 4.3. Authentication and Authorization:

*   **Use a strong authentication scheme:** Use a strong authentication scheme like OAuth 2.0 or JWT.
*   **Store passwords securely:** Store passwords securely using a hashing algorithm like bcrypt.
*   **Implement role-based access control (RBAC):** Use RBAC to control access to resources based on user roles.
*   **Use secure cookies:** Use secure cookies to protect session data.
*   **Implement multi-factor authentication (MFA):** Use MFA to add an extra layer of security.

### 4.4. Data Protection:

*   **Encrypt sensitive data:** Encrypt sensitive data at rest and in transit.
*   **Use HTTPS:** Use HTTPS to encrypt communication between the client and the server.
*   **Store data securely:** Store data in a secure location with appropriate access controls.
*   **Regularly back up data:** Regularly back up data to prevent data loss.
*   **Comply with data privacy regulations:** Comply with data privacy regulations like GDPR and CCPA.

### 4.5. Secure API Communication:

*   **Use HTTPS:** Always use HTTPS for API communication.
*   **Implement API authentication:** Use API keys or tokens to authenticate API requests.
*   **Rate limit API requests:** Implement rate limiting to prevent abuse.
*   **Validate API requests:** Validate API requests to prevent malicious data from entering your application.
*   **Log API requests:** Log API requests for auditing and debugging.

## 5. Testing Approaches

### 5.1. Unit Testing:

*   **Test individual components:** Unit tests should test individual components in isolation.
*   **Use a testing framework:** Use a testing framework like `pytest` or `unittest`.
*   **Write clear and concise tests:** Write clear and concise tests that are easy to understand.
*   **Test edge cases:** Test edge cases and boundary conditions.
*   **Use mocks and stubs:** Use mocks and stubs to isolate components under test.

### 5.2. Integration Testing:

*   **Test interactions between components:** Integration tests should test interactions between different components.
*   **Test with real dependencies:** Integration tests should use real dependencies whenever possible.
*   **Test the entire application flow:** Integration tests should test the entire application flow.
*   **Use a testing database:** Use a testing database to isolate integration tests from the production database.

### 5.3. End-to-End Testing:

*   **Test the entire system:** End-to-end tests should test the entire system from end to end.
*   **Use a testing environment:** Use a testing environment that mimics the production environment.
*   **Automate end-to-end tests:** Automate end-to-end tests to ensure that the system is working correctly.
*   **Use a browser automation tool:** Use a browser automation tool like Selenium or Puppeteer.

### 5.4. Test Organization:

*   **Organize tests by module:** Organize tests by module to improve test discovery and maintainability.
*   **Use descriptive test names:** Use descriptive test names that clearly indicate what the test is verifying.
*   **Use test fixtures:** Use test fixtures to set up and tear down test environments.
*   **Use test markers:** Use test markers to categorize tests and run specific test suites.

### 5.5. Mocking and Stubbing:

*   **Use mocks to simulate dependencies:** Use mocks to simulate the behavior of dependencies.
*   **Use stubs to provide predefined responses:** Use stubs to provide predefined responses to API calls.
*   **Use mocking libraries:** Use mocking libraries like `unittest.mock` or `pytest-mock`.
*   **Avoid over-mocking:** Avoid over-mocking, as it can make tests less reliable.

## 6. Common Pitfalls and Gotchas

### 6.1. Frequent Mistakes:

*   **Not handling exceptions properly:** Always handle exceptions to prevent unexpected behavior.
*   **Using blocking operations in asynchronous code:** Avoid using blocking operations in asynchronous code.
*   **Not closing `ClientSession`:** Always close the `ClientSession` to release resources.
*   **Not validating user input:** Always validate user input to prevent security vulnerabilities.
*   **Not using HTTPS:** Always use HTTPS for secure communication.

### 6.2. Edge Cases:

*   **Handling timeouts:** Implement proper timeout handling to prevent requests from hanging indefinitely.
*   **Handling connection errors:** Handle connection errors gracefully to prevent application crashes.
*   **Handling large payloads:** Handle large payloads efficiently to prevent memory issues.
*   **Handling concurrent requests:** Handle concurrent requests properly to prevent race conditions.
*   **Handling Unicode encoding:** Be aware of Unicode encoding issues when processing text data.

### 6.3. Version-Specific Issues:

*   **aiohttp version compatibility:** Be aware of compatibility issues between different aiohttp versions.
*   **asyncio version compatibility:** Be aware of compatibility issues between aiohttp and different asyncio versions.
*   **Python version compatibility:** Be aware of compatibility issues between aiohttp and different Python versions.

### 6.4. Compatibility Concerns:

*   **Compatibility with other libraries:** Be aware of compatibility issues between aiohttp and other libraries.
*   **Compatibility with different operating systems:** Be aware of compatibility issues between aiohttp and different operating systems.
*   **Compatibility with different web servers:** Be aware of compatibility issues between aiohttp and different web servers.

### 6.5. Debugging Strategies:

*   **Use logging:** Use logging to track application behavior and identify issues.
*   **Use a debugger:** Use a debugger to step through code and examine variables.
*   **Use a profiler:** Use a profiler to identify performance bottlenecks.
*   **Use error reporting tools:** Use error reporting tools to track and fix errors in production.
*   **Use a network analyzer:** Use a network analyzer like Wireshark to capture and analyze network traffic.

## 7. Tooling and Environment

### 7.1. Recommended Development Tools:

*   **IDE:** Use an IDE like VS Code, PyCharm, or Sublime Text.
*   **Virtual Environment:** Use a virtual environment to isolate project dependencies.
*   **Package Manager:** Use a package manager like pip or poetry to manage dependencies.
*   **Testing Framework:** Use a testing framework like pytest or unittest.
*   **Linting Tool:** Use a linting tool like pylint or flake8 to enforce code style.
*   **Formatting Tool:** Use a formatting tool like black or autopep8 to format code automatically.

### 7.2. Build Configuration:

*   **Use a build system:** Use a build system like Make or tox to automate build tasks.
*   **Define dependencies in `requirements.txt` or `pyproject.toml`:** Specify all project dependencies in a `requirements.txt` or `pyproject.toml` file.
*   **Use a Dockerfile:** Use a Dockerfile to create a containerized build environment.
*   **Use Docker Compose:** Use Docker Compose to manage multi-container applications.

### 7.3. Linting and Formatting:

*   **Use a consistent code style:** Use a consistent code style throughout the project.
*   **Configure linting tools:** Configure linting tools to enforce code style rules.
*   **Configure formatting tools:** Configure formatting tools to format code automatically.
*   **Use pre-commit hooks:** Use pre-commit hooks to run linters and formatters before committing code.

### 7.4. Deployment:

*   **Use a web server:** Use a web server like Nginx or Apache to serve the application.
*   **Use a process manager:** Use a process manager like Supervisor or systemd to manage the application process.
*   **Use a reverse proxy:** Use a reverse proxy to improve security and performance.
*   **Use a load balancer:** Use a load balancer to distribute traffic across multiple servers.
*   **Use a monitoring system:** Use a monitoring system to track application health and performance.
*   **Standalone Server:** aiohttp.web.run_app(), simple but doesn't utilize all CPU cores.
*   **Nginx + Supervisord:** Nginx prevents attacks, allows utilizing all CPU cores, and serves static files faster.
*   **Nginx + Gunicorn:** Gunicorn launches the app as worker processes, simplifying deployment compared to bare Nginx.

### 7.5. CI/CD Integration:

*   **Use a CI/CD pipeline:** Use a CI/CD pipeline to automate the build, test, and deployment process.
*   **Use a CI/CD tool:** Use a CI/CD tool like Jenkins, GitLab CI, or GitHub Actions.
*   **Run tests in the CI/CD pipeline:** Run tests in the CI/CD pipeline to ensure that code changes don't break the application.
*   **Automate deployment:** Automate deployment to reduce manual effort and improve consistency.

## Additional Best Practices:

*   **Session Management:** Always create a `ClientSession` for making requests and reuse it across multiple requests to benefit from connection pooling. Avoid creating a new session for each request, as this can lead to performance issues.
*   **Error Handling:** Implement robust error handling in your request handlers. Use try-except blocks to manage exceptions, particularly for network-related errors. For example, handle `ConnectionResetError` to manage client disconnections gracefully.
*   **Middleware Usage:** Utilize middleware for cross-cutting concerns such as logging, error handling, and modifying requests/responses. Define middleware functions that accept a request and a handler, allowing you to process requests before they reach your main handler.
*   **Graceful Shutdown:** Implement graceful shutdown procedures for your server to ensure that ongoing requests are completed before the application exits. This can be achieved by registering shutdown signals and cleaning up resources.
*   **Security Practices:** When deploying, consider using a reverse proxy like Nginx for added security and performance. Configure SSL/TLS correctly to secure your application.
*   **Character Set Detection:**  If a response does not include the charset needed to decode the body, use `ClientSession` accepts a `fallback_charset_resolver` parameter which can be used to introduce charset guessing functionality.
*   **Persistent Session:** Use `Cleanup Context` when creating a persistent session.

By adhering to these practices, developers can enhance the reliability, performance, and security of their `aiohttp` applications.
</file>

<file path=".cursor/rules/click.mdc">
---
description: Comprehensive best practices for developing robust and maintainable command-line interfaces using the Click library in Python. Covers code structure, patterns, performance, security, testing, and common pitfalls.
globs: **/*.py
---
# Click CLI Library Best Practices

This document outlines best practices and coding standards for developing command-line interfaces (CLIs) in Python using the Click library. Click is a powerful and user-friendly library that simplifies the creation of beautiful and functional CLIs.

## 1. Code Organization and Structure

### 1.1. Directory Structure

Adopt a well-organized directory structure to enhance maintainability and scalability.


mycli/
├── mycli.py          # Main application entry point (click command group)
├── commands/
│   ├── __init__.py     # Makes 'commands' a Python package
│   ├── cmd_foo.py    # Implementation of 'foo' command
│   ├── cmd_bar.py    # Implementation of 'bar' command
│   └── ...
├── utils/
│   ├── __init__.py     # Utility functions (e.g., file I/O, API calls)
│   ├── helper.py       # Helper functions
│   └── ...
├── models/
│   ├── __init__.py     # Data models (e.g., classes, data structures)
│   ├── data_model.py  # Data models
│   └── ...
├── tests/
│   ├── __init__.py     # Test suite directory
│   ├── test_mycli.py   # Tests for main application
│   ├── test_commands/
│   │   ├── test_cmd_foo.py  # Tests for 'foo' command
│   │   └── ...
│   └── ...
├── README.md         # Project documentation
├── LICENSE           # License information
├── pyproject.toml    # Project configuration (dependencies, build)
└── .gitignore        # Specifies intentionally untracked files that Git should ignore


### 1.2. File Naming Conventions

*   Use descriptive and consistent file names.
*   Main application file: `mycli.py` (or a similar name reflecting the application's purpose).
*   Command modules: `cmd_<command_name>.py` (e.g., `cmd_create.py`, `cmd_update.py`).
*   Utility modules: `helper.py`, `file_utils.py`, etc.
*   Test files: `test_<module_name>.py` (e.g., `test_mycli.py`, `test_cmd_create.py`).

### 1.3. Module Organization

*   **Main Application Module (`mycli.py`):**
    *   Define the main `click.group()` that serves as the entry point for the CLI.
    *   Import and register subcommands from the `commands` package.
    *   Handle global options and context management.
*   **Command Modules (`commands` package):**
    *   Each command module should define a single `click.command()` decorated function.
    *   Command functions should encapsulate the logic for that specific command.
    *   Import necessary utility functions and data models from the `utils` and `models` packages.
*   **Utility Modules (`utils` package):**
    *   Provide reusable functions for common tasks, such as file I/O, API calls, data validation, etc.
    *   Keep utility functions generic and independent of specific commands.
*   **Data Models (`models` package):**
    *   Define classes and data structures to represent the data used by the CLI application.
    *   Use dataclasses or attrs for creating data models with less boilerplate.

### 1.4. Component Architecture

*   **Separation of Concerns:** Clearly separate the CLI interface from the application logic.
*   **Command Layer:** Click commands handle user input, argument parsing, and invoking the underlying application logic.
*   **Service Layer:** Implement the core application logic in separate modules or classes (services).
*   **Data Access Layer:** Encapsulate data access and persistence logic in dedicated modules or classes.

### 1.5. Code Splitting Strategies

*   **Command-Based Splitting:** Split the application into separate modules based on the CLI commands.
*   **Feature-Based Splitting:** Group related commands and utilities into feature-specific modules.
*   **Layered Splitting:** Divide the application into layers (CLI, service, data access) and organize modules accordingly.

## 2. Common Patterns and Anti-patterns

### 2.1. Design Patterns

*   **Command Pattern:**  Each CLI command is represented by a separate class or function, making it easy to add, remove, or modify commands.
*   **Factory Pattern:** Use factories to create objects based on CLI arguments (e.g., creating different types of data exporters based on the `--format` option).
*   **Dependency Injection:** Inject dependencies (e.g., API clients, database connections) into command functions to improve testability and flexibility.
*   **Context Object:** Use Click's context object to store and share data across commands (see examples in the Click documentation).

### 2.2. Recommended Approaches

*   **Configuration Management:** Use environment variables or configuration files to manage application settings.
*   **Logging:** Implement comprehensive logging using the `logging` module to track application behavior and diagnose issues.
*   **Progress Bars:** Use Click's progress bar (`click.progressbar`) to provide visual feedback for long-running tasks.
*   **Interactive Prompts:** Use Click's prompts (`click.prompt`) to gather user input interactively.
*   **File Handling:** Use `click.File` to handle file I/O with automatic error checking and encoding support.
*   **Exception Handling:** Use try-except blocks to gracefully handle exceptions and provide informative error messages to the user.
*   **Testing:** Implement a comprehensive test suite to ensure the CLI application's correctness and reliability.

### 2.3. Anti-patterns and Code Smells

*   **Tight Coupling:** Avoid tight coupling between the CLI interface and the application logic.
*   **Global State:** Minimize the use of global variables and mutable global state.
*   **Hardcoded Values:** Avoid hardcoding values in the code; use configuration files or environment variables instead.
*   **Duplicated Code:** Refactor duplicated code into reusable functions or classes.
*   **Lack of Error Handling:** Neglecting to handle exceptions can lead to unexpected crashes and poor user experience.
*   **Inadequate Testing:** Insufficient testing can result in undetected bugs and regressions.
*   **Overly Complex Commands:** Break down overly complex commands into smaller, more manageable subcommands.

### 2.4. State Management

*   **Context Object:** Use `click.Context.obj` to store and share state between commands. This is the recommended approach for passing data between different parts of your application within a single CLI invocation.
*   **Environment Variables:**  Use environment variables for global configuration settings that rarely change.
*   **Files:** Store persistent state (e.g., user preferences, cached data) in files.
*   **Databases:** For more complex state management, use a database.

### 2.5. Error Handling

*   **`try...except` Blocks:** Wrap potentially failing operations in `try...except` blocks to catch exceptions.
*   **Click's `click.ClickException`:** Raise `click.ClickException` to display user-friendly error messages.  This will ensure that the error message is formatted correctly and displayed to the user in a consistent manner.
*   **Custom Exception Classes:** Define custom exception classes for specific error conditions.
*   **Logging:** Log all errors to aid in debugging and troubleshooting.
*   **Exit Codes:** Use appropriate exit codes to indicate the success or failure of a command.

python
import click

@click.command()
@click.option('--input', '-i', required=True, type=click.Path(exists=True, dir_okay=False, readable=True))
def process_file(input):
    try:
        with open(input, 'r') as f:
            # Process the file content
            content = f.read()
            click.echo(f'Processing file: {input}')
    except FileNotFoundError:
        raise click.ClickException(f'File not found: {input}')
    except IOError:
        raise click.ClickException(f'Could not read file: {input}')
    except Exception as e:
        click.echo(f'An unexpected error occurred: {e}', err=True)
        raise  # Re-raise the exception for higher-level handling or logging

if __name__ == '__main__':
    try:
        process_file()
    except click.ClickException as e:
        click.echo(f'Error: {e}', err=True)


## 3. Performance Considerations

### 3.1. Optimization Techniques

*   **Minimize I/O Operations:** Reduce the number of file I/O and network operations.
*   **Use Efficient Data Structures:** Choose appropriate data structures (e.g., sets, dictionaries) for optimal performance.
*   **Caching:** Cache frequently accessed data to reduce redundant computations.
*   **Profiling:** Use profiling tools to identify performance bottlenecks.

### 3.2. Memory Management

*   **Large Datasets:** When dealing with large datasets, use generators or iterators to process data in chunks.
*   **Object Creation:** Avoid creating unnecessary objects.
*   **Resource Management:** Release resources (e.g., file handles, network connections) when they are no longer needed.

### 3.3. Bundle Size Optimization

*   **Dependency Management:** Use a virtual environment to isolate project dependencies.
*   **Tree Shaking:** Use tools like `pyinstaller` to remove unused code from the final executable.

### 3.4. Lazy Loading

*   **Import on Demand:** Import modules only when they are needed.
*   **Command Loading:** Load command modules only when the corresponding command is invoked.

## 4. Security Best Practices

### 4.1. Common Vulnerabilities

*   **Command Injection:** Prevent command injection by carefully validating user input.
*   **Path Traversal:** Avoid path traversal vulnerabilities by sanitizing file paths.
*   **Sensitive Data Exposure:** Protect sensitive data (e.g., passwords, API keys) by storing them securely and avoiding logging them.

### 4.2. Input Validation

*   **Type Checking:** Use Click's type system to validate the type of user input.
*   **Range Checking:** Validate that numerical inputs fall within acceptable ranges.
*   **Regular Expressions:** Use regular expressions to validate string inputs.
*   **Whitelist:** Validate inputs against a whitelist of allowed values.
*   **Sanitization:** Sanitize user inputs to remove potentially harmful characters.

python
import click
import re

@click.command()
@click.option('--email', '-e', required=True)
def send_email(email):
    # Input validation using regex
    if not re.match(r"^[\w\.-]+@([\w-]+\.)+[\w-]{2,4}$", email):
        raise click.ClickException("Invalid email format.")
    click.echo(f"Sending email to {email}")

if __name__ == '__main__':
    send_email()



### 4.3. Authentication and Authorization

*   **API Keys:** Use API keys to authenticate users and authorize access to resources.
*   **OAuth:** Implement OAuth 2.0 for secure API authentication.
*   **Role-Based Access Control (RBAC):** Implement RBAC to control access to commands and resources based on user roles.

### 4.4. Data Protection

*   **Encryption:** Encrypt sensitive data at rest and in transit.
*   **Hashing:** Hash passwords and other sensitive data using strong hashing algorithms.
*   **Data Masking:** Mask sensitive data in logs and other outputs.

### 4.5. Secure API Communication

*   **HTTPS:** Use HTTPS for all API communication.
*   **TLS/SSL:** Use TLS/SSL certificates to encrypt data in transit.
*   **API Rate Limiting:** Implement API rate limiting to prevent abuse.

## 5. Testing Approaches

### 5.1. Unit Testing

*   **Test Individual Functions:** Write unit tests for individual functions and classes.
*   **Mock Dependencies:** Mock external dependencies (e.g., API calls, database connections) to isolate the code under test.
*   **Test Edge Cases:** Test edge cases and boundary conditions to ensure code robustness.
*   **Use `click.testing.CliRunner`:** Use `click.testing.CliRunner` to simulate CLI invocations and verify the output.

### 5.2. Integration Testing

*   **Test Command Combinations:** Test combinations of commands and options to ensure they work together correctly.
*   **Test Real Dependencies:** Test with real dependencies (e.g., a test database) to ensure the application integrates properly.

### 5.3. End-to-End Testing

*   **Test Full Workflow:** Test the entire CLI application workflow from start to finish.
*   **Automate Tests:** Automate end-to-end tests to ensure continuous integration and continuous delivery.

### 5.4. Test Organization

*   **Separate Test Directory:** Create a separate `tests` directory for all tests.
*   **Mirror Source Structure:** Mirror the source code structure in the test directory.
*   **Descriptive Test Names:** Use descriptive test names to clearly indicate what each test is verifying.

### 5.5. Mocking and Stubbing

*   **`unittest.mock`:** Use the `unittest.mock` module to create mock objects and stubs.
*   **Mock External Dependencies:** Mock external dependencies to isolate the code under test.
*   **Stub Return Values:** Stub return values to control the behavior of mocked objects.

python
import unittest
from unittest.mock import patch
from click.testing import CliRunner
from mycli import cli  # Assuming your main script is named mycli.py

class TestMyCLI(unittest.TestCase):

    def test_hello_world(self):
        runner = CliRunner()
        result = runner.invoke(cli, ['hello', '--name', 'TestUser'])
        self.assertEqual(result.exit_code, 0)
        self.assertEqual(result.output.strip(), 'Hello, TestUser!')

    @patch('mycli.commands.cmd_foo.get_data')  # Assuming cmd_foo.py has a function get_data to mock
    def test_foo_command(self, mock_get_data):
        mock_get_data.return_value = ['data1', 'data2']
        runner = CliRunner()
        result = runner.invoke(cli, ['foo'])
        self.assertEqual(result.exit_code, 0)
        self.assertIn('data1', result.output)
        self.assertIn('data2', result.output)

if __name__ == '__main__':
    unittest.main()


## 6. Common Pitfalls and Gotchas

### 6.1. Frequent Mistakes

*   **Forgetting `@click.command()` or `@click.group()`:**  Commands will not be registered without these decorators.
*   **Incorrect Argument Types:**  Using the wrong type for a `click.argument` or `click.option` can lead to unexpected behavior.
*   **Missing `required=True`:**  Forgetting to specify `required=True` for mandatory arguments or options.
*   **Not Handling Exceptions:**  Failing to handle exceptions can cause the CLI to crash.
*   **Incorrect File Paths:** Providing incorrect file paths to `click.Path` can lead to errors.

### 6.2. Edge Cases

*   **Unicode Handling:**  Ensure proper handling of Unicode characters in input and output.
*   **Large Input Files:**  Handle large input files efficiently to avoid memory issues.
*   **Concurrent Access:**  Handle concurrent access to shared resources (e.g., files, databases) properly.

### 6.3. Version-Specific Issues

*   **Compatibility:**  Check for compatibility issues between Click and other libraries.
*   **Deprecated Features:**  Be aware of deprecated features and plan for migration.

### 6.4. Compatibility Concerns

*   **Python Versions:**  Ensure compatibility with supported Python versions.
*   **Operating Systems:**  Test the CLI application on different operating systems (Windows, macOS, Linux).
*   **Terminal Emulators:**  Be aware of potential compatibility issues with different terminal emulators.

### 6.5. Debugging Strategies

*   **`print()` Statements:**  Use `print()` statements for basic debugging.
*   **Debuggers:**  Use debuggers (e.g., `pdb`, `ipdb`) for more advanced debugging.
*   **Logging:**  Use logging to track application behavior and diagnose issues.
*   **Click's `echo()` Function:** Use Click's `echo()` to ensure consistent output across different platforms and terminal configurations. Also, use `err=True` to distinguish error messages clearly.

## 7. Tooling and Environment

### 7.1. Recommended Tools

*   **Virtual Environments:** Use virtual environments (e.g., `venv`, `virtualenv`) to isolate project dependencies.
*   **Package Manager:** Use `pip` for installing and managing Python packages.
*   **Text Editor/IDE:** Use a text editor or IDE with Python support (e.g., VS Code, PyCharm).
*   **Linting Tools:** Use linting tools (e.g., `flake8`, `pylint`) to enforce coding style and identify potential errors.
*   **Formatting Tools:** Use formatting tools (e.g., `black`, `autopep8`) to automatically format code.
*   **Testing Frameworks:** Use testing frameworks (e.g., `unittest`, `pytest`) to write and run tests.

### 7.2. Build Configuration

*   **`pyproject.toml`:** Use `pyproject.toml` to specify project dependencies and build configuration.
*   **`setup.py`:** Use `setup.py` to define the project's metadata and entry points.
*   **Build Tools:** Use build tools (e.g., `setuptools`, `poetry`) to package and distribute the CLI application.

### 7.3. Linting and Formatting

*   **`flake8`:** Use `flake8` to check for PEP 8 violations and other coding style issues.
*   **`pylint`:** Use `pylint` for more comprehensive code analysis.
*   **`black`:** Use `black` to automatically format code according to PEP 8.
*   **Pre-commit Hooks:** Use pre-commit hooks to automatically run linting and formatting tools before committing code.

### 7.4. Deployment

*   **Package Managers:** Deploy the CLI application using package managers (e.g., `pip`, `conda`).
*   **Executable Bundles:** Create standalone executable bundles using tools like `pyinstaller` or `cx_Freeze`.
*   **Containers:** Deploy the CLI application in containers (e.g., Docker) for portability and scalability.

### 7.5. CI/CD Integration

*   **Continuous Integration:** Integrate the CLI application into a CI/CD pipeline (e.g., Jenkins, GitLab CI, GitHub Actions).
*   **Automated Testing:** Automate testing as part of the CI/CD pipeline.
*   **Automated Deployment:** Automate deployment to staging and production environments.

By following these best practices, developers can create robust, maintainable, and user-friendly CLI applications using the Click library.
</file>

<file path=".cursor/rules/default.mdc">
---
description:
globs:
alwaysApply: true
---

# Your rule content

- You can @ files here
- You can use markdown but dont have to
</file>

<file path=".cursor/rules/langchain.mdc">
---
description: This rule provides best practices for developing LangChain applications, covering code organization, performance, security, testing, and common pitfalls. It aims to improve code quality, maintainability, and overall project success.
globs: **/*.py
---
# LangChain Development Best Practices

This document outlines the best practices for developing LangChain applications to ensure code quality, maintainability, performance, security, and overall project success. These guidelines cover various aspects of development, from code organization to testing and deployment.

## 1. Code Organization and Structure

### 1.1 Directory Structure Best Practices

Adopt a clear and consistent directory structure to improve code discoverability and maintainability. A recommended structure is:


project_root/
├── data/                   # Raw data, processed data, and datasets
├── src/                    # Source code directory
│   ├── components/          # Reusable LangChain components (e.g., custom chains, tools)
│   ├── chains/              # Definitions of LangChain chains
│   ├── agents/              # Agent implementations
│   ├── memory/              # Memory implementations
│   ├── utils/               # Utility functions and modules
│   ├── models/              # Custom model definitions or wrappers
│   ├── callbacks/           # Custom callback handlers
│   ├── vectorstores/        # Vectorstore configurations and connections
│   ├── document_loaders/  # Custom document loaders
│   ├── prompts/             # Prompt templates and management
│   ├── config/              # Configuration files
│   └── main.py              # Entry point of the application
├── tests/                  # Unit and integration tests
├── notebooks/              # Jupyter notebooks for experimentation and documentation
├── docs/                   # Project documentation
├── requirements.txt        # Project dependencies
├── pyproject.toml          # Project metadata and build configuration
└── README.md               # Project README file


### 1.2 File Naming Conventions

Use descriptive and consistent file names:

-   `module_name.py`: For general modules.
-   `component_name.py`: For LangChain components (e.g., `custom_chain.py`).
-   `test_module_name.py`: For test files.
-   Use lowercase and underscores for file names (snake_case).

### 1.3 Module Organization

Organize code into logical modules based on functionality. Each module should have a clear purpose and minimal dependencies.

-   **Cohesion**: Modules should have high cohesion, meaning their elements are closely related.
-   **Coupling**: Modules should have low coupling, meaning they are independent of each other as much as possible.

### 1.4 Component Architecture

Design LangChain applications using a component-based architecture. Components should be reusable, testable, and well-defined.

-   **Chains**: Define chains as reusable components that encapsulate specific workflows.
-   **Agents**: Implement agents as modular entities that interact with the environment using tools.
-   **Memory**: Manage conversation history and state using memory components.
-   **Tools**: Create tools as independent units that perform specific actions.
-   **Callbacks**: Utilize callbacks for logging, monitoring, and custom event handling.

### 1.5 Code Splitting Strategies

Split large files into smaller, manageable chunks to improve readability and maintainability.

-   **Function-level splitting**: Break down large functions into smaller, single-purpose functions.
-   **Class-level splitting**: Divide large classes into smaller, more focused classes.
-   **Module-level splitting**: Separate modules based on functionality to reduce complexity.

## 2. Common Patterns and Anti-patterns

### 2.1 Design Patterns Specific to LangChain

-   **Chain of Responsibility**: Implement chains of operations where each component handles a specific task, passing the result to the next component.
-   **Strategy Pattern**: Use strategy patterns to encapsulate different algorithms or behaviors within interchangeable strategy objects.
-   **Template Method**: Define the skeleton of an algorithm in a base class, allowing subclasses to override specific steps without changing the algorithm's structure.
-   **Factory Pattern**: Use factory patterns to create instances of LangChain components dynamically, based on configuration or runtime conditions.

### 2.2 Recommended Approaches for Common Tasks

-   **Prompt Engineering**: Use prompt templates to manage and reuse prompts. Optimize prompts for clarity, context, and desired output.
-   **Data Loading**: Implement custom data loaders to handle various data sources and formats. Use text splitters to chunk large documents into smaller pieces for retrieval.
-   **Vector Storage**: Use vector stores to store and retrieve embeddings efficiently. Choose the appropriate vector store based on performance, scalability, and cost.
-   **Agent Design**: Design agents with clear objectives, tools, and decision-making logic. Use observation logs to track agent actions and outcomes.
-   **Memory Management**: Implement memory components to maintain conversation history and context. Use sliding window or summarization techniques to manage long conversations.

### 2.3 Anti-patterns and Code Smells to Avoid

-   **God Classes**: Avoid creating large classes that handle too many responsibilities.
-   **Long Methods**: Avoid creating long methods that are difficult to understand and maintain.
-   **Duplicated Code**: Avoid duplicating code across multiple modules. Extract common code into reusable functions or components.
-   **Magic Numbers**: Avoid using magic numbers or hardcoded values. Define constants or configuration variables instead.
-   **Tight Coupling**: Avoid creating tight coupling between modules. Use interfaces and dependency injection to promote loose coupling.

### 2.4 State Management Best Practices

-   **Stateless Components**: Design components to be stateless whenever possible. This improves testability and scalability.
-   **Centralized State**: Manage application state in a centralized location (e.g., a state management class or library).
-   **Immutable State**: Use immutable data structures to prevent unintended side effects and improve predictability.
-   **Explicit State Transitions**: Define explicit state transitions to make state changes clear and traceable.

### 2.5 Error Handling Patterns

-   **Try-Except Blocks**: Use try-except blocks to handle exceptions and prevent application crashes.
-   **Logging**: Log errors and exceptions to facilitate debugging and monitoring.
-   **Custom Exceptions**: Define custom exceptions to represent specific error conditions.
-   **Retry Logic**: Implement retry logic for transient errors (e.g., network timeouts).
-   **Fallback Strategies**: Implement fallback strategies for critical operations to ensure application resilience.

## 3. Performance Considerations

### 3.1 Optimization Techniques

-   **Caching**: Implement caching mechanisms to store frequently accessed data and results.
-   **Batch Processing**: Process data in batches to reduce overhead and improve throughput.
-   **Asynchronous Operations**: Use asynchronous operations to perform non-blocking I/O and improve responsiveness.
-   **Connection Pooling**: Use connection pooling to reuse database connections and reduce latency.
-   **Data Compression**: Compress data to reduce storage space and network bandwidth.
-   **Vectorstore Optimization**: Use efficient vectorstore implementations (e.g., FAISS, Annoy) and optimize indexing parameters for fast retrieval.

### 3.2 Memory Management

-   **Object Pooling**: Use object pooling to reuse objects and reduce memory allocation overhead.
-   **Garbage Collection**: Monitor garbage collection performance and tune parameters to minimize pauses.
-   **Memory Profiling**: Use memory profiling tools to identify memory leaks and optimize memory usage.
-   **Lazy Loading**: Load data on demand to reduce initial memory footprint.
-   **Chunking Large Documents**: Process large documents in smaller chunks to avoid memory overflow.

### 3.3 Rendering Optimization (if applicable for UI components)

-   **Virtualization**: Use virtualization techniques to render large lists efficiently.
-   **Debouncing and Throttling**: Use debouncing and throttling to reduce the frequency of UI updates.
-   **Memoization**: Use memoization to cache expensive rendering calculations.

### 3.4 Bundle Size Optimization (if applicable for web apps)

-   **Code Splitting**: Split code into smaller chunks to reduce initial load time.
-   **Tree Shaking**: Use tree shaking to remove unused code from bundles.
-   **Minification and Compression**: Minify and compress code to reduce bundle size.
-   **Lazy Loading**: Load components and modules on demand.

### 3.5 Lazy Loading Strategies

-   **On-Demand Loading**: Load data or components only when they are needed.
-   **Intersection Observer**: Use the Intersection Observer API to load components when they become visible in the viewport.
-   **Dynamic Imports**: Use dynamic imports to load modules asynchronously.

## 4. Security Best Practices

### 4.1 Common Vulnerabilities and How to Prevent Them

-   **Prompt Injection**: Prevent prompt injection by validating and sanitizing user inputs. Use prompt templates and parameterized queries to avoid direct injection of malicious code.
-   **Data Exfiltration**: Prevent data exfiltration by restricting access to sensitive data and implementing data masking techniques.
-   **Code Execution**: Prevent arbitrary code execution by avoiding the use of `eval()` or similar functions. Use safe alternatives for dynamic code generation.
-   **Denial of Service (DoS)**: Prevent DoS attacks by implementing rate limiting, input validation, and resource quotas.

### 4.2 Input Validation

-   **Whitelisting**: Validate inputs against a whitelist of allowed values or patterns.
-   **Sanitization**: Sanitize inputs to remove or escape potentially harmful characters or code.
-   **Type Checking**: Enforce type checking to ensure that inputs conform to expected data types.
-   **Length Limits**: Enforce length limits to prevent buffer overflows or excessive memory usage.

### 4.3 Authentication and Authorization Patterns

-   **Authentication**: Use strong authentication mechanisms (e.g., multi-factor authentication) to verify user identities.
-   **Authorization**: Implement role-based access control (RBAC) to restrict access to resources based on user roles.
-   **Least Privilege**: Grant users the minimum necessary privileges to perform their tasks.
-   **Secure Storage**: Store sensitive credentials (e.g., API keys) securely using encryption or secret management tools.

### 4.4 Data Protection Strategies

-   **Encryption**: Encrypt sensitive data at rest and in transit.
-   **Data Masking**: Mask sensitive data to protect it from unauthorized access.
-   **Data Anonymization**: Anonymize data to remove personally identifiable information (PII).
-   **Access Logging**: Log all data access events to track and monitor usage.
-   **Data Retention**: Define and enforce data retention policies to minimize the risk of data breaches.

### 4.5 Secure API Communication

-   **HTTPS**: Use HTTPS to encrypt communication between clients and servers.
-   **API Keys**: Protect API keys and other sensitive credentials.
-   **Rate Limiting**: Implement rate limiting to prevent abuse and DoS attacks.
-   **Input Validation**: Validate all API inputs to prevent injection attacks.
-   **Output Encoding**: Encode API outputs to prevent cross-site scripting (XSS) attacks.

## 5. Testing Approaches

### 5.1 Unit Testing Strategies

-   **Test-Driven Development (TDD)**: Write unit tests before writing the code to be tested.
-   **Mocking**: Use mocking to isolate components and test them independently.
-   **Assertion**: Use assertions to verify that the code behaves as expected.
-   **Coverage**: Aim for high code coverage to ensure that all code paths are tested.
-   **Parameterized Tests**: Use parameterized tests to test multiple scenarios with different inputs.

### 5.2 Integration Testing

-   **Component Integration**: Test the integration between components to ensure that they work together correctly.
-   **API Integration**: Test the integration with external APIs to ensure that data is exchanged correctly.
-   **Database Integration**: Test the integration with databases to ensure that data is stored and retrieved correctly.
-   **End-to-End Flows**: Test end-to-end flows to ensure that the application works as a whole.

### 5.3 End-to-End Testing

-   **UI Testing**: Test the user interface to ensure that it is functional and user-friendly.
-   **Functional Testing**: Test the functional requirements of the application to ensure that it meets the specifications.
-   **Performance Testing**: Test the performance of the application to ensure that it is responsive and scalable.
-   **Security Testing**: Test the security of the application to identify and mitigate vulnerabilities.
-   **Accessibility Testing**: Test the accessibility of the application to ensure that it is usable by people with disabilities.

### 5.4 Test Organization

-   **Test Suites**: Organize tests into test suites based on functionality or component.
-   **Test Naming**: Use descriptive test names to make it clear what each test is testing.
-   **Test Data**: Use realistic test data to simulate real-world scenarios.
-   **Test Environment**: Set up a dedicated test environment to isolate tests from production data.

### 5.5 Mocking and Stubbing

-   **Mocking**: Use mocking to replace external dependencies with controlled substitutes.
-   **Stubbing**: Use stubbing to provide predefined responses to external dependencies.
-   **Dependency Injection**: Use dependency injection to make it easier to mock and stub dependencies.
-   **Mocking Frameworks**: Use mocking frameworks (e.g., `unittest.mock`) to simplify the mocking process.

## 6. Common Pitfalls and Gotchas

### 6.1 Frequent Mistakes Developers Make

-   **Hardcoding API Keys**: Storing API keys directly in the code instead of using environment variables.
-   **Ignoring Rate Limits**: Failing to handle API rate limits, leading to errors and service disruptions.
-   **Lack of Input Validation**: Not validating user inputs, making the application vulnerable to prompt injection attacks.
-   **Insufficient Error Handling**: Not handling errors properly, leading to application crashes and data loss.
-   **Over-Reliance on Default Settings**: Using default settings without considering their impact on performance and security.

### 6.2 Edge Cases to Be Aware Of

-   **Empty Inputs**: Handling empty inputs gracefully to prevent errors.
-   **Long Inputs**: Handling long inputs efficiently to avoid performance issues.
-   **Special Characters**: Handling special characters correctly to prevent injection attacks.
-   **Unicode Support**: Ensuring proper Unicode support to handle different languages and character sets.
-   **Network Errors**: Handling network errors gracefully to ensure application resilience.

### 6.3 Version-Specific Issues

-   **API Changes**: Being aware of API changes in different LangChain versions and updating code accordingly.
-   **Compatibility**: Ensuring compatibility between different LangChain components and versions.
-   **Deprecated Features**: Avoiding the use of deprecated features and migrating to their replacements.

### 6.4 Compatibility Concerns

-   **Python Versions**: Ensuring compatibility with different Python versions.
-   **Operating Systems**: Ensuring compatibility with different operating systems (e.g., Windows, macOS, Linux).
-   **Dependency Conflicts**: Resolving dependency conflicts between different libraries.

### 6.5 Debugging Strategies

-   **Logging**: Use logging to track the execution flow and identify errors.
-   **Debugging Tools**: Use debugging tools (e.g., `pdb`) to step through code and inspect variables.
-   **Print Statements**: Use print statements strategically to output debugging information.
-   **Error Messages**: Pay attention to error messages and stack traces to understand the root cause of errors.
-   **Remote Debugging**: Use remote debugging to debug applications running on remote servers.

## 7. Tooling and Environment

### 7.1 Recommended Development Tools

-   **IDE**: Use a powerful IDE (e.g., VS Code, PyCharm) with support for Python and LangChain.
-   **Linters**: Use linters (e.g., `flake8`, `pylint`) to enforce code style and identify potential errors.
-   **Formatters**: Use formatters (e.g., `black`, `autopep8`) to automatically format code according to PEP 8 standards.
-   **Debuggers**: Use debuggers (e.g., `pdb`, `ipdb`) to step through code and inspect variables.
-   **Version Control**: Use Git for version control and collaboration.

### 7.2 Build Configuration

-   **`pyproject.toml`**: Use `pyproject.toml` file to manage project metadata, dependencies, and build configuration.
-   **`requirements.txt`**: Generate and update the `requirements.txt` file to specify project dependencies.
-   **Virtual Environments**: Use virtual environments (`venv`, `conda`) to isolate project dependencies.

### 7.3 Linting and Formatting

-   **Linting**: Configure linters to enforce code style and identify potential errors automatically.
-   **Formatting**: Configure formatters to automatically format code according to PEP 8 standards.
-   **Pre-commit Hooks**: Use pre-commit hooks to run linters and formatters before committing code.

### 7.4 Deployment Best Practices

-   **Containerization**: Use containerization (e.g., Docker) to package the application and its dependencies.
-   **Orchestration**: Use orchestration tools (e.g., Kubernetes) to manage and scale the application.
-   **Infrastructure as Code (IaC)**: Use IaC tools (e.g., Terraform, CloudFormation) to provision and manage infrastructure.
-   **Monitoring**: Implement monitoring and logging to track application performance and identify issues.
-   **Continuous Deployment**: Implement continuous deployment to automate the deployment process.

### 7.5 CI/CD Integration

-   **Continuous Integration (CI)**: Use CI tools (e.g., GitHub Actions, GitLab CI, Jenkins) to automatically build, test, and analyze code.
-   **Continuous Delivery (CD)**: Use CD tools to automatically deploy code to staging or production environments.
-   **Automated Testing**: Integrate automated testing into the CI/CD pipeline to ensure code quality.
-   **Rollback Strategies**: Implement rollback strategies to quickly revert to previous versions in case of deployment failures.

By following these best practices, developers can build robust, scalable, and maintainable LangChain applications that meet the needs of their users and stakeholders.

This comprehensive guide is designed to help developers create high-quality LangChain applications by adhering to industry-standard coding practices and principles.


@file best_practices_python.mdc
@file best_practices_langchain_specific.mdc
</file>

<file path=".cursor/rules/langgraph.mdc">
---
description: This rule file provides comprehensive best practices for developing with LangGraph, covering code organization, performance, security, testing, and common pitfalls. It offers actionable guidance for developers to build robust and maintainable LangGraph applications.
globs: **/*.py
---
# LangGraph Best Practices and Coding Standards

This document outlines best practices and coding standards for developing with LangGraph. It aims to provide clear, actionable guidance for developers to build robust, maintainable, and scalable LangGraph applications. It covers various aspects, including code organization, common patterns, performance considerations, security, testing, and common pitfalls.

## Library Information:

- Name: langgraph
- Tags: ai, ml, llm, python, agent-framework, workflow

## 1. Code Organization and Structure

### 1.1. Directory Structure Best Practices


my_langgraph_project/
├── data/                      # Datasets, knowledge bases, or other data files.
├── src/                       # Source code.
│   ├── components/             # Reusable components (e.g., custom nodes, tools).
│   │   ├── __init__.py
│   │   ├── retrieval.py        # Retrieval-related nodes
│   │   ├── tool_selector.py    # Logic for selecting which tool to use
│   │   └── ...
│   ├── graphs/                 # Graph definitions.
│   │   ├── __init__.py
│   │   ├── customer_support.py  # Example: Customer support graph.
│   │   ├── rag_pipeline.py     # Example: RAG pipeline graph.
│   │   └── ...
│   ├── utils/                  # Utility functions and helpers.
│   │   ├── __init__.py
│   │   ├── config.py           # Configuration loading
│   │   ├── logging.py          # Logging setup
│   │   └── ...
│   ├── schemas/                # Data schemas and type definitions.
│   │   ├── __init__.py
│   │   ├── agent_state.py      # Definition of agent state
│   │   └── ...
│   ├── main.py                 # Entry point of the application.
│   └── ...
├── tests/                     # Unit and integration tests.
│   ├── __init__.py
│   ├── components/             # Tests for custom components.
│   ├── graphs/                 # Tests for graph definitions.
│   ├── utils/                  # Tests for utility functions.
│   └── ...
├── .env                       # Environment variables.
├── requirements.txt           # Project dependencies.
├── pyproject.toml            # Project metadata and build settings
└── README.md                  # Project documentation.


### 1.2. File Naming Conventions

-   Python files: `snake_case.py` (e.g., `customer_support.py`, `retrieval_node.py`).
-   Class names: `PascalCase` (e.g., `CustomerSupportGraph`, `RetrievalNode`).
-   Variables and functions: `snake_case` (e.g., `user_query`, `process_message`).
-   Configuration files: `config.yaml` or `config.json`

### 1.3. Module Organization Best Practices

-   Group related functionalities into modules (e.g., `components`, `graphs`, `utils`).
-   Use `__init__.py` files to make directories packages.
-   Keep modules focused and avoid overly large files.
-   Use relative imports within modules to avoid naming conflicts.

### 1.4. Component Architecture Recommendations

-   Design reusable components for common tasks (e.g., data retrieval, text summarization, tool selection).
-   Create abstract base classes or interfaces for components to promote code reuse and modularity.
-   Use dependency injection to configure components and their dependencies.
-   Adhere to the Single Responsibility Principle (SRP) when designing components.

### 1.5. Code Splitting Strategies

-   Split large graph definitions into smaller, more manageable files.
-   Use lazy loading for components that are not immediately needed.
-   Consider using a module bundler (e.g., esbuild via a plugin) to optimize bundle size for deployment.
-   Break the system down into microservices if warranted by scale and complexity of the overall system. Communicate between microservices using REST or message queues.

## 2. Common Patterns and Anti-patterns

### 2.1. Design Patterns

-   **State Management Pattern**: Encapsulate the agent state in a dedicated class or data structure to ensure consistency and maintainability. Use LangGraph's `StateGraph` to clearly define the state transitions.
-   **Node Pattern**: Define reusable nodes for common tasks such as information retrieval, tool selection, and response generation.
-   **Conditional Edge Pattern**: Use conditional edges to implement branching logic based on the agent state or external factors. This makes the graph more dynamic and responsive.
-   **Retry Pattern:** Implement retry logic within nodes or edges to handle transient errors or API rate limits.  Use exponential backoff to avoid overwhelming failing services.
-   **Orchestration Pattern**: Use LangGraph as the orchestrator for complex agentic workflows, delegating specific tasks to specialized components or services.

### 2.2. Recommended Approaches for Common Tasks

-   **Information Retrieval**: Use LangChain's retrieval chain or custom nodes to fetch relevant information from external sources.
-   **Tool Selection**: Implement a tool selection node that dynamically chooses the appropriate tool based on the user query and agent state.
-   **Response Generation**: Use LangChain's LLMChain or custom nodes to generate responses based on the retrieved information and agent state.
-   **Error Handling:** Implement robust error handling within nodes and edges to gracefully handle exceptions and prevent application crashes. Log all errors and implement monitoring to quickly detect and resolve issues.

### 2.3. Anti-patterns and Code Smells

-   **Monolithic Graphs**: Avoid creating overly complex graphs with too many nodes and edges. Break them down into smaller, more manageable subgraphs.
-   **Hardcoded Values**: Avoid hardcoding values directly into the graph definition. Use configuration files or environment variables to manage configurable parameters.
-   **Ignoring Errors**: Always handle exceptions and log errors appropriately. Ignoring errors can lead to unexpected behavior and difficult-to-debug issues.
-   **Over-Reliance on Global State**: Minimize the use of global state to avoid unintended side effects and make the application more testable.
-   **Lack of Testing**: Thoroughly test all components and graph definitions to ensure they function correctly and handle edge cases.
-   **Infinite Loops:** Ensure the conditional edges within the graph are well-defined to avoid infinite loops.

### 2.4. State Management Best Practices

-   Define a clear and concise agent state schema.
-   Use immutable data structures for the agent state to avoid accidental modifications.
-   Persist the agent state to a database or other storage medium to support long-running conversations or task executions.  Consider using vector databases for efficient retrieval.
-   Implement versioning for the agent state schema to support schema migrations.
-   Use LangGraph's checkpointing feature to save and restore the agent state.

### 2.5. Error Handling Patterns

-   Use try-except blocks to catch exceptions within nodes and edges.
-   Log all errors and warnings with relevant context information.
-   Implement retry logic for transient errors or API rate limits.
-   Use fallback mechanisms to gracefully handle unrecoverable errors.
-   Centralize error handling logic in a dedicated module or class.
-   Implement circuit breaker pattern to prevent cascading failures.

## 3. Performance Considerations

### 3.1. Optimization Techniques

-   **Caching**: Implement caching for frequently accessed data or LLM responses.
-   **Batching**: Batch multiple requests to external APIs to reduce latency.
-   **Asynchronous Operations**: Use asynchronous operations to perform non-blocking I/O and improve responsiveness.
-   **Parallel Processing**: Use multi-threading or multi-processing to parallelize computationally intensive tasks.
-   **Graph Optimization**: Optimize the graph structure to minimize the number of nodes and edges.
-   **Prompt Optimization**: Carefully design prompts to reduce the number of tokens and improve LLM performance.
-   **Reduce LLM calls**: Cache LLM responses when possible. Fine-tune smaller models for specific tasks to reduce latency and cost.

### 3.2. Memory Management Considerations

-   Monitor memory usage to detect memory leaks or excessive memory consumption.
-   Use garbage collection to reclaim unused memory.
-   Avoid storing large objects in the agent state.
-   Use streaming or lazy loading for large data sets.

### 3.3. (Not applicable, as LangGraph doesn't directly handle rendering)

### 3.4. Bundle Size Optimization

-   Use a module bundler (e.g., esbuild) to optimize bundle size.
-   Remove unused code and dependencies.
-   Use code splitting to load only the necessary code for each route or component.
-   Compress the bundle using gzip or Brotli.

### 3.5. Lazy Loading Strategies

-   Use lazy loading for components that are not immediately needed.
-   Load large data sets or models on demand.
-   Implement code splitting to load only the necessary code for each graph or component.

## 4. Security Best Practices

### 4.1. Common Vulnerabilities and Prevention

-   **Prompt Injection**: Prevent prompt injection by carefully validating user inputs and sanitizing prompts.
-   **Data Leaks**: Protect sensitive data by encrypting it at rest and in transit.
-   **Unauthorized Access**: Implement strong authentication and authorization mechanisms to control access to the application and its data.
-   **Denial of Service (DoS)**: Implement rate limiting and request filtering to prevent DoS attacks.
-   **Code Injection**: Avoid executing arbitrary code based on user inputs to prevent code injection vulnerabilities.
-   **API Key Exposure**: Store API keys securely using environment variables or a secrets management system and avoid committing them to version control.

### 4.2. Input Validation

-   Validate all user inputs to prevent prompt injection and other vulnerabilities.
-   Use regular expressions or other validation techniques to ensure that inputs conform to the expected format.
-   Sanitize inputs to remove potentially harmful characters or code.
-   Enforce input length limits to prevent buffer overflows.

### 4.3. Authentication and Authorization

-   Use strong authentication mechanisms (e.g., multi-factor authentication) to verify user identities.
-   Implement role-based access control (RBAC) to restrict access to sensitive data and functionality.
-   Use secure session management to protect user sessions from hijacking.
-   Store passwords securely using hashing and salting.

### 4.4. Data Protection

-   Encrypt sensitive data at rest and in transit.
-   Use secure protocols (e.g., HTTPS) for all API communication.
-   Implement data masking to protect sensitive data from unauthorized access.
-   Regularly back up data to prevent data loss.
-   Comply with relevant data privacy regulations (e.g., GDPR, CCPA).

### 4.5. Secure API Communication

-   Use HTTPS for all API communication.
-   Implement API authentication and authorization.
-   Validate API requests and responses.
-   Use rate limiting to prevent API abuse.
-   Monitor API traffic for suspicious activity.

## 5. Testing Approaches

### 5.1. Unit Testing

-   Write unit tests for all components and utility functions.
-   Use mocking and stubbing to isolate components during testing.
-   Test edge cases and error conditions.
-   Aim for high test coverage.

### 5.2. Integration Testing

-   Write integration tests to verify the interactions between different components.
-   Test the integration with external APIs and services.
-   Use a test environment that closely resembles the production environment.

### 5.3. End-to-End Testing

-   Write end-to-end tests to verify the entire application flow.
-   Use a testing framework such as Playwright or Selenium to automate end-to-end tests.
-   Test the application from the user's perspective.

### 5.4. Test Organization

-   Organize tests into separate directories for unit tests, integration tests, and end-to-end tests.
-   Use descriptive names for test files and test functions.
-   Follow a consistent naming convention for test files and test functions.
-   Use test suites to group related tests.

### 5.5. Mocking and Stubbing

-   Use mocking to replace external dependencies with mock objects.
-   Use stubbing to replace complex components with simplified versions.
-   Use a mocking framework such as pytest-mock to simplify mocking and stubbing.

## 6. Common Pitfalls and Gotchas

### 6.1. Frequent Mistakes

-   **Incorrect State Management**: Failing to properly manage the agent state can lead to inconsistent behavior and incorrect results.
-   **Ignoring Edge Cases**: Neglecting to handle edge cases can cause unexpected errors and application crashes.
-   **Over-Engineering**: Over-complicating the graph definition can make it difficult to understand and maintain.
-   **Insufficient Testing**: Lack of thorough testing can lead to undetected bugs and application failures.
-   **Not Handling Asynchronous Operations Correctly:** LangGraph, and LLMs generally, use async operations, and failing to await these operations will cause unpredictable results.

### 6.2. Edge Cases

-   **Empty User Inputs**: Handle cases where the user provides empty or invalid inputs.
-   **API Rate Limits**: Implement retry logic and rate limiting to handle API rate limits.
-   **Unexpected API Responses**: Handle cases where external APIs return unexpected responses.
-   **Large Data Sets**: Use streaming or lazy loading to handle large data sets.

### 6.3. Version-Specific Issues

-   Be aware of compatibility issues between different versions of LangGraph and LangChain.
-   Consult the documentation and release notes for any version-specific issues.
-   Pin dependencies to specific versions to avoid unexpected behavior.

### 6.4. Compatibility Concerns

-   Ensure compatibility between LangGraph and other technologies used in the application.
-   Test the integration with external APIs and services.
-   Use a consistent set of libraries and dependencies.

### 6.5. Debugging Strategies

-   Use logging to track the execution flow and identify errors.
-   Use a debugger to step through the code and inspect variables.
-   Use a testing framework to write unit tests and integration tests.
-   Use monitoring tools to track performance and identify bottlenecks.
-   Visualize the graph structure to understand the flow of execution.

## 7. Tooling and Environment

### 7.1. Recommended Tools

-   **IDE**: PyCharm, Visual Studio Code with Python extension.
-   **Virtual Environment Manager**: venv, conda.
-   **Testing Framework**: pytest.
-   **Mocking Framework**: pytest-mock.
-   **Linting and Formatting**: pylint, black.
-   **Module Bundler**: esbuild via a plugin.
-   **CI/CD**: GitHub Actions, GitLab CI.
-   **Secrets Management**: HashiCorp Vault, AWS Secrets Manager, Azure Key Vault.
-   **Monitoring**: LangSmith, Prometheus, Grafana.

### 7.2. Build Configuration

-   Use a build system such as `poetry` or `pip` to manage dependencies.
-   Use a configuration file such as `pyproject.toml` or `setup.py` to define project metadata and build settings.

### 7.3. Linting and Formatting

-   Use a linter such as `pylint` or `flake8` to enforce code style and identify potential errors.
-   Use a code formatter such as `black` or `autopep8` to automatically format the code.
-   Configure the linter and formatter to use a consistent set of rules and settings.

### 7.4. Deployment

-   Containerize the application using Docker.
-   Deploy the application to a cloud platform such as AWS, Azure, or Google Cloud.
-   Use a deployment tool such as Terraform or Ansible to automate the deployment process.
-   Implement a blue-green deployment strategy to minimize downtime.

### 7.5. CI/CD

-   Use a CI/CD tool such as GitHub Actions or GitLab CI to automate the testing, building, and deployment processes.
-   Configure the CI/CD pipeline to run tests, linters, and formatters.
-   Use a code review process to ensure code quality and security.

## Conclusion

By following these best practices and coding standards, developers can build robust, maintainable, and scalable LangGraph applications. This will also help with collaboration amongst team members working in the same codebase.
</file>

<file path=".cursor/rules/mypy.mdc">
---
description: This rule file outlines best practices for using mypy in Python projects, emphasizing gradual adoption, consistent configuration, and leveraging advanced features for improved code quality and maintainability. It covers code organization, performance, security, testing, common pitfalls, and tooling.
globs: **/*.py
---
# Mypy Best Practices and Coding Standards

This document outlines the recommended best practices for using Mypy in Python projects. Following these guidelines can lead to more maintainable, robust, and understandable code.

## 1. Gradual Typing and Adoption

- **Start Small:** When introducing Mypy to an existing codebase, focus on a manageable subset of the code first. Choose modules or files that are relatively isolated or self-contained.
- **Iterative Annotation:** Gradually increase the coverage by adding type hints as you modify or add new code. Avoid large-scale refactoring solely for the purpose of adding type hints.
- **`# type: ignore` Strategically:** Use `# type: ignore` comments sparingly to temporarily suppress errors in code that is not yet fully typed. Always include a specific error code when using `# type: ignore` to avoid unintentionally ignoring other errors.  Review these regularly.
- **Prioritize Widely Imported Modules:** Focus on annotating modules that are imported by many other modules. This will provide the greatest benefit in terms of type checking and error detection.

## 2. Consistent Configuration and Integration

- **Standardized Configuration:** Ensure that all developers use the same Mypy configuration and version. Use a `mypy.ini` file or a `pyproject.toml` file to define the project's Mypy settings. This file should be version-controlled.
- **CI Integration:** Integrate Mypy checks into your Continuous Integration (CI) pipeline to catch type errors early in the development process. Use a pre-commit hook to run Mypy before committing code.
- **Editor Integration:** Encourage developers to use Mypy integration in their code editors to get real-time feedback on type errors. Most popular Python editors, such as VS Code, PyCharm, and Sublime Text, have Mypy plugins or extensions.
- **Version Pinning:** Pin the version of mypy in your project's dependencies to ensure consistent behavior across different environments.

## 3. Leveraging Advanced Features

- **Strict Mode:** Utilize Mypy's strict mode (enabled with the `--strict` flag) to catch more potential errors. Strict mode enables a collection of stricter type checking options.
- **`--warn-unused-ignores`:** Use this flag to identify `# type: ignore` comments that are no longer necessary because the corresponding errors have been fixed.
- **`--disallow-untyped-defs`:** Use this flag to require type annotations for all function definitions.
- **`--disallow-incomplete-defs`:** Use this flag to disallow function definitions with incomplete type annotations.
- **`--check-untyped-defs`:** Use this flag to type check function bodies even if the signature lacks type annotations.
- **Protocols and Structural Subtyping:** Take advantage of Mypy's support for protocols and structural subtyping to define flexible interfaces and improve code reusability.
- **Generics:** Use generics to write type-safe code that can work with different types of data.
- **TypedDict:** Use `TypedDict` to define the types of dictionaries with known keys and values. This can help to prevent errors when working with data structures.

## 4. Code Organization and Structure

- **Directory Structure:** Use a well-defined directory structure to organize your code. A common pattern is the `src` layout, where the project's source code is located in a `src` directory.
- **File Naming Conventions:** Follow consistent file naming conventions. Use lowercase letters and underscores for module names (e.g., `my_module.py`).
- **Module Organization:** Organize your code into logical modules. Each module should have a clear purpose and a well-defined interface.
- **Component Architecture:** Design your application using a component-based architecture. Each component should be responsible for a specific task and should have well-defined inputs and outputs.
- **Code Splitting:** Split large modules into smaller, more manageable files. This can improve code readability and maintainability.

## 5. Common Patterns and Anti-patterns

- **Dependency Injection:** Use dependency injection to decouple components and improve testability.
- **Abstract Factories:** Use abstract factories to create families of related objects without specifying their concrete classes.
- **Singletons:** Avoid using singletons excessively. They can make code harder to test and reason about.
- **Global State:** Minimize the use of global state. It can make code harder to understand and debug.
- **Exception Handling:** Use exception handling to gracefully handle errors and prevent the application from crashing. Avoid catching generic exceptions (e.g., `except Exception:`). Catch specific exceptions and handle them appropriately.

## 6. Performance Considerations

- **Profiling:** Use profiling tools to identify performance bottlenecks in your code.
- **Caching:** Use caching to store frequently accessed data and reduce the number of expensive operations.
- **Lazy Loading:** Use lazy loading to defer the loading of resources until they are actually needed.
- **Efficient Data Structures:** Choose appropriate data structures for your data. For example, use sets for membership testing and dictionaries for key-value lookups.
- **Avoid Unnecessary Copying:** Avoid making unnecessary copies of data. This can consume memory and slow down your code.

## 7. Security Best Practices

- **Input Validation:** Validate all user inputs to prevent injection attacks and other security vulnerabilities. Use type annotations to enforce type constraints on function arguments.
- **Authentication and Authorization:** Implement robust authentication and authorization mechanisms to protect your application from unauthorized access.
- **Data Protection:** Protect sensitive data by encrypting it and storing it securely.
- **Secure API Communication:** Use HTTPS to encrypt communication between your application and external APIs.
- **Dependency Management:** Regularly audit your project's dependencies for security vulnerabilities and update them to the latest versions.

## 8. Testing Approaches

- **Unit Testing:** Write unit tests for all components in your application. Unit tests should verify that each component behaves as expected in isolation.
- **Integration Testing:** Write integration tests to verify that different components in your application work together correctly.
- **End-to-End Testing:** Write end-to-end tests to verify that the entire application works as expected from the user's perspective.
- **Test Organization:** Organize your tests into a clear and logical structure. Use separate directories for unit tests, integration tests, and end-to-end tests.
- **Mocking and Stubbing:** Use mocking and stubbing to isolate components during testing and to simulate external dependencies.

## 9. Common Pitfalls and Gotchas

- **`Any` Type:** Avoid using the `Any` type excessively. It effectively disables type checking for the corresponding code.
- **Inconsistent Type Annotations:** Ensure that type annotations are consistent throughout your codebase. Inconsistent type annotations can lead to unexpected errors.
- **Ignoring Errors:** Avoid ignoring Mypy errors without a good reason. Mypy errors usually indicate a real problem in your code.
- **Version Compatibility:** Be aware of compatibility issues between different versions of Mypy and other libraries.
- **Circular Dependencies:** Avoid circular dependencies between modules. They can make code harder to understand and test.

## 10. Tooling and Environment

- **Development Tools:** Use a good code editor with Mypy integration, such as VS Code, PyCharm, or Sublime Text.
- **Build Configuration:** Use a build system, such as `setuptools` or `poetry`, to manage your project's dependencies and build process.
- **Linting and Formatting:** Use a linter, such as `flake8` or `ruff`, to enforce code style and detect potential errors. Use a code formatter, such as `black` or `ruff`, to automatically format your code.
- **Deployment:** Deploy your application to a production environment using a container system, such as Docker, or a cloud platform, such as AWS or Google Cloud.
- **CI/CD:** Integrate Mypy into your CI/CD pipeline to automatically check your code for type errors before deploying it to production.

## 11. Additional Best Practices

- **Use type hints for all function arguments and return values.** This makes it easier to understand what types of data the function expects and returns.
- **Use type aliases to simplify complex type annotations.** This can make your code more readable and maintainable.
- **Use the `typing` module to access advanced type features.** The `typing` module provides a number of useful type-related classes and functions, such as `List`, `Dict`, `Union`, and `Optional`.
- **Use the `reveal_type()` function to inspect the type of an expression.** This can be helpful for debugging type-related issues.
- **Keep your code clean and well-organized.** This makes it easier to understand and maintain.
- **Write clear and concise comments.** This helps others understand your code and how it works.
- **Follow the PEP 8 style guide.** This ensures that your code is consistent and readable.
- **Use a version control system.** This allows you to track changes to your code and collaborate with others.

By following these best practices, you can improve the quality, maintainability, and robustness of your Python code that uses mypy and other Python tools.
</file>

<file path=".cursor/rules/numpy.mdc">
---
description: This rule provides best practices for using NumPy in Python, covering coding standards, performance optimization, security, and testing strategies to enhance code quality and maintainability.
globs: **/*.py
---
# NumPy Library Best Practices and Coding Standards

This document outlines best practices for using the NumPy library in Python for scientific computing, data science, machine learning, and AI development. Adhering to these guidelines will improve code readability, maintainability, performance, security, and overall project success.

## 1. Code Organization and Structure

### 1.1. Directory Structure

*   **`root_directory/`**: The project's root directory.
    *   **`data/`**: Stores datasets (e.g., CSV files, NumPy arrays in `.npy` format).  Consider using subdirectories within `data/` to categorize datasets (e.g., `raw/`, `processed/`).  Use version control (e.g., DVC) for larger datasets.
    *   **`notebooks/`**: Jupyter notebooks for exploration, prototyping, and EDA. Number notebooks sequentially (e.g., `01_data_loading.ipynb`, `02_feature_engineering.ipynb`).  Keep notebooks clean and well-documented; move reusable code to modules.
    *   **`src/`** (or `package_name/`): Contains the Python source code.
        *   **`package_name/`**:  The main package directory.  Use your project's name as the package name.
            *   **`__init__.py`**: Marks the directory as a Python package.  Can be empty or contain package-level initialization code.
            *   **`utils/`**: Utility functions and helper classes.  Split into submodules if necessary (e.g., `utils/data_handling.py`, `utils/math_functions.py`).
            *   **`models/`**:  Model definitions and training scripts (if applicable).  Subdirectories can organize models further (e.g., `models/regression/`, `models/classification/`).
            *   **`preprocessing/`**:  Data preprocessing functions and classes.
            *   **`visualization/`**: Functions for creating plots and visualizations.
            *   **`config.py`**:  Configuration settings (e.g., file paths, hyperparameters). Use a library like `python-box` or `dynaconf` to manage configurations.
            *   **`main.py`**:  The main entry point for the application (if applicable).  Use `if __name__ == "__main__":` to control execution.
    *   **`tests/`**: Unit and integration tests.  Mirror the `src/` structure.  Use `pytest` or `unittest`.
        *   **`tests/utils/`**: Tests for the `utils/` module.
        *   **`tests/models/`**: Tests for the `models/` module.
        *   **`conftest.py`**:  Configuration file for `pytest`.  Can contain fixtures and hooks.
    *   **`docs/`**: Documentation for the project (e.g., using Sphinx).
    *   **`scripts/`**:  Scripts for data downloading, processing, or model deployment.
    *   **`requirements.txt`**:  Lists Python dependencies.  Use `pip freeze > requirements.txt` to generate.
    *   **`.gitignore`**:  Specifies files and directories to ignore in Git (e.g., `data/`, `notebooks/`, `__pycache__/`).
    *   **`README.md`**:  Provides a high-level overview of the project.
    *   **`LICENSE`**:  Specifies the license for the project.
    *   **`setup.py`** or **`pyproject.toml`**: Used for packaging and distribution.

### 1.2. File Naming Conventions

*   **Python files**: Use lowercase with underscores (snake_case): `data_loader.py`, `feature_engineering.py`.
*   **Class names**: Use CamelCase: `DataLoader`, `FeatureEngineer`.
*   **Function and variable names**: Use snake_case: `load_data()`, `feature_names`.
*   **Constants**: Use uppercase with underscores: `MAX_ITERATIONS`, `DEFAULT_LEARNING_RATE`.

### 1.3. Module Organization

*   **Separate concerns**: Each module should have a single, well-defined purpose.
*   **Avoid circular dependencies**:  Design modules to minimize dependencies on each other.
*   **Use relative imports**: Within a package, use relative imports to refer to other modules: `from . import utils`, `from ..models import BaseModel`.
*   **Expose a clear API**: Each module should have a well-defined API (Application Programming Interface) of functions and classes that are intended for external use. Hide internal implementation details using leading underscores (e.g., `_helper_function()`).

### 1.4. Component Architecture

*   **Layered architecture**:  Divide the application into layers (e.g., data access, business logic, presentation) to promote separation of concerns.
*   **Microservices**:  For larger applications, consider breaking them down into smaller, independent microservices.
*   **Design patterns**: Implement relevant design patterns to enhance flexibility and maintainability.

### 1.5. Code Splitting

*   **Functions**: Break down large functions into smaller, more manageable functions.
*   **Classes**: Use classes to encapsulate related data and behavior.
*   **Modules**: Split code into multiple modules based on functionality.
*   **Packages**: Organize modules into packages to create a hierarchical structure.

## 2. Common Patterns and Anti-patterns

### 2.1. Design Patterns

*   **Strategy**: Use a Strategy pattern to encapsulate different algorithms or approaches for a specific task.
*   **Factory**: Employ a Factory pattern to create objects without specifying their concrete classes.
*   **Observer**: Use an Observer pattern to notify dependent objects when the state of an object changes.

### 2.2. Recommended Approaches

*   **Vectorization**:  Leverage NumPy's vectorized operations whenever possible to avoid explicit loops. Vectorization significantly improves performance.
*   **Broadcasting**: Understand and utilize NumPy's broadcasting rules to perform operations on arrays with different shapes.
*   **Ufuncs (Universal Functions)**: Use NumPy's built-in ufuncs (e.g., `np.sin`, `np.exp`, `np.add`) for element-wise operations. Ufuncs are highly optimized.
*   **Masking**: Use boolean masks to select and modify specific elements in arrays.
*   **Views vs. Copies**: Be aware of the difference between views and copies when slicing and manipulating arrays. Modifying a view affects the original array.

### 2.3. Anti-patterns and Code Smells

*   **Explicit loops**: Avoid explicit loops for element-wise operations. Use vectorized operations instead.
*   **Unnecessary copies**: Avoid creating unnecessary copies of arrays. Use views when possible.
*   **Modifying arrays in place**: Be careful when modifying arrays in place, as it can lead to unexpected side effects.
*   **Ignoring broadcasting rules**:  Not understanding broadcasting rules can lead to incorrect results or errors.
*   **Hardcoding array shapes**:  Avoid hardcoding array shapes. Use `array.shape` to get the shape dynamically.
*   **Relying on mutable default arguments**:  Avoid using mutable default arguments (e.g., lists, dictionaries) in function definitions.

### 2.4. State Management

*   **Immutable data structures**: When appropriate, use immutable data structures to prevent accidental modification of data.  Consider libraries like `namedtuple` or `dataclasses` (with `frozen=True`).
*   **Encapsulation**: Encapsulate state within classes to control access and modification.
*   **Dependency injection**: Use dependency injection to decouple components and make them more testable.

### 2.5. Error Handling

*   **Exceptions**: Use exceptions to handle errors and unexpected conditions. Raise specific exceptions (e.g., `ValueError`, `TypeError`) when appropriate.
*   **Assertions**: Use assertions to check for conditions that should always be true. Assertions are helpful for debugging and validating data.
*   **Logging**: Use logging to record errors, warnings, and informational messages. Configure logging to write to a file or stream.
*   **`np.errstate`**: Use `np.errstate` context manager to handle floating-point exceptions (e.g., division by zero, overflow). You can configure how NumPy handles these exceptions (e.g., ignore, warn, raise).

## 3. Performance Considerations

### 3.1. Optimization Techniques

*   **Vectorization**:  As mentioned earlier, prioritize vectorized operations over explicit loops.
*   **Memory Alignment**: NumPy arrays are typically aligned in memory, which can improve performance. Ensure that your data is aligned correctly.
*   **Data Types**: Choose the smallest possible data type that can represent your data. Using smaller data types reduces memory usage and improves performance. For example, use `np.int8` instead of `np.int64` if the values are within the range of `np.int8`.
*   **`np.einsum`**: Use `np.einsum` (Einstein summation) for complex array operations. `np.einsum` can often be more efficient than explicit loops or other NumPy functions.
*   **Numba**: Use Numba to JIT (Just-In-Time) compile NumPy code. Numba can significantly improve the performance of computationally intensive code.
*   **Cython**: Use Cython to write NumPy code in C. Cython allows you to take advantage of C's performance while still using Python's syntax.
*   **BLAS/LAPACK**: NumPy relies on BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra Package) libraries for linear algebra operations. Ensure that you are using an optimized BLAS/LAPACK implementation (e.g., OpenBLAS, MKL).
*   **`np.fft`**: For FFT (Fast Fourier Transform) operations, use the functions provided in NumPy's `np.fft` module. They're usually highly optimized.

### 3.2. Memory Management

*   **Avoid creating large temporary arrays**:  Minimize the creation of large temporary arrays, especially within loops.  Use in-place operations when possible.
*   **`np.empty`**: Use `np.empty` to create uninitialized arrays when you don't need to initialize the values immediately.  This can be faster than `np.zeros` or `np.ones`.
*   **`np.memmap`**: Use `np.memmap` to create memory-mapped arrays. Memory-mapped arrays allow you to work with large datasets that don't fit in memory.
*   **Garbage collection**: Be mindful of Python's garbage collection. Explicitly delete large arrays when they are no longer needed to free up memory: `del my_array`.

### 3.3. Rendering Optimization (If Applicable)

*   This is mainly relevant when NumPy arrays are used for image processing or other visualization tasks. Libraries like Matplotlib, Seaborn, or specialized rendering engines may offer specific optimizations for handling NumPy arrays.

### 3.4. Bundle Size Optimization (If Applicable)

*   If you are deploying a web application or other application that includes NumPy, consider using tree shaking to remove unused code.  Tools like Webpack or Parcel can help with tree shaking.

### 3.5. Lazy Loading

*   If you are working with very large datasets, use lazy loading to load data only when it is needed. Libraries like Dask or Apache Arrow can help with lazy loading.

## 4. Security Best Practices

### 4.1. Common Vulnerabilities

*   **Arbitrary code execution**:  Avoid using `np.fromstring` or `np.frombuffer` with untrusted input, as this can lead to arbitrary code execution.
*   **Denial of service**:  Be careful when using NumPy functions with user-supplied input, as this can lead to denial-of-service attacks. Validate input to prevent excessively large array allocations or computationally intensive operations.
*   **Integer overflow**: Be aware of integer overflow issues when performing arithmetic operations on large arrays. Use appropriate data types to prevent overflow.

### 4.2. Input Validation

*   **Data type validation**:  Ensure that the input data has the expected data type.
*   **Shape validation**:  Ensure that the input data has the expected shape.
*   **Range validation**:  Ensure that the input data falls within the expected range.
*   **Sanitize input**: Sanitize input data to prevent injection attacks.
*   **Use `np.asarray`**: When receiving data from external sources, use `np.asarray` to convert it to a NumPy array. This ensures that you are working with a NumPy array and not some other type of object that might have unexpected behavior.

### 4.3. Authentication and Authorization (If Applicable)

*   NumPy itself doesn't handle authentication or authorization.  If your application requires these features, use appropriate authentication and authorization mechanisms (e.g., OAuth, JWT).

### 4.4. Data Protection

*   **Encryption**: Encrypt sensitive data at rest and in transit.
*   **Access control**: Restrict access to data based on user roles and permissions.
*   **Data masking**: Mask sensitive data to protect it from unauthorized access.
*   **Regular Security Audits**: Conduct regular security audits to identify and address potential vulnerabilities.

### 4.5. Secure API Communication (If Applicable)

*   Use HTTPS for all API communication.
*   Validate all API requests.
*   Use rate limiting to prevent denial-of-service attacks.

## 5. Testing Approaches

### 5.1. Unit Testing

*   **Test individual functions and classes**: Unit tests should focus on testing individual functions and classes in isolation.
*   **Use `pytest` or `unittest`**: Use a testing framework like `pytest` or `unittest` to write and run unit tests.
*   **Test edge cases**: Test edge cases and boundary conditions to ensure that your code handles them correctly.
*   **Test for exceptions**: Test that your code raises the correct exceptions when errors occur.
*   **Use parametrization**: Use parametrization to run the same test with multiple sets of inputs.
*   **Assert almost equal**: Use `np.testing.assert_allclose` instead of `assert a == b` when comparing floating-point numbers.  This accounts for potential floating-point precision errors.

### 5.2. Integration Testing

*   **Test interactions between components**: Integration tests should focus on testing the interactions between different components of your application.
*   **Use mock objects**: Use mock objects to isolate components during integration testing.

### 5.3. End-to-End Testing (If Applicable)

*   **Test the entire application flow**: End-to-end tests should focus on testing the entire application flow from start to finish.

### 5.4. Test Organization

*   **Mirror the source code structure**: Organize your tests in a directory structure that mirrors the source code structure.
*   **Use descriptive test names**: Use descriptive test names that clearly indicate what the test is testing.
*   **Keep tests small and focused**: Keep tests small and focused to make them easier to understand and maintain.

### 5.5. Mocking and Stubbing

*   **Use mock objects to isolate components**: Use mock objects to isolate components during testing.
*   **Use `unittest.mock` or `pytest-mock`**: Use a mocking library like `unittest.mock` or `pytest-mock` to create mock objects.
*   **Mock external dependencies**: Mock external dependencies (e.g., databases, APIs) to avoid relying on them during testing.

## 6. Common Pitfalls and Gotchas

### 6.1. Frequent Mistakes

*   **Incorrect array indexing**: NumPy uses 0-based indexing, which can be confusing for developers who are used to other languages.
*   **Incorrect array slicing**: Be careful when slicing arrays, as this can create views or copies depending on the slicing operation.
*   **Incorrect broadcasting**: Not understanding broadcasting rules can lead to incorrect results or errors.
*   **Modifying views**: Modifying a view affects the original array, which can lead to unexpected side effects.
*   **Ignoring data types**:  Not specifying the correct data type can lead to integer overflow or other issues.

### 6.2. Edge Cases

*   **Empty arrays**: Be careful when working with empty arrays, as they can have unexpected behavior.
*   **Arrays with NaN or Inf values**: Be aware that arrays can contain NaN (Not a Number) or Inf (Infinity) values, which can affect calculations.
*   **Arrays with mixed data types**:  NumPy arrays should typically have a single data type.  Be cautious when working with arrays that have mixed data types (e.g., object arrays).

### 6.3. Version-Specific Issues

*   Consult the NumPy release notes for information on version-specific issues and changes.

### 6.4. Compatibility Concerns

*   **Python version**: Ensure that your code is compatible with the Python version you are using.
*   **Other libraries**: Be aware of compatibility issues between NumPy and other libraries.
*   **Operating system**:  Be aware of potential differences in behavior across different operating systems.

### 6.5. Debugging Strategies

*   **Print statements**: Use print statements to inspect the values of variables and arrays.
*   **Debuggers**: Use a debugger (e.g., pdb, ipdb) to step through your code and inspect the state of the program.
*   **Logging**: Use logging to record errors, warnings, and informational messages.
*   **`np.seterr`**: Use `np.seterr` to configure how NumPy handles floating-point exceptions.
*   **`np.info`**: Use `np.info` to get information about NumPy objects.
*   **Visual Inspection**: Use visualization tools (Matplotlib, Seaborn, etc.) to visually inspect data and identify potential problems.

## 7. Tooling and Environment

### 7.1. Recommended Development Tools

*   **IDE**: Use an IDE (Integrated Development Environment) like VS Code, PyCharm, or Spyder.
*   **Jupyter Notebook**: Use Jupyter Notebook for exploration, prototyping, and EDA.
*   **IPython**: Use IPython for interactive computing.
*   **Debugging Tools**: Utilize debuggers like pdb or IDE-integrated debuggers for stepping through code and inspecting variables.

### 7.2. Build Configuration

*   **`setup.py` or `pyproject.toml`**:  Use `setup.py` or `pyproject.toml` to configure the build process.
*   **`requirements.txt`**:  Use `requirements.txt` to specify dependencies.
*   **Virtual environments**:  Use virtual environments to isolate dependencies.
*   **Conda**: Consider using Conda for environment and package management, particularly for scientific computing workflows.

### 7.3. Linting and Formatting

*   **PEP 8**: Follow PEP 8 style guidelines for Python code.
*   **Linters**: Use a linter (e.g., pylint, flake8) to check for style violations and potential errors.
*   **Formatters**: Use a formatter (e.g., black, autopep8) to automatically format your code.
*   **Pre-commit hooks**: Use pre-commit hooks to run linters and formatters before committing code.

### 7.4. Deployment

*   **Containerization (Docker)**: Use Docker to create a containerized environment for your application. This helps ensure consistency across different environments.
*   **Cloud platforms**: Deploy your application to a cloud platform (e.g., AWS, Azure, GCP).
*   **Serverless functions**: Consider using serverless functions (e.g., AWS Lambda, Azure Functions, Google Cloud Functions) for simple applications.
*   **Model serving frameworks**: If deploying machine learning models, use a model serving framework like TensorFlow Serving or TorchServe.

### 7.5. CI/CD Integration

*   **Continuous Integration (CI)**: Use a CI system (e.g., Jenkins, Travis CI, CircleCI, GitHub Actions) to automatically build and test your code when changes are made.
*   **Continuous Delivery (CD)**: Use a CD system to automatically deploy your code to production after it has been tested.
*   **Automated Testing**: Integrate automated tests into your CI/CD pipeline to ensure code quality and prevent regressions.
*   **Infrastructure as Code (IaC)**: Define your infrastructure using code (e.g., Terraform, CloudFormation) to automate the provisioning and management of your environment.

By following these best practices, you can write high-quality, maintainable, and performant NumPy code.
</file>

<file path=".cursor/rules/openai.mdc">
---
description: Comprehensive best practices and coding standards for projects using the openai library, covering code structure, performance, security, and common pitfalls.
globs: **/*.py
---
# openai Library Best Practices and Coding Standards

This document outlines best practices and coding standards for developing applications using the `openai` library. Following these guidelines will lead to more maintainable, performant, and secure code.

## Library Information:
- Name: openai
- Tags: ai, ml, llm, api

## 1. Code Organization and Structure

### 1.1 Directory Structure Best Practices

Adopt a clear and consistent directory structure to improve code organization and maintainability. Here's a recommended structure for projects using the openai library:


project_root/
├── src/                        # Source code directory
│   ├── models/              # Definitions for your models (e.g., data classes, schemas)
│   ├── services/             # Service layer for interacting with the OpenAI API
│   │   ├── openai_service.py   # Encapsulates OpenAI API calls
│   ├── utils/                # Utility functions
│   ├── main.py               # Entry point of your application
├── tests/                      # Tests directory
│   ├── unit/                 # Unit tests
│   ├── integration/          # Integration tests
│   ├── conftest.py           # Pytest configuration file
├── data/                       # Data storage (e.g., prompts, training data)
├── docs/                       # Documentation
├── .env                        # Environment variables
├── requirements.txt            # Dependencies
├── README.md                   # Project README


### 1.2 File Naming Conventions

-   Use descriptive and consistent file names.
-   Python files should use snake_case (e.g., `openai_service.py`).
-   Class names should use CamelCase (e.g., `OpenAIService`).
-   Variable names should use snake_case (e.g., `api_key`).

### 1.3 Module Organization

-   Group related functionalities into modules.
-   Avoid circular dependencies between modules.
-   Use clear and concise module names.
-   Use `__init__.py` files to define packages and control namespace.

### 1.4 Component Architecture

Consider using a layered architecture to separate concerns:

-   **Presentation Layer:** Handles user interface or external API interactions.
-   **Service Layer:** Encapsulates business logic and interacts with the OpenAI API.
-   **Data Access Layer:** Handles data persistence and retrieval.

This separation makes testing and maintenance easier.

### 1.5 Code Splitting Strategies

-   Split large files into smaller, more manageable modules based on functionality.
-   Use abstract base classes and interfaces to define contracts between components.
-   Apply the Single Responsibility Principle (SRP) to classes and functions.

## 2. Common Patterns and Anti-patterns

### 2.1 Design Patterns

-   **Factory Pattern:** Use a factory to create OpenAI API client instances with different configurations.
-   **Strategy Pattern:** Implement different prompt strategies based on the task.
-   **Decorator Pattern:** Add logging, caching, or rate limiting to OpenAI API calls.

### 2.2 Recommended Approaches

-   **Prompt Engineering:** Follow best practices for prompt design. Place clear instructions at the beginning of prompts, be specific, and use examples.
-   **Configuration:** Store API keys and other sensitive information in environment variables using a library like `python-dotenv`.
-   **Asynchronous Calls:** Use `asyncio` and `aiohttp` for non-blocking API calls to improve performance.
-   **Retries and Exponential Backoff:** Implement retry mechanisms with exponential backoff to handle transient API errors.

### 2.3 Anti-patterns

-   **Hardcoding API Keys:** Never hardcode API keys directly into your code. Always use environment variables.
-   **Ignoring Rate Limits:** Implement rate limiting to avoid exceeding OpenAI API limits.
-   **Lack of Error Handling:** Always handle API errors gracefully and provide informative error messages.
-   **Overly Complex Prompts:** Keep prompts simple and focused. Break down complex tasks into smaller steps.
-   **Mixing Concerns:** Avoid mixing presentation, business logic, and data access in the same component.

### 2.4 State Management

-   Use appropriate data structures to manage the state of your OpenAI interactions.
-   Consider using a state management library if your application has complex state requirements.
-   Avoid storing sensitive information in application state.

### 2.5 Error Handling

-   Use `try-except` blocks to catch potential exceptions.
-   Log errors with sufficient context for debugging.
-   Implement custom exception classes for specific error conditions.
-   Handle rate limit errors and implement retry logic.

## 3. Performance Considerations

### 3.1 Optimization Techniques

-   **Caching:** Cache API responses to reduce the number of API calls.
-   **Batching:** Batch multiple API requests into a single request when possible.
-   **Asynchronous Operations:** Use asynchronous programming to avoid blocking the main thread.
-   **Token Optimization:** Reduce the number of tokens in your prompts to lower costs and improve response times.

### 3.2 Memory Management

-   Be mindful of the size of your prompts and responses, especially when working with large language models.
-   Use generators to process large datasets in chunks.
-   Clean up resources (e.g., file handles, network connections) promptly.

### 3.3 Rendering Optimization (If Applicable)

-   If your application involves rendering OpenAI-generated content, optimize the rendering process to minimize latency.

### 3.4 Bundle Size Optimization (If Applicable)

-   For web applications, minimize bundle size by using tree shaking and code splitting.

### 3.5 Lazy Loading

-   Use lazy loading to load modules or data only when they are needed.

## 4. Security Best Practices

### 4.1 Common Vulnerabilities

-   **API Key Exposure:** Protect your OpenAI API key. Never commit it to version control or share it publicly.
-   **Prompt Injection:** Validate and sanitize user inputs to prevent prompt injection attacks.
-   **Data Leakage:** Avoid exposing sensitive data in prompts or API responses.

### 4.2 Input Validation

-   Validate all user inputs to prevent malicious or unexpected data from being sent to the OpenAI API.
-   Sanitize inputs to remove potentially harmful characters or code.

### 4.3 Authentication and Authorization

-   Implement authentication and authorization mechanisms to protect your application and data.
-   Use secure storage for API keys and other sensitive information.

### 4.4 Data Protection

-   Encrypt sensitive data at rest and in transit.
-   Follow data privacy regulations (e.g., GDPR, CCPA).

### 4.5 Secure API Communication

-   Use HTTPS to encrypt communication with the OpenAI API.
-   Verify the authenticity of the OpenAI API server using SSL certificates.

## 5. Testing Approaches

### 5.1 Unit Testing

-   Write unit tests for individual components to ensure they function correctly in isolation.
-   Use mocking and stubbing to isolate components from external dependencies (e.g., the OpenAI API).

### 5.2 Integration Testing

-   Write integration tests to verify that different components work together correctly.
-   Test the interaction between your application and the OpenAI API.

### 5.3 End-to-End Testing

-   Write end-to-end tests to simulate user interactions and verify that the entire application works as expected.

### 5.4 Test Organization

-   Organize your tests into a clear and consistent directory structure.
-   Use descriptive test names.
-   Follow a consistent testing style.

### 5.5 Mocking and Stubbing

-   Use mocking libraries like `unittest.mock` or `pytest-mock` to mock the OpenAI API.
-   Create stubs for API responses to control the behavior of the API during testing.

## 6. Common Pitfalls and Gotchas

### 6.1 Frequent Mistakes

-   **Not handling API errors:** Implement proper error handling for OpenAI API calls.
-   **Exceeding rate limits:** Implement rate limiting and exponential backoff to avoid exceeding API limits.
-   **Incorrect prompt formatting:** Follow OpenAI's prompt engineering guidelines to optimize model performance.
-   **Not validating inputs:** Validate user inputs to prevent prompt injection attacks and unexpected behavior.

### 6.2 Edge Cases

-   **Handling very long prompts:** Be aware of token limits and consider splitting long prompts into smaller chunks.
-   **Dealing with ambiguous or unclear instructions:** Craft prompts carefully to provide clear and specific instructions.
-   **Handling unexpected API responses:** Implement robust error handling to deal with unexpected API responses.

### 6.3 Version-Specific Issues

-   Be aware of changes between different versions of the `openai` library.
-   Consult the release notes and migration guides when upgrading to a new version.

### 6.4 Compatibility Concerns

-   Ensure compatibility between the `openai` library and other libraries used in your project.
-   Test your application thoroughly after upgrading any dependencies.

### 6.5 Debugging Strategies

-   Use logging to track the flow of your application and identify potential issues.
-   Use a debugger to step through your code and inspect variables.
-   Use unit tests to isolate and debug individual components.

## 7. Tooling and Environment

### 7.1 Recommended Development Tools

-   **IDE:** VS Code, PyCharm
-   **Virtual Environment Manager:** venv, conda, pipenv
-   **Package Manager:** pip, poetry
-   **Testing Framework:** pytest, unittest
-   **Linting and Formatting:** pylint, flake8, black

### 7.2 Build Configuration

-   Use a `requirements.txt` or `pyproject.toml` file to manage dependencies.
-   Use a build system like `setuptools` or `poetry` to package your application.

### 7.3 Linting and Formatting

-   Use a linter like `pylint` or `flake8` to enforce coding style guidelines.
-   Use a formatter like `black` to automatically format your code.

### 7.4 Deployment Best Practices

-   Use a containerization technology like Docker to package your application and its dependencies.
-   Deploy your application to a cloud platform like AWS, Azure, or Google Cloud.
-   Use a process manager like `systemd` or `supervisor` to manage your application.

### 7.5 CI/CD Integration

-   Use a CI/CD pipeline to automate the build, test, and deployment process.
-   Integrate your tests into the CI/CD pipeline to ensure that all changes are thoroughly tested before being deployed.
</file>

<file path=".cursor/rules/pandas.mdc">
---
description: This rule outlines best practices for using the pandas library in Python, covering code style, performance, data handling, and testing. It aims to promote maintainable, efficient, and robust data analysis workflows.
globs: **/*.py
---
# Pandas Best Practices

This document provides guidelines for writing high-quality pandas code, covering various aspects from code style to performance optimization.

## 1. Code Organization and Structure

- **Directory Structure:**
    - Organize your project with a clear directory structure.
    - Use separate directories for data, scripts, modules, tests, and documentation.
    - Example:
        
        project_root/
        ├── data/
        │   ├── raw/
        │   └── processed/
        ├── src/
        │   ├── modules/
        │   │   ├── data_cleaning.py
        │   │   ├── feature_engineering.py
        │   │   └── ...
        │   ├── main.py
        ├── tests/
        │   ├── unit/
        │   ├── integration/
        │   └── ...
        ├── docs/
        ├── notebooks/
        ├── requirements.txt
        └── ...
        

- **File Naming Conventions:**
    - Use descriptive and consistent file names.
    - Use snake_case for Python files and variables.
    - Example: `data_processing.py`, `load_data.py`

- **Module Organization:**
    - Break down your code into reusable modules.
    - Each module should focus on a specific task or functionality.
    - Use clear and concise function and class names within modules.
    - Follow the Single Responsibility Principle (SRP).

- **Component Architecture:**
    - Design your pandas-based applications with a clear component architecture.
    - Separate data loading, preprocessing, analysis, and visualization into distinct components.
    - Use classes to encapsulate related functionality and data.

- **Code Splitting Strategies:**
    - Split large pandas operations into smaller, more manageable steps.
    - Use functions or methods to encapsulate these steps.
    - This improves readability and makes debugging easier.

## 2. Common Patterns and Anti-patterns

- **Design Patterns Specific to Pandas:**
    - **Chain of Responsibility:** Use method chaining to perform a sequence of operations on a DataFrame.
    - **Strategy Pattern:** Implement different data processing strategies based on input parameters.
    - **Factory Pattern:** Use factory functions to create DataFrames or Series from various sources.

- **Recommended Approaches for Common Tasks:**
    - **Data Loading:** Use `pd.read_csv()`, `pd.read_excel()`, `pd.read_sql()` to load data from different sources.
    - **Data Cleaning:** Use `dropna()`, `fillna()`, `replace()` to clean missing or incorrect data.
    - **Data Transformation:** Use `apply()`, `map()`, `groupby()`, `pivot_table()` to transform data.
    - **Data Aggregation:** Use `groupby()`, `agg()`, `transform()` to aggregate data.
    - **Data Visualization:** Use `matplotlib`, `seaborn`, or `plotly` to visualize data.

- **Anti-patterns and Code Smells to Avoid:**
    - **Iterating over DataFrames with `iterrows()` or `itertuples()`:** These are slow; prefer vectorized operations.
    - **Chained Indexing:** Avoid using chained indexing (e.g., `df['A']['B']`) as it can lead to unexpected behavior. Use `.loc` or `.iloc` instead.
    - **Ignoring Data Types:** Always be aware of your data types and convert them appropriately using `astype()`.
    - **Writing loops to filter/process the data:** Vectorize these operations to avoid performance issues.
    - **Modifying DataFrame inplace:** Creating copies is a safer approach.

- **State Management Best Practices:**
    - Avoid modifying DataFrames in place unless absolutely necessary.
    - Make copies of DataFrames using `.copy()` to avoid unintended side effects.
    - Use functional programming principles where possible to minimize state changes.

- **Error Handling Patterns:**
    - Use `try-except` blocks to handle potential errors during data loading, processing, or analysis.
    - Log errors and warnings using the `logging` module.
    - Raise exceptions when necessary to signal errors to the calling code.

## 3. Performance Considerations

- **Optimization Techniques:**
    - **Vectorization:** Use vectorized operations instead of loops whenever possible. Pandas is optimized for vectorized operations.
    - **Cython:** Consider using Cython to optimize performance-critical parts of your code.
    - **Numba:** Use Numba to JIT-compile NumPy and pandas code for improved performance.
    - **Dask:** Use Dask for parallel processing of large datasets that don't fit in memory.
    - **Parquet or Feather:** Use Parquet or Feather file formats for efficient data storage and retrieval.
    - **Categorical Data:** Use categorical data types for columns with a limited number of unique values.

- **Memory Management:**
    - **Data Types:** Choose appropriate data types to minimize memory usage (e.g., `int8`, `float32` instead of `int64`, `float64`).
    - **Chunking:** Load large datasets in chunks to avoid memory errors.
    - **Garbage Collection:** Use `gc.collect()` to explicitly release memory.
    - **Sparse Data:** Use sparse data structures for data with many missing values.

## 4. Security Best Practices

- **Common Vulnerabilities and How to Prevent Them:**
    - **SQL Injection:** When reading data from SQL databases, use parameterized queries or ORM frameworks to prevent SQL injection attacks.
    - **CSV Injection:** Be cautious when loading CSV files from untrusted sources, as they may contain malicious formulas that can execute arbitrary code.
    - **Arbitrary Code Execution:** Avoid using `eval()` or `exec()` on data loaded from untrusted sources, as this can lead to arbitrary code execution.

- **Input Validation:**
    - Validate user inputs to prevent malicious data from entering your pandas workflows.
    - Use regular expressions or custom validation functions to check the format and content of inputs.

- **Data Protection Strategies:**
    - Encrypt sensitive data at rest and in transit.
    - Use access control mechanisms to restrict access to sensitive data.
    - Anonymize or pseudonymize data when possible to protect privacy.

## 5. Testing Approaches

- **Unit Testing Strategies:**
    - Write unit tests for individual functions and classes.
    - Use `pytest` or `unittest` for writing and running tests.
    - Test edge cases and boundary conditions.
    - Use assert statements to verify the correctness of your code.

- **Integration Testing:**
    - Write integration tests to verify the interaction between different modules or components.
    - Test the end-to-end functionality of your pandas-based applications.

- **Test Organization:**
    - Organize your tests in a separate `tests` directory.
    - Use a clear naming convention for your test files and functions.
    - Example: `test_data_cleaning.py`, `test_load_data()`

- **Mocking and Stubbing:**
    - Use mocking and stubbing to isolate units of code during testing.
    - Use the `unittest.mock` module or third-party libraries like `pytest-mock`.

## 6. Common Pitfalls and Gotchas

- **Frequent Mistakes Developers Make:**
    - **Forgetting to set the index properly:** This can lead to performance issues when joining or merging DataFrames.
    - **Incorrectly handling missing data:** Be aware of how missing data is represented in your DataFrames and handle it appropriately.
    - **Not understanding the difference between `.loc` and `.iloc`:** These methods are used for different types of indexing and can lead to unexpected results if used incorrectly.

- **Edge Cases to Be Aware Of:**
    - **Empty DataFrames:** Handle the case where a DataFrame is empty.
    - **DataFrames with duplicate indices:** Be aware of how pandas handles DataFrames with duplicate indices.

- **Debugging Strategies:**
    - Use the `print()` function or the `logging` module to debug your code.
    - Use a debugger to step through your code and inspect variables.
    - Use the `pdb` module for interactive debugging.

## 7. Tooling and Environment

- **Recommended Development Tools:**
    - **Jupyter Notebook/Lab:** For interactive data exploration and analysis.
    - **VS Code with Python extension:** For code editing, debugging, and testing.
    - **PyCharm:** A full-featured IDE for Python development.

- **Linting and Formatting:**
    - Use `flake8` to lint your code and identify potential issues.
    - Use `black` to automatically format your code according to PEP 8.
    - Use `isort` to automatically sort your imports.
    - Integrate these tools into your pre-commit hooks to ensure consistent code style.

- **CI/CD Integration:**
    - Use CI/CD pipelines to automate the testing and deployment of your pandas-based applications.
    - Integrate your CI/CD pipelines with your version control system (e.g., GitHub, GitLab, Bitbucket).
    - Use Docker to containerize your applications for consistent deployment.

This comprehensive guide covers the key aspects of pandas best practices and coding standards. By following these guidelines, you can write more maintainable, efficient, and robust pandas code.
</file>

<file path=".cursor/rules/pillow.mdc">
---
description: This rule provides best practices for using the Pillow image processing library in Python, covering code organization, performance, security, testing, and common pitfalls. It aims to help developers write maintainable, efficient, and secure image processing applications.
globs: **/*.py
---
# Pillow Library Best Practices and Coding Standards

This document outlines the recommended best practices and coding standards for effectively using the Pillow library in Python for image processing tasks. These guidelines aim to promote maintainable, efficient, and secure applications.

## 1. Installation and Setup

- **Use a Virtual Environment:** Always create a virtual environment for your Pillow projects to isolate dependencies and avoid conflicts. Use `python -m venv .venv` and activate it.
- **Install with pip:** Install Pillow using `pip install Pillow`.  Consider using `pip install -U Pillow` for upgrades.
- **Verify Installation:**  Verify the installation using `python -c "from PIL import Image; print(Image.VERSION)"`. This also serves as a smoke test for the library.
- **Pin Dependencies:** Specify Pillow and other dependencies in a `requirements.txt` file, pinning the versions for reproducible builds. Use `pip freeze > requirements.txt`.
- **Install dependencies with UV:** Use the uv package manager when installing dependencies: `pip install uv` then `uv pip install -r requirements.txt`.

## 2. Code Organization and Structure

- **Directory Structure:** Organize your project into logical directories. A typical structure might include:
  
  project_name/
  ├── src/
  │   ├── __init__.py
  │   ├── image_processing.py  # Pillow-related functions
  │   ├── utils.py            # Helper functions
  ├── tests/
  │   ├── __init__.py
  │   ├── test_image_processing.py
  ├── data/                   # Input and output images
  ├── notebooks/              # Jupyter notebooks for experimentation
  ├── requirements.txt
  ├── pyproject.toml         # Optional: For poetry or flit
  ├── README.md
  
- **File Naming Conventions:** Use descriptive and consistent file names:
  - Python modules: `image_utils.py`, `image_filters.py`
  - Test files: `test_image_utils.py`, `test_image_filters.py`
  - Image data: `input_image.jpg`, `output_image.png`
- **Module Organization:** Group related functions and classes into modules. For example:
  - `image_processing.py`: Contains functions for core image manipulations (resizing, cropping, color conversions).
  - `image_filters.py`:  Contains functions for applying image filters (blur, sharpen, edge detection).
  - `image_io.py`: Contains functions for image loading and saving.
- **Component Architecture:** Consider using a component-based architecture for larger projects:
  - **Data Access Layer:**  Handles image loading and saving.
  - **Business Logic Layer:**  Implements image processing algorithms.
  - **Presentation Layer:**  Provides a user interface (if applicable) or API endpoint.
- **Code Splitting:** For large image processing pipelines, split the code into smaller, manageable functions. Use generators for lazy processing of large image datasets.

## 3. Coding Standards and Best Practices

- **Importing:** Always import Pillow using `from PIL import Image`. This is the standard and recommended way.
- **Explicit Imports:** Use explicit imports (`from PIL import Image, ImageFilter`) rather than wildcard imports (`from PIL import *`). This improves code readability and avoids namespace pollution.
- **Clear Naming:** Use descriptive variable and function names:
  - `image = Image.open("input.jpg")`
  - `resized_image = image.resize((width, height))`
- **Error Handling:** Use `try...except` blocks to handle potential errors, such as `FileNotFoundError` when opening images or `IOError` when saving images.
- **Context Managers:** Use `with` statements to ensure proper resource management (especially file closing) when working with images:
  python
  try:
    with Image.open("input.jpg") as image:
        # Process the image
        image.save("output.png")
  except FileNotFoundError:
      print("Error: Input image not found.")
  except IOError:
      print("Error: Could not save the image.")
  
- **Image Modes:** Be mindful of image modes (RGB, RGBA, L, CMYK) and convert images to the appropriate mode when necessary using `image.convert("RGB")`.
- **Resampling Filters:**  Choose appropriate resampling filters for resizing operations:
  - `Image.LANCZOS`: High-quality resampling for downscaling.
  - `Image.NEAREST`: Fastest resampling, but lowest quality.
  - `Image.BILINEAR`, `Image.BICUBIC`: Intermediate quality and speed.
  - Use `Image.Resampling.LANCZOS` (or `Image.Resampling.BOX`, `Image.Resampling.NEAREST`, etc.) if using Pillow 9.2.0 or newer.
- **Saving Images:** Always specify the format when saving images, especially if the filename extension is ambiguous. Use `image.save("output.png", format="PNG")`.
- **Documentation:** Write clear and concise docstrings for all functions and classes, explaining their purpose, arguments, and return values.
- **Type Hints:** Use type hints to improve code readability and help catch errors early.
  python
  from PIL import Image

def resize_image(image_path: str, width: int, height: int) -> Image.Image:
    """Resizes an image to the specified dimensions."""
    with Image.open(image_path) as image:
      resized_image = image.resize((width, height))
      return resized_image
  
- **Linting and Formatting:** Use a linter (e.g., pylint, flake8) and a formatter (e.g., black, autopep8) to maintain consistent code style.

## 4. Common Patterns and Anti-patterns

- **Factory Pattern:** Use a factory pattern to create different types of image objects based on input data.
- **Strategy Pattern:** Implement different image processing algorithms as strategies, allowing you to easily switch between them.
- **Anti-pattern: Ignoring Errors:** Avoid ignoring exceptions without proper handling.  Always log errors or raise them appropriately.
- **Anti-pattern: Excessive Memory Usage:** Avoid loading entire image datasets into memory at once. Use generators or iterators for large datasets.
- **State Management:** For applications with image editing features, use a state management system to track changes and enable undo/redo functionality. Redux, Zustand, or custom state management are possibilities.
- **Error Handling Patterns:** Use specific exception types (e.g., `FileNotFoundError`, `ValueError`) to handle different error scenarios. Provide informative error messages to the user or log them for debugging.

## 5. Performance Considerations

- **Optimize Image Formats:** Choose the appropriate image format for your needs. JPEG is good for photographs, PNG is good for images with transparency or sharp edges, and WebP offers good compression and quality.
- **Image Optimization Libraries:** Utilize libraries like `optipng` or `jpegoptim` to further optimize images for web delivery. These tools can reduce file size without significant quality loss.
- **Lazy Loading:** Load images only when they are needed, especially for web applications. Use placeholders or low-resolution previews while the full image is loading.
- **Caching:** Cache frequently accessed images to reduce loading times. Use a memory-based cache (e.g., `functools.lru_cache`) or a disk-based cache (e.g., `diskcache`).
- **Thumbnail Generation:** Generate thumbnails for large images to improve performance in image galleries or previews. Use `image.thumbnail()`.
- **Efficient Resizing:**  Use `image.thumbnail()` when creating thumbnails as it preserves aspect ratio. If ignoring aspect ratio, use `image.resize()` but be mindful of performance implications.
- **Memory Management:**
  - **Use `del` to release memory:** Explicitly delete large image objects when they are no longer needed using `del image`.
  - **Limit Image Size:** Avoid loading extremely large images that can consume excessive memory.  Consider resizing images to a reasonable size before processing.
  - **Chunked Processing:** For very large images, process them in chunks to reduce memory usage.
- **Rendering Optimization:**  When displaying images in a GUI application, use optimized rendering techniques to improve performance.  Consider using hardware acceleration if available.
- **Asynchronous Processing:**  Offload image processing tasks to separate threads or processes to avoid blocking the main thread.  Use `threading` or `multiprocessing` modules.
- **Numpy integration:** Convert Pillow images to NumPy arrays for faster calculations.

## 6. Security Best Practices

- **Input Validation:** Validate all image file names and paths to prevent directory traversal attacks and other security vulnerabilities. Use `os.path.abspath()` and `os.path.normpath()` to sanitize paths.
- **File Extension Validation:** Verify that the file extension matches the actual image format.  Don't rely solely on the extension.
- **Limit File Size:**  Limit the maximum file size of uploaded images to prevent denial-of-service attacks.
- **Prevent Image Bomb Attacks:** Implement checks to prevent image bomb attacks, which involve maliciously crafted images designed to consume excessive resources.
- **Use a Security Linter:**  Incorporate a security linter (e.g., bandit) into your CI/CD pipeline to identify potential security vulnerabilities in your code.
- **Authentication and Authorization:**  Implement proper authentication and authorization mechanisms to protect access to image processing services.
- **Data Protection:**  Encrypt sensitive image data at rest and in transit.
- **Avoid Executing Untrusted Code:** Be cautious when using Pillow to process images from untrusted sources, as vulnerabilities in the library could be exploited. Regularly update Pillow to the latest version.
- **Regularly Update Pillow:** Stay up-to-date with the latest Pillow releases to patch security vulnerabilities.
- **Safe Image Handling:** Consider using a sandboxed environment to handle untrusted images, further isolating potential security risks.

## 7. Testing Approaches

- **Unit Testing:** Write unit tests for individual functions and classes in your Pillow-related code. Use a testing framework like `pytest` or `unittest`.
  - **Test Image Transformations:** Verify that image transformations (resizing, cropping, color conversions) produce the expected results.
  - **Test Error Handling:** Ensure that your code handles errors gracefully (e.g., invalid image files, incorrect parameters).
- **Integration Testing:** Test the interaction between different components of your image processing pipeline.
- **End-to-End Testing:** Verify that the entire application works as expected, including image loading, processing, and saving.
- **Test Organization:** Organize your tests into logical directories that mirror your code structure.
- **Mocking and Stubbing:** Use mocking and stubbing to isolate components during testing.  For example, mock the `Image.open()` function to avoid accessing real image files during unit tests.
- **Property-based Testing:** Use property-based testing (e.g., Hypothesis) to automatically generate test cases and verify that your code satisfies certain properties.
- **Golden Image Tests:** Compare the output of your image processing functions with known "golden" images to ensure that the results are consistent.

## 8. Common Pitfalls and Gotchas

- **Incorrect Image Mode:** Forgetting to convert images to the correct mode (e.g., RGB) before performing certain operations can lead to unexpected results.
- **Integer Division:**  Be aware of integer division in Python 2. Use `from __future__ import division` to ensure that division always returns a float.
- **File Closing:**  Forgetting to close image files after processing can lead to resource leaks. Use `with` statements to ensure that files are closed automatically.
- **Out-of-Memory Errors:** Processing extremely large images can lead to out-of-memory errors.  Use techniques like chunked processing and lazy loading to avoid this.
- **Color Profile Issues:** Be aware of color profile issues when working with images that have embedded color profiles.  Consider using `image.convert('RGB')` to strip color profiles or use a color management library like `littlecms`.
- **Pillow Version Compatibility:** Be aware of compatibility issues between different Pillow versions.  Check the Pillow documentation for version-specific changes and bug fixes.
- **Floating point errors:** Be aware of the loss of precision due to floating point math and how to properly fix it.

## 9. Tooling and Environment

- **Development Tools:**
  - **IDE:** Use a powerful IDE like VS Code with the Python extension or PyCharm.
  - **Debugging:** Use a debugger to step through your code and identify errors.
  - **Profiling:** Use a profiler to identify performance bottlenecks.
- **Build Configuration:**
  - **`setup.py`:** Use `setup.py` or `pyproject.toml` to manage your project's dependencies and build process.
  - **Virtual Environments:** Use virtual environments to isolate your project's dependencies.
- **Linting and Formatting:**
  - **Pylint:** Use Pylint to check your code for style errors and potential problems.
  - **Black:** Use Black to automatically format your code to a consistent style.
  - **Flake8:** Use Flake8 to check your code for style errors and potential problems.
- **Deployment:**
  - **Docker:** Use Docker to containerize your application and ensure consistent deployment across different environments.
  - **Cloud Platforms:** Deploy your application to a cloud platform like AWS, Google Cloud, or Azure.
- **CI/CD Integration:**
  - **GitHub Actions:** Use GitHub Actions to automate your build, test, and deployment process.
  - **Jenkins:** Use Jenkins for continuous integration and continuous delivery.
  - **CircleCI:** Use CircleCI for continuous integration and continuous delivery.

By following these best practices and coding standards, you can create robust, efficient, and secure image processing applications using the Pillow library in Python.
</file>

<file path=".cursor/rules/pytest.mdc">
---
description: This rule file outlines comprehensive best practices for using pytest in Python projects, covering code organization, testing strategies, performance optimization, security measures, and common pitfalls to avoid.
globs: **/*.py
---
# Pytest Best Practices: A Comprehensive Guide

This document provides a detailed guide to using pytest effectively in Python projects, covering various aspects from code organization to security considerations. It aims to provide actionable guidance for developers to improve their testing practices and build robust applications.

## Library Information:
- Name: pytest
- Tags: development, testing, python

## 1. Code Organization and Structure

A well-organized codebase is crucial for maintainability and testability. Here are best practices for structuring your pytest projects:

### 1.1. Directory Structure

- **Separate `tests/` directory:** Keep your tests in a directory separate from your application code, typically named `tests/`. This promotes isolation and cleaner project structure.

  
  my_project/
  ├── my_app/
  │   ├── __init__.py
  │   ├── module1.py
  │   └── module2.py
  ├── tests/
  │   ├── __init__.py
  │   ├── test_module1.py
  │   └── test_module2.py
  └── pyproject.toml
  

- **`src` layout (Recommended):** Consider using a `src` layout to further isolate application code from the project root. This prevents import conflicts and improves clarity.

  
  my_project/
  ├── src/
  │   └── my_app/
  │       ├── __init__.py
  │       ├── module1.py
  │       └── module2.py
  ├── tests/
  │   ├── __init__.py
  │   ├── test_module1.py
  │   └── test_module2.py
  └── pyproject.toml
  

### 1.2. File Naming Conventions

- **`test_*.py` or `*_test.py`:** pytest automatically discovers test files matching these patterns.
- **Descriptive names:** Use clear and descriptive names for your test files to indicate what they are testing (e.g., `test_user_authentication.py`).

### 1.3. Module Organization

- **Mirror application structure:** Structure your test modules to mirror the structure of your application code. This makes it easier to locate tests for specific modules.
- **`__init__.py`:** Include `__init__.py` files in your test directories to ensure they are treated as Python packages.

### 1.4. Component Architecture

- **Isolate components:** Design your application with well-defined components that can be tested independently.
- **Dependency injection:** Use dependency injection to provide components with their dependencies, making it easier to mock and stub external resources during testing.

### 1.5. Code Splitting

- **Small, focused functions:** Break down large functions into smaller, focused functions that are easier to test.
- **Modular design:** Organize your code into modules with clear responsibilities.

## 2. Common Patterns and Anti-patterns

### 2.1. Design Patterns

- **Arrange-Act-Assert (AAA):** Structure your tests following the AAA pattern for clarity.
    - **Arrange:** Set up the test environment and prepare any necessary data.
    - **Act:** Execute the code being tested.
    - **Assert:** Verify that the code behaved as expected.

  python
  def test_example():
      # Arrange
      data = ...
      expected_result = ...

      # Act
      result = function_under_test(data)

      # Assert
      assert result == expected_result
  

- **Fixture factory:** Use fixture factories to create reusable test data.

  python
  import pytest

  @pytest.fixture
  def user_factory():
      def create_user(username, email):
          return {"username": username, "email": email}
      return create_user

  def test_create_user(user_factory):
      user = user_factory("testuser", "test@example.com")
      assert user["username"] == "testuser"
  

### 2.2. Recommended Approaches

- **Use fixtures for setup and teardown:** Fixtures help manage test dependencies and ensure a clean test environment.
- **Parameterize tests:** Use `@pytest.mark.parametrize` to run the same test with different inputs and expected outputs, reducing code duplication.
- **Use descriptive names for tests and fixtures:** This makes it easier to understand the purpose of each test and fixture.
- **Single Assertion per Test:** A single assertion per test makes it easier to identify the specific failure point.

### 2.3. Anti-patterns and Code Smells

- **Over-reliance on fixtures:** Avoid creating too many fixtures, especially for simple data.  Use direct data definition in the test if it's not reused.
- **Implicit dependencies:** Make dependencies explicit by passing them as arguments to your functions and tests.
- **Testing implementation details:** Focus on testing the behavior of your code, not the implementation details.  This makes your tests more resilient to refactoring.
- **Skipping Tests Without a Reason:** Don't skip tests without a valid reason or comment explaining why.

### 2.4. State Management

- **Stateless tests:** Ensure your tests are stateless and independent to avoid unexpected side effects. Each test should set up its own data and clean up after itself.
- **Fixture scopes:** Use fixture scopes (`session`, `module`, `function`) to control the lifecycle of fixtures and manage state effectively.

### 2.5. Error Handling

- **Test exception handling:** Write tests to verify that your code handles exceptions correctly.

  python
  import pytest

  def divide(a, b):
      if b == 0:
          raise ValueError("Cannot divide by zero")
      return a / b

  def test_divide_by_zero():
      with pytest.raises(ValueError) as e:
          divide(10, 0)
      assert str(e.value) == "Cannot divide by zero"
  

- **Use `pytest.raises`:** Use `pytest.raises` to assert that a specific exception is raised.
- **Log errors:** Ensure your application logs errors appropriately, and consider writing tests to verify that errors are logged correctly.

## 3. Performance Considerations

### 3.1. Optimization Techniques

- **Profile slow tests:** Use the `--durations` option to identify slow tests and optimize them.
- **Parallel test execution:** Use `pytest-xdist` to run tests in parallel and reduce overall test execution time. `pip install pytest-xdist` then run `pytest -n auto`.  The `auto` option utilizes all available CPU cores.
- **Caching:** Cache expensive computations to avoid redundant calculations during testing.

### 3.2. Memory Management

- **Resource cleanup:** Ensure your tests clean up any resources they allocate, such as temporary files or database connections.
- **Limit fixture scope:** Use the appropriate fixture scope to minimize the lifetime of fixtures and reduce memory consumption.

### 3.3. Bundle Size Optimization

- **N/A:** Pytest itself doesn't directly impact bundle sizes, but your application code should be optimized separately.

### 3.4. Lazy Loading

- **N/A:** Lazy loading is more relevant to application code than pytest itself, but can be used within fixtures if necessary to defer initialization.

## 4. Security Best Practices

### 4.1. Common Vulnerabilities

- **Injection attacks:** Prevent injection attacks by validating and sanitizing user inputs.
- **Cross-site scripting (XSS):** Protect against XSS vulnerabilities by escaping user-generated content.
- **Authentication and authorization flaws:** Implement secure authentication and authorization mechanisms to protect sensitive data.

### 4.2. Input Validation

- **Validate all inputs:** Validate all user inputs to ensure they conform to expected formats and ranges.
- **Use parameterized tests:** Use parameterized tests to test input validation logic with a variety of inputs, including edge cases and invalid values.

### 4.3. Authentication and Authorization

- **Test authentication:** Write tests to verify that your authentication mechanisms are working correctly.
- **Test authorization:** Write tests to verify that users only have access to the resources they are authorized to access.

### 4.4. Data Protection

- **Encrypt sensitive data:** Encrypt sensitive data at rest and in transit.
- **Use secure storage:** Store sensitive data in secure storage locations with appropriate access controls.

### 4.5. Secure API Communication

- **Use HTTPS:** Always use HTTPS for API communication to protect data in transit.
- **Validate API responses:** Validate API responses to ensure they are valid and haven't been tampered with.

## 5. Testing Approaches

### 5.1. Unit Testing

- **Test individual units:** Unit tests should focus on testing individual functions, methods, or classes in isolation.
- **Mock dependencies:** Use mocking to isolate units under test from their dependencies.

### 5.2. Integration Testing

- **Test interactions:** Integration tests should focus on testing the interactions between different components of your application.
- **Use real dependencies (where appropriate):** For integration tests, it's often appropriate to use real dependencies, such as databases or external APIs, to ensure that the different components work together correctly.  Consider using test containers for database and service dependencies.

### 5.3. End-to-End Testing

- **Test complete workflows:** End-to-end tests should focus on testing complete user workflows, from start to finish.
- **Use browser automation:** Use browser automation tools like Selenium or Playwright to simulate user interactions with your application.

### 5.4. Test Organization

- **Organize tests by feature:** Group tests by the feature they are testing to improve organization and maintainability.
- **Use clear naming conventions:** Use clear naming conventions for your tests and test files to indicate what they are testing.

### 5.5. Mocking and Stubbing

- **Use `mocker` fixture:** Use the `mocker` fixture provided by the `pytest-mock` plugin for mocking and stubbing.
- **Mock external dependencies:** Mock external dependencies, such as databases or APIs, to isolate your tests and prevent them from relying on external resources.
- **Use `autospec=True`:** Use `autospec=True` when mocking to ensure that your mocks have the same API as the original objects. This helps prevent errors caused by incorrect mock implementations.

  python
  def test_example(mocker):
      mock_external_api = mocker.patch("module.external_api", autospec=True)
      mock_external_api.return_value = {"data": "test data"}
  

## 6. Common Pitfalls and Gotchas

### 6.1. Frequent Mistakes

- **Not isolating tests:** Failing to isolate tests can lead to unpredictable results and make it difficult to debug failures.
- **Testing implementation details:** Testing implementation details makes your tests brittle and difficult to maintain.
- **Ignoring warnings:** Ignoring warnings from pytest can mask underlying problems in your tests.

### 6.2. Edge Cases

- **Empty inputs:** Test your code with empty inputs to ensure it handles them gracefully.
- **Invalid inputs:** Test your code with invalid inputs to ensure it handles them correctly and raises appropriate exceptions.
- **Boundary conditions:** Test your code with boundary conditions to ensure it handles them correctly.

### 6.3. Version-Specific Issues

- **Check release notes:** Check the release notes for each new version of pytest to be aware of any breaking changes or new features.
- **Pin dependencies:** Pin your pytest dependency to a specific version to avoid unexpected behavior caused by updates.

### 6.4. Compatibility Concerns

- **Check compatibility:** Check the compatibility of pytest with other technologies you are using, such as specific versions of Python or Django.

### 6.5. Debugging Strategies

- **Use `--pdb`:** Use the `--pdb` option to drop into the Python debugger when a test fails.
- **Use logging:** Use logging to add debugging information to your tests.
- **Simplify tests:** Simplify failing tests to isolate the cause of the failure.

## 7. Tooling and Environment

### 7.1. Recommended Development Tools

- **IDE:** Use a good IDE with pytest support, such as VS Code with the Python extension, PyCharm, or Sublime Text with the appropriate plugins.
- **pytest-watch:** Use `pytest-watch` for automatic test rerunning on file changes. `pip install pytest-watch`, then run `ptw`.

### 7.2. Build Configuration

- **Use `pyproject.toml`:** Use a `pyproject.toml` file to configure your pytest settings.

  toml
  [tool.pytest.ini_options]
  addopts = [
      "--cov=my_app",
      "--cov-report term-missing",
      "-v",
  ]
  testpaths = [
      "tests",
  ]
  

### 7.3. Linting and Formatting

- **Use `flake8-pytest-style`:** Use the `flake8-pytest-style` plugin to enforce pytest-specific coding standards.  `pip install flake8 flake8-pytest-style`
- **Use `black` or `autopep8`:** Use a code formatter like `black` or `autopep8` to ensure consistent code formatting.  `pip install black`, then run `black .`

### 7.4. Deployment

- **Include tests in your deployment pipeline:** Ensure your tests are run as part of your deployment pipeline to prevent regressions.
- **Use a dedicated test environment:** Use a dedicated test environment to avoid interfering with your production environment.

### 7.5. CI/CD Integration

- **Integrate with CI/CD:** Integrate pytest with your CI/CD system, such as GitHub Actions, GitLab CI, or Jenkins, to automatically run your tests on every commit.

  Example GitHub Actions workflow (`.github/workflows/test.yml`):

  
  name: Test
  on:
    push:
      branches: [ main ]
    pull_request:
      branches: [ main ]
  jobs:
    build:
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v3
        - name: Set up Python 3.10
          uses: actions/setup-python@v3
          with:
            python-version: "3.10"
        - name: Install dependencies
          run: |
            python -m pip install --upgrade pip
            pip install pytest pytest-cov flake8 flake8-pytest-style black
            pip install -e .  # Install your project in editable mode
        - name: Lint with flake8
          run: |
            flake8 .
        - name: Test with pytest
          run: |
            pytest --cov --cov-report xml
        - name: Upload coverage to Codecov
          uses: codecov/codecov-action@v3
          with:
            token: ${{ secrets.CODECOV_TOKEN }}
            flags: unittests
            env_vars: OS,PYTHON
            name: codecov-pytest
  

By following these best practices, you can write effective and maintainable tests with pytest, improving the quality and reliability of your Python applications.
</file>

<file path=".cursor/rules/rich.mdc">
---
description: Comprehensive best practices and coding standards for the Rich library, focusing on code quality, performance, and maintainability within Python terminal applications.
globs: **/*.py
---
- **General Guidelines**
  - Adhere to PEP 8 coding style guidelines for Python code, emphasizing readability and consistency.
  - Limit lines to a maximum of 79 characters to enhance readability across different environments.
  - Prioritize code clarity and simplicity, making it easy to understand and maintain.
  - Use UTF-8 encoding for source files to ensure compatibility with a wide range of characters.

- **Installation and Environment**
  - Use `uv` for installing dependencies to improve speed and reliability, if appropriate.
  - Specify Python 3.12 or later to leverage the latest language features and performance improvements.  (If applicable for the specific features of rich you are using)

- **Code Organization and Structure**

  - **Directory Structure:** Follow a logical directory structure.
    
    project_root/
    ├── src/
    │   ├── main.py        # Entry point of the application
    │   ├── utils/
    │   │   ├── __init__.py
    │   │   ├── helper_functions.py
    │   ├── modules/
    │   │   ├── __init__.py
    │   │   ├── module_a.py  # Rich-related components
    │   │   ├── module_b.py
    ├── tests/
    │   ├── __init__.py
    │   ├── test_main.py
    │   ├── test_module_a.py
    ├── README.md
    ├── pyproject.toml  # Or requirements.txt
    
  - **File Naming:** Use descriptive lowercase names with underscores (e.g., `console_output.py`).
  - **Module Organization:** Group related functionalities into separate modules (e.g., `rich_display.py`, `data_formatting.py`).
  - **Component Architecture:** Design modular components with clear interfaces for reusability and maintainability.
  - **Code Splitting:**  Break down large files into smaller, more manageable pieces based on functionality. Consider lazy loading of less-frequently used modules.

- **Coding Style and Best Practices**

  - **Indentation:** Use 4 spaces for indentation.
  - **Blank Lines:** Separate top-level functions and classes with two blank lines, and methods within a class with one blank line.
  - **Imports:**
    - Group imports in the following order:
      1. Standard library imports
      2. Third-party library imports (including Rich)
      3. Local application imports
    - Use absolute imports for clarity, unless relative imports significantly improve readability within a package.
    - Avoid wildcard imports (`from module import *`).
  - **Naming Conventions:**
    - Use lowercase with underscores for function and variable names (`my_variable`, `my_function`).
    - Use CapWords for class names (`MyClass`).
    - Use UPPER_CASE_WITH_UNDERSCORES for constants (`MAX_VALUE`).
  - **String Quotes:** Use double quotes consistently for strings, especially for docstrings.

- **Rich Library Specific Best Practices**

  - **Console Instantiation:** Create a single `Console` instance for your application and reuse it throughout.
    python
    from rich.console import Console

    console = Console()

    def my_function():
        console.print("Hello, [bold red]World![/bold red]")
    
  - **Styling:** Use Rich's markup system for styling text.  Refer to the Rich documentation for available styles and their usage.
    python
    console.print("[link=https://example.com]Click here[/link] to visit example.com")
    
  - **Tables:** Utilize the `Table` class for structured data display.  Configure columns and rows appropriately.
    python
    from rich.table import Table

    table = Table(title="My Data")
    table.add_column("Name", style="cyan", no_wrap=True)
    table.add_column("Age", style="magenta")
    table.add_row("Alice", "30")
    table.add_row("Bob", "25")
    console.print(table)
    
  - **Progress Bars:** Employ the `Progress` class for tracking long-running tasks.  Configure the progress bar to accurately reflect the task's progress.
    python
    from rich.progress import Progress
    import time

    with Progress() as progress:
        task1 = progress.add_task("[red]Downloading...", total=1000)
        task2 = progress.add_task("[green]Processing...", total=100)

        while not progress.finished:
            progress.update(task1, advance=0.5)
            progress.update(task2, advance=0.1)
            time.sleep(0.01) #Simulate some work
    
  - **Inspect:** Use `console.inspect()` for debugging and understanding objects.  This is invaluable for exploring Rich's own classes and objects, or any object in your application.
    python
    from rich.panel import Panel

    panel = Panel("Hello, World!")
    console.inspect(panel, methods=True)
    

- **Common Patterns and Anti-patterns**
  - **Design Patterns:**  Employ appropriate design patterns such as Factory Pattern for creating Rich objects or Strategy Pattern for different output styles.
  - **Recommended Approaches:**  Use Rich's features for displaying data structures (lists, dictionaries) in a human-readable format.
  - **Anti-patterns:**  Avoid directly printing to the console using `print()` when you should be using `console.print()` to take advantage of Rich's features. Avoid excessive nesting of Rich markup, which can reduce readability.
  - **State Management:** Manage the state of Rich components appropriately, especially when creating dynamic displays.  Avoid mutating objects directly without updating the console output.
  - **Error Handling:** Handle exceptions gracefully and use Rich's features to display error messages to the user in a clear and informative way.
    python
    try:
        result = 1 / 0
    except Exception as e:
        console.print_exception(show_locals=True)
    

- **Performance Considerations**
  - **Optimization Techniques:** Use Rich's built-in caching mechanisms to avoid re-rendering the same content repeatedly.  Minimize the use of computationally expensive Rich features when performance is critical.
  - **Memory Management:** Be mindful of memory usage when displaying large amounts of data with Rich. Consider using generators or iterators to process data in chunks.
  - **Rendering Optimization:** If applicable, profile your Rich-based application to identify rendering bottlenecks. Optimize the rendering of complex Rich elements by simplifying the markup or reducing the number of elements.
  - **Bundle Size Optimization:** Not directly applicable since rich is primarily server-side for terminal apps but for web integrated terminals, ensure only necessary Rich dependencies are bundled.
  - **Lazy Loading:** Not directly applicable, but relevant for other parts of your application that might interact with Rich.

- **Security Best Practices**
  - **Vulnerabilities:** Be aware of potential vulnerabilities related to untrusted input. Sanitize any user-provided text before displaying it with Rich to prevent markup injection attacks.
  - **Input Validation:** Validate any input before using it in Rich's markup to prevent unexpected behavior or security issues.
  - **Authentication and Authorization:** Not directly applicable to Rich, but ensure proper authentication and authorization mechanisms are in place for any data displayed by Rich.
  - **Data Protection:** Protect sensitive data by masking or redacting it before displaying it with Rich.  Utilize Rich's styling options to emphasize sensitive data that requires special attention.

- **Testing Approaches**
  - **Unit Testing:** Write unit tests for individual Rich components to ensure they function correctly.  Mock the `Console` object to isolate the component under test.
  - **Integration Testing:** Test the integration of Rich components with other parts of your application to ensure they work together seamlessly.
  - **End-to-end Testing:** Verify the overall behavior of your Rich-based application by simulating user interactions and validating the output displayed on the console.
  - **Test Organization:** Organize your tests into logical modules that correspond to the structure of your application.
  - **Mocking and Stubbing:** Use mocking and stubbing techniques to isolate components and simulate dependencies during testing.  The `unittest.mock` module provides tools for creating mock objects.

- **Common Pitfalls and Gotchas**
  - **Frequent Mistakes:** Forgetting to import the `Console` class or using `print()` instead of `console.print()`.  Overcomplicating Rich markup, leading to unreadable code.
  - **Edge Cases:** Handling Unicode characters correctly.  Dealing with terminals that have limited color support. Handling very large data sets, which can cause memory issues or performance problems.
  - **Version-Specific Issues:** Being aware of breaking changes or new features in different versions of Rich. Consult the Rich changelog for details.
  - **Compatibility Concerns:** Ensuring compatibility between Rich and other terminal libraries or frameworks you are using.
  - **Debugging:** Using `console.inspect()` to explore objects and identify issues.  Setting breakpoints and stepping through the code to understand the flow of execution.

- **Tooling and Environment**
  - **Recommended Tools:** VS Code with the Python extension, PyCharm, or any other IDE that supports Python development. Consider using a Rich-specific plugin if available.
  - **Build Configuration:** Use `pyproject.toml` (preferred) or `requirements.txt` to manage project dependencies, including Rich.
  - **Linting and Formatting:** Use Pylint, Flake8, and Black to enforce code style guidelines and catch potential errors.
  - **Deployment:** Ensure the target environment has Python and Rich installed. Consider using a virtual environment to isolate dependencies.
  - **CI/CD Integration:** Integrate Rich-based applications into your CI/CD pipeline to automate testing and deployment.  Use tools like Jenkins, GitLab CI, or GitHub Actions.

- **Additional Notes**
  - Consult the official Rich documentation (https://rich.readthedocs.io) for the most up-to-date information and examples.
  - Explore Rich's examples and demonstrations to learn more about its capabilities.
  - Contribute to the Rich community by reporting bugs, suggesting features, or submitting pull requests.

- **References**
  - PEP 8: Style Guide for Python Code (https://peps.python.org/pep-0008/)
  - Rich Documentation: (https://rich.readthedocs.io)
</file>

<file path=".cursor/rules/tqdm.mdc">
---
description: This rule file provides best practices and coding standards for using the `tqdm` library in Python. It focuses on performance, customization, and avoiding common pitfalls.
globs: **/*.py
---
# tqdm Best Practices and Coding Standards

This document outlines best practices for using the `tqdm` library in Python, focusing on simplicity, customization, performance optimization, and avoiding common pitfalls. Adhering to these guidelines will help you create efficient and informative progress bars for your projects.

## Library Information

- Name: tqdm
- Tags: python, utilities, progress-bar, command-line

## 1. Basic Usage and Integration

- **Wrap Iterables Directly:** The simplest and most common way to use `tqdm` is by wrapping your iterable object directly with the `tqdm()` function.

  python
  from tqdm import tqdm

  for item in tqdm(my_iterable):
      # Process item
      ...
  

- **Descriptive Progress Bars:** Always use the `desc` parameter to add a short, descriptive text to your progress bar, providing context to the user.

  python
  for item in tqdm(my_iterable, desc="Processing Data"):
      ...
  

- **Integration with Pandas:**  Use `tqdm` with Pandas `apply` functions for data analysis tasks.

  python
  import pandas as pd
  from tqdm import tqdm

  tqdm.pandas()
  df['column'].progress_apply(lambda x: some_function(x))
  

## 2. Performance Considerations

- **Update Frequency:** Avoid updating the progress bar too frequently, as it can significantly impact performance, especially with large datasets or computationally intensive tasks. Adjust `mininterval` and `maxinterval` to optimize refresh rates.

  python
  for item in tqdm(my_iterable, desc="Processing Data", mininterval=1, maxinterval=10):
      ...
  

- **Manual Control for Performance:** In scenarios where automatic iteration tracking isn't feasible, use manual control with `tqdm` to update the progress bar at strategic intervals.

  python
  from tqdm import tqdm
  import time

  total_iterations = 1000
  with tqdm(total=total_iterations, desc="Manual Progress") as pbar:
      for i in range(total_iterations):
          # Perform some operation
          time.sleep(0.001) # simulate work
          if i % 10 == 0:
              pbar.update(10) # update after every 10 iterations
  

- **`tqdm.write()` for Printing:**  Use `tqdm.write()` to print messages to the console without disrupting the progress bar. This is especially useful for logging information or displaying intermediate results.

  python
  from tqdm import tqdm

  for i in tqdm(range(100), desc='Processing'):
      if i % 10 == 0:
          tqdm.write(f'Iteration {i}: Some intermediate result')
  

## 3. Nested Progress Bars

- **`leave=False` for Nested Bars:** When using nested loops with `tqdm`, set `leave=False` for inner loops to prevent cluttering the output. This ensures that only the outer loop's progress bar remains after the inner loop completes.

  python
  from tqdm import tqdm
  import time

  for i in tqdm(range(5), desc="Outer Loop", leave=True):
      for j in tqdm(range(3), desc="Inner Loop", leave=False):
          time.sleep(0.1)
  

## 4. Customization and Advanced Features

- **Dynamic Descriptions:** Update the progress bar description dynamically during iterations to provide more context-specific information.

  python
  from tqdm import tqdm
  import time

  with tqdm(range(10), desc="Starting") as pbar:
      for i in pbar:
          time.sleep(0.5)
          pbar.set_description(f"Step {i+1} completed")
  

- **Custom Formatting:** Customize the appearance of the progress bar using `bar_format` to control the layout, colors, and displayed information.

  python
  from tqdm import tqdm
  import time

  for i in tqdm(range(5), bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}]"):
      time.sleep(0.5)
  

- **GUI Mode (Jupyter Notebooks):** Use `tqdm.notebook` for a more visually appealing progress bar in Jupyter notebooks.  Import `tqdm.notebook` rather than `tqdm`.

  python
  from tqdm.notebook import tqdm
  import time

  for i in tqdm(range(1000)):
      time.sleep(0.001)
  

## 5. Common Pitfalls and Anti-Patterns

- **Over-Updating:** Updating the progress bar too frequently is a common mistake.  This can significantly slow down your code.  Adjust `mininterval` and `maxinterval`, or use manual updates.

- **Ignoring `leave=False` in Nested Loops:** Forgetting to set `leave=False` in nested loops can lead to cluttered output, making it difficult to read the progress of the outer loop.

- **Not Closing the Progress Bar:**  If you're using manual control, ensure you close the progress bar with `pbar.close()` to release resources.

- **Incorrect Total Value:** Providing an incorrect `total` value in the `tqdm()` constructor can lead to inaccurate progress display. Double-check the iterable's length.

- **Using `print()` Within the Loop:** Using the standard `print()` function within the loop can disrupt the progress bar display.  Use `tqdm.write()` instead.

## 6. Testing Approaches

- **Unit Tests:** When using `tqdm` in functions, test the function's core logic independently of `tqdm`. If you need to verify `tqdm` output, consider capturing standard output for assertions, though this is generally less valuable than testing the core function logic.

- **Integration Tests:** Ensure that `tqdm` integrates correctly with your data processing pipelines. Verify that the progress bars are displayed accurately and don't introduce unexpected performance bottlenecks.

## 7. Tooling and Environment

- **Development Tools:**  Use standard Python development tools like VS Code, PyCharm, or Jupyter Notebooks for working with `tqdm`.

- **Linting and Formatting:** Adhere to PEP 8 style guidelines and use linters like `flake8` or `pylint` to maintain code quality. Format your code with `black` or `autopep8` for consistency.

## 8. Example: Downloading Files with Progress

python
import requests
from tqdm import tqdm


def download_file(url, filename):
    """Downloads a file from a URL with a progress bar."""
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
        total_size = int(response.headers.get('content-length', 0))
        block_size = 8192  # 8KB
        with tqdm(desc=filename, total=total_size, unit='B', unit_scale=True, unit_divisor=1024) as pbar:
            with open(filename, 'wb') as f:
                for data in response.iter_content(block_size):
                    f.write(data)
                    pbar.update(len(data))
        print(f"Download complete: {filename}")
    except requests.exceptions.RequestException as e:
        print(f"Error downloading {url}: {e}")
    except IOError as e:
        print(f"Error writing to file {filename}: {e}")


# Example usage:
file_url = "https://www.example.com/large_file.zip"  # Replace with an actual URL
file_name = "large_file.zip"
download_file(file_url, file_name)


## 9. Conclusion

By following these best practices, you can effectively leverage the `tqdm` library to create informative and efficient progress bars in your Python projects, improving the user experience and providing valuable insights into the execution of your code.
</file>

<file path=".github/workflows/integration-tests.yml">
# This workflow will run integration tests for the current project once per day

name: Integration Tests

on:
  schedule:
    - cron: "37 14 * * *" # Run at 7:37 AM Pacific Time (14:37 UTC) every day
  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI

# If another scheduled run starts while this workflow is still running,
# cancel the earlier run in favor of the next run.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  integration-tests:
    name: Integration Tests
    strategy:
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.11", "3.12"]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          uv venv
          uv pip install -r pyproject.toml
          uv pip install -U pytest-asyncio vcrpy
      - name: Run integration tests
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
          LANGSMITH_TRACING: true
          LANGSMITH_TEST_CACHE: tests/cassettes
        run: |
          uv run pytest tests/integration_tests
</file>

<file path=".github/workflows/unit-tests.yml">
# This workflow will run unit tests for the current project

name: CI

on:
  push:
    branches: ["main"]
  pull_request:
  workflow_dispatch: # Allows triggering the workflow manually in GitHub UI

# If another push to the same PR or branch happens while this workflow is still running,
# cancel the earlier run in favor of the next run.
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  unit-tests:
    name: Unit Tests
    strategy:
      matrix:
        os: [ubuntu-latest]
        python-version: ["3.11", "3.12"]
    runs-on: ${{ matrix.os }}
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          uv venv
          uv pip install -r pyproject.toml
      - name: Lint with ruff
        run: |
          uv pip install ruff
          uv run ruff check .
      - name: Lint with mypy
        run: |
          uv pip install mypy
          uv run mypy --strict src/
      - name: Check README spelling
        uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_file: .codespellignore
          path: README.md
      - name: Check code spelling
        uses: codespell-project/actions-codespell@v2
        with:
          ignore_words_file: .codespellignore
          path: src/
      - name: Run tests with pytest
        run: |
          uv pip install pytest
          uv run pytest tests/unit_tests
</file>

<file path="src/react_agent/graphs/research.py">
"""Enhanced modular research framework using LangGraph.

This module implements a modular approach to the research process,
with specialized components for different research categories and
improved error handling and validation.
"""


from __future__ import annotations
import contextlib
import json
import asyncio
from typing import Any, Dict, List, Optional, Sequence, Union, cast, Tuple, Literal, Hashable, Set
from datetime import datetime, timezone
from urllib.parse import urlparse
import re

from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages
from langgraph.constants import START
from langgraph.graph.state import CompiledStateGraph
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.runnables import RunnableConfig, ensure_config
from langchain_core.documents import Document
from typing_extensions import Annotated, TypedDict

from react_agent.utils.validations import is_valid_url
from react_agent.utils.llm import call_model, call_model_json
from react_agent.tools.jina import search
from react_agent.prompts.research import (
    QUERY_ANALYSIS_PROMPT,
    CLARIFICATION_PROMPT,
    EXTRACTION_PROMPTS,
    SYNTHESIS_PROMPT,
    VALIDATION_PROMPT,
    SEARCH_QUALITY_THRESHOLDS,
    get_extraction_prompt,
    get_default_extraction_result
)
from react_agent.prompts.synthesis import ENHANCED_REPORT_TEMPLATE
from react_agent.utils.logging import get_logger, log_dict, info_highlight, warning_highlight, error_highlight, log_step
from react_agent.utils.content import (
    preprocess_content,
    should_skip_content,
    chunk_text,
    merge_chunk_results,
    validate_content,
    detect_content_type
)
from react_agent.utils.extraction import (
    extract_statistics,
    enrich_extracted_fact,
    extract_category_information as extract_category_info
)
from react_agent.utils.statistics import (
    calculate_category_quality_score,
    calculate_overall_confidence,
    assess_synthesis_quality
)
from react_agent.prompts.query import (
    optimize_query,
    detect_vertical,
    expand_acronyms
)
from react_agent.utils.cache import create_checkpoint, load_checkpoint, cache_result

# Initialize logger
logger = get_logger(__name__)

# Define SearchType as a Literal type
SearchType = Literal['general', 'authoritative', 'recent', 'comprehensive', 'technical']

# Add at module level after imports
from typing import Dict, List, Any, Optional, Tuple, cast

class ResearchCategory(TypedDict):
    """State for a specific research category."""
    category: str  # The category being researched (market_dynamics, etc.)
    query: str  # The search query for this category
    search_results: List[Dict[str, Any]]  # Raw search results
    extracted_facts: List[Dict[str, Any]]  # Extracted facts
    sources: List[Dict[str, Any]]  # Source information
    complete: bool  # Whether this category is complete
    quality_score: float  # Quality score for this category (0.0-1.0)
    retry_count: int  # Number of retry attempts
    last_search_query: Optional[str]  # Last search query used
    status: str  # Status of this category (pending, in_progress, complete, failed)
    statistics: List[Dict[str, Any]]  # Extracted statistics from facts
    confidence_score: float  # Confidence score for this category (0.0-1.0)
    cross_validation_score: float  # Cross-validation score for facts (0.0-1.0)
    source_quality_score: float  # Quality score for sources (0.0-1.0)
    recency_score: float  # Recency score for sources (0.0-1.0)
    statistical_content_score: float  # Score for statistical content (0.0-1.0)

class ResearchState(TypedDict):
    """Main research state with modular components."""
    # Basic conversation data
    messages: Annotated[Sequence[BaseMessage], add_messages]

    # Original query and analysis
    original_query: str
    query_analysis: Optional[Dict[str, Any]]

    # Clarity and context
    missing_context: List[str]
    needs_clarification: bool
    clarification_request: Optional[str]
    human_feedback: Optional[str]

    # Category-specific research
    categories: Dict[str, ResearchCategory]

    # Synthesis and validation
    synthesis: Optional[Dict[str, Any]]
    validation_result: Optional[Dict[str, Any]]

    # Overall status
    status: str
    error: Optional[Dict[str, Any]]
    complete: bool
    
class GraphState(TypedDict):
    """Type definition for the research graph state."""
    query: str
    iteration: int
    max_iterations: int
    messages: List[BaseMessage]
    urls: Set[str]
    findings: List[str]
    search_results: Optional[List[Document]]
    credibility: Optional[str]
    gaps: Optional[List[str]]
    decision: Optional[Literal["continue", "complete"]]
    reasoning: Optional[str]

# --------------------------------------------------------------------
# 2. Core control flow nodes
# --------------------------------------------------------------------

async def initialize_research(state: ResearchState) -> Dict[str, Any]:
    """Initialize the research process with the user's query."""
    log_step("Initializing research process", 1, 10)

    if not state["messages"]:
        warning_highlight("No messages found in state")
        return {"error": {"message": "No messages in state", "phase": "initialization"}}

    last_message = state["messages"][-1]
    query = last_message.content if isinstance(last_message, BaseMessage) else ""

    if not query:
        warning_highlight("Empty query")
        return {"error": {"message": "Empty query", "phase": "initialization"}}

    info_highlight(f"Initializing research for query: {query}")

    categories = {
        category: {
            "category": category,
            "query": "",  # Will be filled by query analysis
            "search_results": [],
            "extracted_facts": [],
            "sources": [],
            "complete": False,
            "quality_score": 0.0,
            "retry_count": 0,
            "last_search_query": None,
            "status": "pending",
            "statistics": [],
            "confidence_score": 0.0,
            "cross_validation_score": 0.0,
            "source_quality_score": 0.0,
            "recency_score": 0.0,
            "statistical_content_score": 0.0
        }
        for category in SEARCH_QUALITY_THRESHOLDS.keys()
    }
    return {
        "original_query": query,
        "status": "initialized",
        "categories": categories,
        "missing_context": [],
        "needs_clarification": False,
        "complete": False
    }

async def analyze_query(state: ResearchState, config: Optional[RunnableConfig] = None) -> Dict[str, Any]:
    """Analyze the query to determine research categories and search terms."""
    log_step("Analyzing research query", 2, 10)

    query = state["original_query"].strip()
    if not query:
        warning_highlight("No query to analyze")
        return {"error": {"message": "No query to analyze", "phase": "query_analysis"}}

    if human_feedback := state.get("human_feedback", ""):
        info_highlight(f"Including user feedback in analysis: {human_feedback}")
        query = f"{query}\n\nAdditional context: {human_feedback}"

    # Prepare the analysis prompt
    analysis_prompt = QUERY_ANALYSIS_PROMPT.format(query=query)

    try:
        analysis_result = await call_model_json(
            messages=[{"role": "human", "content": analysis_prompt}],
            config=ensure_config(config)
        )

        # Ensure the response has the required structure
        if not isinstance(analysis_result, dict):
            error_highlight("Invalid response format from query analysis")
            return {"error": {"message": "Invalid response format", "phase": "query_analysis"}}

        # Initialize default values
        analysis_result = {
            "unspsc_categories": analysis_result.get("unspsc_categories", []),
            "search_components": analysis_result.get("search_components", {
                "primary_topic": "",
                "industry": "",
                "product_type": "",
                "geographical_focus": ""
            }),
            "search_terms": analysis_result.get("search_terms", {
                "market_dynamics": [],
                "provider_landscape": [],
                "technical_requirements": [],
                "regulatory_landscape": [],
                "cost_considerations": [],
                "best_practices": [],
                "implementation_factors": []
            }),
            "boolean_query": analysis_result.get("boolean_query", ""),
            "missing_context": analysis_result.get("missing_context", [])
        }

        log_dict(analysis_result, title="Query Analysis Result")

        # Check for missing context
        missing_context = analysis_result.get("missing_context", [])
        needs_clarification = len(missing_context) >= 2  # Only request clarification for multiple missing elements

        # Update category queries based on analysis
        categories = state["categories"]
        for category, category_state in categories.items():
            if search_terms := analysis_result.get("search_terms", {}).get(
                category, []
            ):
                # Detect vertical from the query
                vertical = detect_vertical(query)

                # Create optimized query for this category using the enhanced optimization
                optimized_query = optimize_query(
                    original_query=query,
                    category=category,
                    vertical=vertical,
                    include_all_keywords=len(search_terms) > 5  # Include all keywords for complex queries
                )
                category_state["query"] = optimized_query
                info_highlight(f"Set optimized query for {category}: {optimized_query}")
            else:
                # Use default query if no specific terms
                category_state["query"] = query

        return {
            "query_analysis": analysis_result,
            "categories": categories,
            "missing_context": missing_context,
            "needs_clarification": needs_clarification,
            "status": "analyzed"
        }
    except Exception as e:
        error_highlight(f"Error in query analysis: {str(e)}")
        return {"error": {"message": f"Error in query analysis: {str(e)}", "phase": "query_analysis"}}

async def request_clarification(state: ResearchState) -> Dict[str, Any]:
    """Request clarification from the user for missing context."""
    log_step("Requesting clarification", 3, 10)
    
    missing_context = state["missing_context"]
    if not missing_context:
        return {"needs_clarification": False}
    
    # Format the missing context items
    missing_sections = "\n".join([f"- {item}" for item in missing_context])
    
    # Get analysis data
    analysis = state.get("query_analysis", {})
    search_components = analysis.get("search_components", {}) if analysis else {}
    
    # Prepare the clarification request
    try:
        clarification_message = CLARIFICATION_PROMPT.format(
            query=state["original_query"],
            product_vs_service=search_components.get("product_type", "Unknown"),
            industry_context=search_components.get("industry", "Unknown"),
            geographical_focus=search_components.get("geographical_focus", "Unknown"),
            missing_sections=missing_sections
        )
        
        info_highlight("Generated clarification request")
        
        # Set up interrupt for user input
        return {
            "clarification_request": clarification_message,
            "__interrupt__": {
                "value": {
                    "question": clarification_message,
                    "missing_context": missing_context
                },
                "resumable": True,
                "ns": ["request_clarification"],
                "when": "during"
            }
        }
    except Exception as e:
        error_highlight(f"Error creating clarification request: {str(e)}")
        # Proceed without clarification in case of error
        return {"needs_clarification": False}

async def process_clarification(state: ResearchState) -> Dict[str, Any]:
    """Process user clarification and update the research context."""
    log_step("Processing clarification", 4, 10)
    
    if not state["messages"]:
        warning_highlight("No messages found for clarification")
        return {}
    
    # Get the latest message which should contain the user's clarification
    last_message = state["messages"][-1]
    clarification_content = str(last_message.content).strip()
    
    info_highlight(f"Processing user clarification: {clarification_content}")
    
    # Update the original query with the clarification
    updated_query = f"{state['original_query']}\n\nAdditional context: {clarification_content}"
    
    return {
        "human_feedback": clarification_content,
        "original_query": updated_query,  # Update the original query with clarification
        "needs_clarification": False,
        "missing_context": [],
        "status": "clarified"
    }

# --------------------------------------------------------------------
# 3. Module-specific research nodes
# --------------------------------------------------------------------

async def execute_category_search(
    state: ResearchState, 
    category: str,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Execute search for a specific research category."""
    log_step(f"Executing search for category: {category}", 5, 10)
    
    categories = state["categories"]
    if category not in categories:
        warning_highlight(f"Unknown category: {category}")
        return {}
    
    category_state = categories[category]
    query = category_state["query"]
    
    if not query:
        warning_highlight(f"No query available for category: {category}")
        return {}
    
    # Update status
    category_state["status"] = "searching"
    category_state["retry_count"] += 1
    
    # Get category-specific search parameters
    thresholds = SEARCH_QUALITY_THRESHOLDS.get(category, {})
    recency_days = int(str(thresholds.get("recency_threshold_days", 365)), base=10)
    
    # Configure search params based on category
    search_type_mapping: Dict[str, SearchType] = {
        "market_dynamics": "recent",  # Need fresh market data
        "provider_landscape": "comprehensive",  # Need diverse providers
        "technical_requirements": "technical",  # Technical content
        "regulatory_landscape": "authoritative",  # Authoritative sources
        "cost_considerations": "recent",  # Fresh pricing data
        "best_practices": "comprehensive",  # Diverse practices
        "implementation_factors": "comprehensive"  # Diverse approaches
    }
    
    search_type = search_type_mapping.get(category, "general")
    
    info_highlight(f"Executing {search_type} search for {category} with query: {query}")
    
    try:
        # Keep track of the query for retry tracking
        category_state["last_search_query"] = query
        
        # Execute the search with category parameter
        search_results = await search(
            query=query,
            search_type=search_type,
            recency_days=recency_days,
            category=category,  # Pass the category parameter
            config=ensure_config(config)
        )
        
        if not search_results:
            warning_highlight(f"No search results for {category}")
            category_state["status"] = "search_failed"
            return {"categories": categories}
        
        # Convert to the expected format and filter problematic content
        formatted_results = []
        for doc in search_results:
            url = doc.metadata.get("url", "")
            
            # Skip problematic content types
            if should_skip_content(url):
                info_highlight(f"Skipping problematic content type: {url}")
                continue
                
            # Validate and detect content type
            content = doc.page_content
            if not validate_content(content):
                info_highlight(f"Invalid content from {url}, skipping")
                continue
                
            content_type = detect_content_type(url, content)
            info_highlight(f"Detected content type: {content_type} for {url}")
            
            result = {
                "url": url,
                "title": doc.metadata.get("title", ""),
                "snippet": content,
                "source": doc.metadata.get("source", ""),
                "quality_score": doc.metadata.get("quality_score", 0.5),
                "published_date": doc.metadata.get("published_date"),
                "content_type": content_type
            }
            formatted_results.append(result)
        
        # Update the category state
        category_state["search_results"] = formatted_results
        category_state["status"] = "searched"
        
        info_highlight(f"Found {len(formatted_results)} results for {category}")
        return {"categories": categories}
        
    except Exception as e:
        error_highlight(f"Error in search for {category}: {str(e)}")
        category_state["status"] = "search_failed"
        return {"categories": categories}

async def _process_search_result(
    result: Dict[str, Any],
    category: str,
    original_query: str,
    config: Optional[RunnableConfig] = None
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """Process a single search result and extract information."""
    url = result.get("url", "")
    if not url or should_skip_content(url):
        return [], [], []
    
    # Check checkpoint first
    cache_key = f"search_result_{url}_{category}"
    if cached_state := load_checkpoint(cache_key):
        info_highlight(f"Using cached result for {url} in {category}")
        return cached_state.get("facts", []), cached_state.get("sources", []), cached_state.get("statistics", [])
        
    content = result.get("snippet", "")
    if not validate_content(content):
        return [], [], []
        
    content_type = detect_content_type(url, content)
    prompt_template = get_extraction_prompt(
        category=category,
        query=original_query,
        url=url,
        content=content
    )
    
    try:
        facts, relevance_score = await extract_category_info(
            content=content,
            url=url,
            title=result.get("title", ""),
            category=category,
            original_query=original_query,
            prompt_template=prompt_template,
            extraction_model=call_model_json,
            config=ensure_config(config)
        )
        
        statistics = extract_statistics(content) or []
        
        # Add source information to facts
        for fact in facts:
            fact["source_url"] = url
            fact["source_title"] = result.get("title", "")
            
        source = {
            "url": url,
            "title": result.get("title", ""),
            "published_date": result.get("published_date"),
            "fact_count": len(facts),
            "relevance_score": relevance_score,
            "quality_score": result.get("quality_score", 0.5),
            "content_type": content_type
        }
        
        result_tuple = (facts, [source], statistics)
        
        # Save to checkpoint with TTL
        create_checkpoint(
            cache_key,
            {
                "facts": facts,
                "sources": [source],
                "statistics": statistics,
                "timestamp": datetime.now(timezone.utc).isoformat()
            },
            ttl=3600  # 1 hour TTL
        )
        
        return result_tuple
        
    except Exception as e:
        warning_highlight(f"Error extracting from {url}: {str(e)}")
        return [], [], []

async def extract_category_information(
    state: ResearchState,
    category: str,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Extract information from search results for a specific category."""
    log_step(f"Extracting information for category: {category}", 6, 10)
    
    categories = state["categories"]
    if category not in categories:
        warning_highlight(f"Unknown category: {category}")
        return {}
    
    category_state = categories[category]
    search_results = category_state["search_results"]
    
    if not search_results:
        category_state["status"] = "extraction_failed"
        return {"categories": categories}
    
    category_state["status"] = "extracting"
    
    # Process all search results in parallel
    tasks = [
        asyncio.create_task(_process_search_result(
            result, category, state["original_query"], config
        ))
        for result in search_results
    ]
    
    results = await asyncio.gather(*tasks)
    
    # Aggregate results
    extracted_facts = []
    sources = []
    all_statistics = []
    
    for facts, source, statistics in results:
        extracted_facts.extend(facts)
        sources.extend(source)
        all_statistics.extend(statistics)
    
    # Update category state with quality scores from statistics module
    thresholds = SEARCH_QUALITY_THRESHOLDS.get(category, {})
    quality_score = calculate_category_quality_score(
        category=category,
        extracted_facts=extracted_facts,
        sources=sources,
        thresholds=thresholds
    )
    
    category_state.update({
        "extracted_facts": extracted_facts,
        "sources": sources,
        "statistics": all_statistics,
        "quality_score": quality_score
    })
    
    # Calculate derived scores based on quality_score
    category_state.update({
        "confidence_score": quality_score,
        "cross_validation_score": quality_score * 0.8,
        "source_quality_score": quality_score * 0.9,
        "recency_score": quality_score * 0.7,
        "statistical_content_score": quality_score * 0.85
    })
    
    # Determine completion status
    min_facts = thresholds.get("min_facts", 3)
    min_sources = thresholds.get("min_sources", 2)
    
    category_state["complete"] = (
        len(extracted_facts) >= min_facts and len(sources) >= min_sources
    ) or category_state["retry_count"] >= 3
    
    category_state["status"] = "extracted" if category_state["complete"] else "extraction_incomplete"
    
    return {"categories": categories}

async def execute_research_for_categories(
    state: ResearchState,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Execute research for all categories in parallel with improved caching."""
    log_step("Executing research for all categories", 5, 10)
    
    categories = state["categories"]
    research_tasks = []
    
    # Track completed categories to avoid reprocessing
    completed_categories = set()
    
    for category, category_state in categories.items():
        if category_state["complete"]:
            info_highlight(f"Category {category} already complete, skipping")
            completed_categories.add(category)
            continue
            
        # Check checkpoint for this category
        cache_key = f"category_{state['original_query']}_{category}"
        if cached_state := load_checkpoint(cache_key):
            info_highlight(f"Using cached results for category: {category}")
            category_state["search_results"] = cached_state.get("search_results", [])
            category_state["extracted_facts"] = cached_state.get("extracted_facts", [])
            category_state["sources"] = cached_state.get("sources", [])
            category_state["complete"] = True
            completed_categories.add(category)
            continue
        
        # Execute search followed by extraction
        info_highlight(f"Adding research task for {category}")
        
        # Create async task for this category
        async def process_category(cat: str, cat_state: Dict[str, Any]) -> None:
            try:
                # Execute search
                search_result = await execute_category_search(state, cat, config)
                
                # Only extract if search was successful
                if cat in search_result.get("categories", {}) and search_result["categories"][cat]["status"] == "searched":
                    # Execute extraction
                    extraction_result = await extract_category_information(state, cat, config)
                    
                    # Cache the results if successful
                    if extraction_result and cat in extraction_result.get("categories", {}):
                        cat_state["search_results"] = extraction_result["categories"][cat]["search_results"]
                        cat_state["extracted_facts"] = extraction_result["categories"][cat]["extracted_facts"]
                        cat_state["sources"] = extraction_result["categories"][cat]["sources"]
                        
                        # Save to checkpoint with TTL
                        create_checkpoint(
                            f"category_{state['original_query']}_{cat}",
                            {
                                "search_results": cat_state["search_results"],
                                "extracted_facts": cat_state["extracted_facts"],
                                "sources": cat_state["sources"],
                                "timestamp": datetime.now(timezone.utc).isoformat()
                            },
                            ttl=86400  # 24 hour TTL
                        )
            except Exception as e:
                error_highlight(f"Error processing category {cat}: {str(e)}")
                cat_state["status"] = "failed"
        
        task = asyncio.create_task(process_category(category, cast(Dict[str, Any], category_state)))
        research_tasks.append(task)
    
    # Wait for all research tasks to complete
    if research_tasks:
        await asyncio.gather(*research_tasks)
    
    # Check if all categories are complete
    all_complete = all(
        category_state["complete"] for category_state in categories.values()
    )
    
    if all_complete:
        info_highlight("All categories research complete")
        return {
            "status": "researched",
            "categories": categories
        }
    else:
        # Some categories still incomplete
        incomplete = [
            category for category, category_state in categories.items()
            if not category_state["complete"]
        ]
        info_highlight(f"Categories still incomplete: {', '.join(incomplete)}")
        return {
            "status": "research_incomplete",
            "categories": categories
        }

# --------------------------------------------------------------------
# 4. Synthesis and validation nodes
# --------------------------------------------------------------------

async def synthesize_research(
    state: ResearchState,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Synthesize all research data into a comprehensive result."""
    log_step("Synthesizing research results", 7, 10)

    categories = state["categories"]
    original_query = state["original_query"]

    research_data = {
        category: {
            "facts": category_state["extracted_facts"],
            "sources": category_state["sources"],
            "quality_score": category_state["quality_score"],
            "statistics": category_state["statistics"],
            "confidence_score": category_state["confidence_score"],
            "cross_validation_score": category_state["cross_validation_score"],
            "source_quality_score": category_state["source_quality_score"],
            "recency_score": category_state["recency_score"],
            "statistical_content_score": category_state[
                "statistical_content_score"
            ],
        }
        for category, category_state in categories.items()
    }
    # Generate prompt
    synthesis_prompt = SYNTHESIS_PROMPT.format(
        query=original_query,
        research_json=json.dumps(research_data, indent=2)
    )

    try:
        synthesis_result = await call_model_json(
            messages=[{"role": "human", "content": synthesis_prompt}],
            config=ensure_config(config)
        )

        # Calculate overall confidence
        category_scores = {
            category: state["quality_score"]
            for category, state in categories.items()
        }

        synthesis_quality = assess_synthesis_quality(synthesis_result)
        validation_score = 0.8  # Default validation score, can be updated later

        overall_confidence = calculate_overall_confidence(
            category_scores=category_scores,
            synthesis_quality=synthesis_quality,
            validation_score=validation_score
        )

        # Add confidence assessment to synthesis result
        synthesis_result["confidence_assessment"] = {
            "overall_score": overall_confidence,
            "synthesis_quality": synthesis_quality,
            "validation_score": validation_score,
            "category_scores": category_scores
        }

        log_dict(
            {
                "synthesis_sections": list(synthesis_result.get("synthesis", {}).keys()),
                "confidence_score": overall_confidence,
                "synthesis_quality": synthesis_quality,
                "validation_score": validation_score
            },
            title="Synthesis Overview"
        )

        info_highlight(f"Research synthesis complete with confidence score: {overall_confidence:.2f}")

        return {
            "synthesis": synthesis_result,
            "status": "synthesized"
        }
    except Exception as e:
        error_highlight(f"Error in research synthesis: {str(e)}")
        return {"error": {"message": f"Error in research synthesis: {str(e)}", "phase": "synthesis"}}

async def validate_synthesis(
    state: ResearchState,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Validate the synthesized research results."""
    log_step("Validating research synthesis", 8, 10)
    
    synthesis = state.get("synthesis")
    if not synthesis:
        warning_highlight("No synthesis to validate")
        return {"error": {"message": "No synthesis to validate", "phase": "validation"}}
    
    # Generate validation prompt
    validation_prompt = VALIDATION_PROMPT.format(
        synthesis_json=json.dumps(synthesis, indent=2)
    )
    
    try:
        validation_result = await call_model_json(
            messages=[{"role": "human", "content": validation_prompt}],
            config=ensure_config(config)
        )
        
        # Get validation status
        validation_results = validation_result.get("validation_results", {})
        is_valid = validation_results.get("is_valid", False)
        validation_score = validation_results.get("validation_score", 0.0)
        
        # Log validation results
        log_dict(
            {
                "is_valid": is_valid,
                "validation_score": validation_score,
                "critical_issues": validation_results.get("critical_issues", [])
            },
            title="Validation Results"
        )
        
        if is_valid:
            info_highlight(f"Validation passed with score: {validation_score:.2f}")
            return {
                "validation_result": validation_result,
                "status": "validated",
                "complete": True
            }
        else:
            warning_highlight(f"Validation failed with score: {validation_score:.2f}")
            return {
                "validation_result": validation_result,
                "status": "validation_failed"
            }
    except Exception as e:
        error_highlight(f"Error in synthesis validation: {str(e)}")
        return {"error": {"message": f"Error in synthesis validation: {str(e)}", "phase": "validation"}}

def format_citation(citation: Dict[str, Any]) -> str:
    """Format a citation into a readable string."""
    if not isinstance(citation, dict):
        return ""

    title = citation.get("title", "")
    url = citation.get("url", "")
    if title or url:
        return f"[{title}]({url})" if title and url else title or url
    else:
        return ""

def highlight_statistics_in_content(content: str, statistics: List[Dict[str, Any]]) -> str:
    """Highlight statistics in content by wrapping them in markdown bold."""
    if not statistics:
        return content
        
    highlighted = content
    for stat in statistics:
        if value := stat.get("value"):
            highlighted = highlighted.replace(str(value), f"**{value}**")
    return highlighted

def _format_section_content(section_data: Dict[str, Any]) -> str:
    """Format a single section's content with statistics and citations."""
    if not isinstance(section_data, dict) or "content" not in section_data:
        return ""
        
    content = section_data.get("content", "")
    statistics = section_data.get("statistics", [])
    citations = section_data.get("citations", [])
    
    # Highlight statistics in content
    highlighted_content = highlight_statistics_in_content(content, statistics)
    
    # Add citations if present
    if citations and isinstance(citations, list):
        citation_text = "\n\n**Sources:** " + ", ".join(
            format_citation(citation) for citation in citations if citation
        )
        return highlighted_content + citation_text
    
    return highlighted_content

def format_statistic(stat: Dict[str, Any]) -> str:
    """Format a statistic into a readable string."""
    if not isinstance(stat, dict):
        return ""
        
    value = stat.get("value")
    context = stat.get("context", "")
    if not value:
        return ""
        
    return f"{value} ({context})" if context else str(value)

def _format_key_statistics(statistics: List[Dict[str, Any]]) -> str:
    """Format key statistics section."""
    key_stats = [
        f"- {format_statistic(stat)}"
        for stat in sorted(statistics, key=lambda x: x.get("quality_score", 0), reverse=True)[:10]
        if format_statistic(stat)
    ]
    return "\n".join(key_stats) if key_stats else "No key statistics available."

def _format_sources(synthesis_content: Dict[str, Any]) -> str:
    """Format sources section from citations."""
    sources = {
        format_citation(citation)
        for section_data in synthesis_content.values()
        if isinstance(section_data, dict)
        for citation in section_data.get("citations", [])
        if isinstance(citation, dict) and format_citation(citation)
    }
    return "\n".join(f"- {source}" for source in sorted(sources)) if sources else "No sources available."

def _format_confidence_notes(confidence: Dict[str, Any]) -> str:
    """Format confidence notes from limitations and knowledge gaps."""
    notes = []
    if limitations := confidence.get("limitations", []):
        notes.append("**Limitations:** " + ", ".join(limitations))
    if knowledge_gaps := confidence.get("knowledge_gaps", []):
        notes.append("**Knowledge Gaps:** " + ", ".join(knowledge_gaps))
    return "\n".join(notes)

async def generate_recommendations(
    synthesis_content: Dict[str, Any],
    query: str,
    config: Optional[RunnableConfig] = None
) -> str:
    """Generate recommendations based on synthesis content."""
    if not synthesis_content:
        return "No recommendations available."
        
    recommendations = []
    for section_data in synthesis_content.values():
        if isinstance(section_data, dict) and "recommendations" in section_data:
            recommendations.extend(section_data["recommendations"])
            
    if not recommendations:
        return "No specific recommendations available based on the research."
        
    return "\n".join(f"- {rec}" for rec in recommendations)

async def prepare_final_response(
    state: ResearchState,
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Prepare the final response with enhanced statistics and citations."""
    log_step("Preparing final response", 8, 10)
    
    synthesis = state.get("synthesis", {})
    synthesis_content = synthesis.get("synthesis", {}) if synthesis else {}
    confidence = synthesis.get("confidence_assessment", {}) if synthesis else {}
    
    # Collect all statistics
    all_statistics = [
        stat
        for category_state in state["categories"].values()
        for stat in category_state.get("statistics", [])
    ]
    
    # Format sections
    sections = {
        name: _format_section_content(data)
        for name, data in synthesis_content.items()
    }
    
    # Format final response
    response = ENHANCED_REPORT_TEMPLATE.format(
        title=f"Research Results: {state['original_query'].capitalize()}",
        executive_summary=sections.get("executive_summary", ""),
        key_statistics=_format_key_statistics(all_statistics),
        domain_overview=sections.get("domain_overview", ""),
        market_size=synthesis_content.get("market_dynamics", {}).get("market_size", ""),
        competitive_landscape=synthesis_content.get("market_dynamics", {}).get("competitive_landscape", ""),
        market_trends=synthesis_content.get("market_dynamics", {}).get("trends", ""),
        key_vendors=synthesis_content.get("provider_landscape", {}).get("key_vendors", ""),
        vendor_comparison=synthesis_content.get("provider_landscape", {}).get("vendor_comparison", ""),
        technical_requirements=sections.get("technical_requirements", ""),
        regulatory_landscape=sections.get("regulatory_landscape", ""),
        implementation_factors=sections.get("implementation_factors", ""),
        cost_structure=synthesis_content.get("cost_considerations", {}).get("cost_structure", ""),
        pricing_models=synthesis_content.get("cost_considerations", {}).get("pricing_models", ""),
        roi_considerations=synthesis_content.get("cost_considerations", {}).get("roi_considerations", ""),
        best_practices=sections.get("best_practices", ""),
        procurement_strategy=sections.get("contract_procurement_strategy", ""),
        recommendations=await generate_recommendations(synthesis_content, state["original_query"], config),
        sources=_format_sources(synthesis_content),
        confidence_score=confidence.get("overall_score", 0.0),
        generation_date=datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC"),
        confidence_notes=_format_confidence_notes(confidence)
    )
    
    return {
        "messages": [AIMessage(content=response)],
        "status": "complete",
        "complete": True
    }

# --------------------------------------------------------------------
# 5. Conditional routing functions
# --------------------------------------------------------------------

def should_request_clarification(state: ResearchState) -> Hashable:
    """Determine if clarification is needed."""
    needs_clarification = state.get("needs_clarification", False)
    has_feedback = bool(state.get("human_feedback"))
    
    if needs_clarification and not has_feedback:
        return "request_clarification"
    else:
        return "execute_research_for_categories"

def check_retry_or_continue(state: ResearchState) -> Hashable:
    """Determine if we should retry incomplete categories or continue."""
    categories = state["categories"]

    # Check if any categories need retry
    categories_to_retry = []
    categories_to_retry.extend(
        category
        for category, category_state in categories.items()
        if not category_state["complete"] and category_state["retry_count"] < 3
    )
    if categories_to_retry:
        # Still have categories to retry
        info_highlight(f"Categories to retry: {categories_to_retry}")
        return "execute_research_for_categories"
    else:
        # All categories either complete or max retries reached
        info_highlight("Research complete or max retries reached for all categories")
        return "synthesize_research"

def validate_or_complete(state: ResearchState) -> Hashable:
    """Determine if we should validate the synthesis or finish."""
    validation_result = state.get("validation_result", {})
    validation_results = validation_result.get("validation_results", {}) if validation_result else {}
    if is_valid := validation_results.get("is_valid", False):
        info_highlight("Validation passed, preparing final response")
    else:
        # Not valid, but we'll still finish
        warning_highlight("Validation failed, but preparing final response anyway")
    return "prepare_final_response"

def handle_error_or_continue(state: ResearchState) -> Hashable:
    """Determine if we should handle an error or continue."""
    error = state.get("error")
    
    if error:
        error_highlight(f"Error detected: {error.get('message')} in phase {error.get('phase')}")
        return "handle_error"
    else:
        return "continue"

# --------------------------------------------------------------------
# 6. Error handling
# --------------------------------------------------------------------

def _format_fact(fact: Dict[str, Any]) -> Optional[str]:
    """Format a single fact into a readable string."""
    if not isinstance(fact, dict):
        return None
        
    data = fact.get("data", {})
    fact_text = (
        data.get("fact") or
        data.get("requirement") or
        next((v for v in data.values() if isinstance(v, str) and v), None) or
        fact.get("fact")
    )
    return f"\n- {fact_text}" if fact_text else None

async def handle_error(state: ResearchState) -> Dict[str, Any]:
    """Handle errors gracefully and return a helpful message."""
    error = state.get("error") or {}
    phase = error.get("phase", "unknown")
    message = error.get("message", "An unknown error occurred")

    error_highlight(f"Handling error in phase {phase}: {message}")

    # Build error response with partial results
    error_response = [
        f"I encountered an issue while researching your query: {message}",
        "\nHere's what I was able to find before the error occurred:"
    ]

    # Add completed categories and their facts
    categories = state.get("categories", {})
    if complete_categories := [
        cat
        for cat, state in categories.items()
        if state.get("complete", False)
    ]:
        error_response.append(f"\n\nI completed research on: {', '.join(complete_categories)}")

        for category in complete_categories:
            if facts := categories[category].get("extracted_facts", []):
                error_response.append(f"\n\n## {category.replace('_', ' ').title()}")
                error_response.extend(
                    fact_text for fact in facts[:3]
                    if (fact_text := _format_fact(fact))
                )
    else:
        error_response.append("\n\nUnfortunately, I wasn't able to complete any research categories before the error occurred.")

    error_response.append("\n\nWould you like me to try again with a more specific query?")

    return {
        "messages": [AIMessage(content="".join(error_response))],
        "status": "error",
        "complete": True
    }

# --------------------------------------------------------------------
# 7. Build the graph
# --------------------------------------------------------------------

def create_research_graph() -> CompiledStateGraph:
    """Create the modular research graph."""
    graph = StateGraph(ResearchState)

    # Add the main nodes
    graph.add_node("initialize", initialize_research)
    graph.add_node("analyze_query", analyze_query)
    graph.add_node("request_clarification", request_clarification)
    graph.add_node("process_clarification", process_clarification)
    graph.add_node("execute_research_for_categories", execute_research_for_categories)
    graph.add_node("synthesize_research", synthesize_research)
    graph.add_node("validate_synthesis", validate_synthesis)
    graph.add_node("prepare_final_response", prepare_final_response)
    graph.add_node("handle_error", handle_error)
    graph.add_node("check_retry", lambda state: {"next_step": check_retry_or_continue(state)})
    graph.add_node("validate_or_complete", lambda state: {"next_step": validate_or_complete(state)})

    # Add error handling edges
    graph.add_conditional_edges(
        "initialize",
        handle_error_or_continue,
        {
            "handle_error": "handle_error",
            "continue": "analyze_query"
        }
    )

    # Set start point
    graph.add_edge(START, "initialize")

    # Add conditional branch for clarification from analyze_query
    graph.add_conditional_edges(
        "analyze_query",
        should_request_clarification,
        {
            "request_clarification": "request_clarification",
            "execute_research_for_categories": "execute_research_for_categories"
        }
    )
    
    graph.add_edge("request_clarification", "process_clarification")
    graph.add_edge("process_clarification", "analyze_query")

    # Research flow
    graph.add_conditional_edges(
        "execute_research_for_categories",
        handle_error_or_continue,
        {
            "handle_error": "handle_error",
            "continue": "check_retry"
        }
    )

    graph.add_conditional_edges(
        "check_retry",
        lambda state: state.get("next_step", "synthesize_research"),
        {
            "execute_research_for_categories": "execute_research_for_categories",
            "synthesize_research": "synthesize_research"
        }
    )

    # Synthesis and validation
    graph.add_conditional_edges(
        "synthesize_research",
        handle_error_or_continue,
        {
            "handle_error": "handle_error",
            "continue": "validate_synthesis"
        }
    )

    graph.add_conditional_edges(
        "validate_synthesis",
        handle_error_or_continue,
        {
            "handle_error": "handle_error",
            "continue": "validate_or_complete"
        }
    )

    graph.add_conditional_edges(
        "validate_or_complete",
        lambda state: state.get("next_step", "prepare_final_response"),
        {
            "prepare_final_response": "prepare_final_response"
        }
    )

    # Final steps
    graph.add_edge("prepare_final_response", END)
    graph.add_edge("handle_error", END)

    return graph.compile(interrupt_before=["process_clarification"])

# Create the graph instance
research_graph = create_research_graph()

# def create_category_graph(category: str) -> CompiledStateGraph:
#     """Create a specialized graph for a single research category."""
#     graph = StateGraph(ResearchState)
    
#     # Add nodes for single category research
#     graph.add_node("initialize", initialize_research)
#     graph.add_node("analyze_query", analyze_query)
#     graph.add_node("execute_category_search", lambda state, config: execute_category_search(state, category, config))
#     graph.add_node("extract_category_information", lambda state, config: extract_category_information(state, category, config))
#     graph.add_node("prepare_final_response", prepare_final_response)
#     graph.add_node("handle_error", handle_error)
    
#     # Add edges
#     graph.add_edge(START, "initialize")
#     graph.add_edge("initialize", "analyze_query")
#     graph.add_edge("analyze_query", "execute_category_search")
#     graph.add_edge("execute_category_search", "extract_category_information")
#     graph.add_edge("extract_category_information", "prepare_final_response")
#     graph.add_edge("prepare_final_response", END)
#     graph.add_edge("handle_error", END)
    
#     return graph.compile()

def research(
    query: str,
    max_iterations: int = 5,
    checkpoint_path: Optional[str] = None,
    config: Optional[RunnableConfig] = None,
) -> CompiledStateGraph:
    """
    Create a research graph that can iteratively search and analyze information.

    Args:
        query: The research query to investigate
        max_iterations: Maximum number of iterations to run
        checkpoint_path: Path to save/load checkpoint data
        config: Configuration for the runnable

    Returns:
        A compiled state graph that can be executed
    """
    config = ensure_config(config)

    # Initialize state
    workflow = StateGraph(ResearchState)

    # Load checkpoint if exists
    if checkpoint_path:
        workflow = load_checkpoint(workflow, checkpoint_path)

    # Add nodes
    workflow.add_node("search", search_node)
    workflow.add_node("analyze", analyze_node)
    workflow.add_node("decide", decide_node)

    # Build edges
    workflow.add_edge("search", "analyze")
    workflow.add_edge("analyze", "decide")
    workflow.add_conditional_edges(
        "decide",
        decide_route,
        {
            "continue": "search",
            "complete": END,
        },
    )

    workflow.set_entry_point("search")

    # Compile graph
    graph = workflow.compile()

    # Create checkpoint for saving state
    if checkpoint_path:
        graph = create_checkpoint(graph, checkpoint_path)

    # Cache the graph execution
    @cache_result(key=f"research_{query}_{max_iterations}")
    def execute_graph():
        return graph.invoke(
            {
                "query": query,
                "iteration": 0,
                "max_iterations": max_iterations,
                "messages": [],
                "urls": set(),
                "findings": [],
            },
            config=config,
        )

    return execute_graph()

async def search_node(state: GraphState, config: RunnableConfig) -> GraphState:
    """Search for relevant information based on the current query."""
    query = state["query"]
    iteration = state["iteration"]
    urls = state["urls"]

    @cache_result(key=f"search_{query}_{iteration}")
    async def perform_search():
        # Search for relevant information
        search_results = await search(query, config=config)
        
        # Filter out already visited URLs
        new_results = [r for r in search_results if r.metadata["url"] not in urls]
        
        # Update visited URLs
        urls.update(r.metadata["url"] for r in new_results)
        
        return new_results

    search_results = await perform_search()
    
    # Add search results to state
    state["search_results"] = search_results
    state["urls"] = urls
    
    return state

async def analyze_node(state: GraphState, config: RunnableConfig) -> GraphState:
    """Analyze the search results and extract relevant information."""
    query = state["query"]
    search_results = state["search_results"]
    iteration = state["iteration"]

    if not search_results:
        return state

    # Prepare documents for analysis
    docs = [
        f"Source {i+1}: {doc.page_content}\nURL: {doc.metadata['url']}"
        for i, doc in enumerate(search_results)
    ]
    context = "\n\n".join(docs)

    @cache_result(key=f"analyze_{query}_{iteration}")
    async def perform_analysis():
        # Analyze the search results
        analysis_prompt = f"""You are a research assistant analyzing search results for the query: {query}

        Here are the search results:
        {context}

        Extract and summarize the key findings from these sources. Focus on:
        1. Main points relevant to the query
        2. Any contradictions or disagreements between sources
        3. Credibility of the sources
        4. Areas that need more investigation

        Format your response as a JSON object with these fields:
        - findings: list of key findings
        - credibility: assessment of source credibility
        - gaps: areas needing more research
        """

        analysis = await call_model_json(
            analysis_prompt,
            config=config,
            json_schema={
                "type": "object",
                "properties": {
                    "findings": {"type": "array", "items": {"type": "string"}},
                    "credibility": {"type": "string"},
                    "gaps": {"type": "array", "items": {"type": "string"}},
                },
                "required": ["findings", "credibility", "gaps"],
            },
        )
        
        return analysis

    analysis = await perform_analysis()
    
    # Update state with analysis
    state["findings"].extend(analysis["findings"])
    state["credibility"] = analysis["credibility"]
    state["gaps"] = analysis["gaps"]
    
    return state

async def decide_node(state: GraphState, config: RunnableConfig) -> GraphState:
    """Decide whether to continue searching or complete the research."""
    query = state["query"]
    iteration = state["iteration"]
    max_iterations = state["max_iterations"]
    findings = state["findings"]
    gaps = state.get("gaps", [])

    if iteration >= max_iterations:
        state["decision"] = "complete"
        return state

    @cache_result(key=f"decide_{query}_{iteration}")
    async def make_decision():
        # Make decision about whether to continue
        decision_prompt = f"""You are a research assistant deciding whether to continue searching for information about: {query}

        Current findings:
        {json.dumps(findings, indent=2)}

        Identified gaps:
        {json.dumps(gaps, indent=2)}

        Current iteration: {iteration}
        Maximum iterations: {max_iterations}

        Should we continue searching or have we found sufficient information?
        Format your response as a JSON object with these fields:
        - decision: either "continue" or "complete"
        - reasoning: explanation for the decision
        - next_query: if continuing, what should we search for next (focus on gaps)
        """

        decision = await call_model_json(
            decision_prompt,
            config=config,
            json_schema={
                "type": "object",
                "properties": {
                    "decision": {"type": "string", "enum": ["continue", "complete"]},
                    "reasoning": {"type": "string"},
                    "next_query": {"type": "string"},
                },
                "required": ["decision", "reasoning", "next_query"],
            },
        )
        
        return decision

    decision = await make_decision()
    
    # Update state with decision
    state["decision"] = decision["decision"]
    state["reasoning"] = decision["reasoning"]
    
    if decision["decision"] == "continue":
        state["query"] = decision["next_query"]
        state["iteration"] += 1
    
    return state

def decide_route(state: GraphState) -> Literal["continue", "complete"]:
    """Route to the next node based on the decision."""
    return cast(Literal["continue", "complete"], state["decision"])
</file>

<file path="src/react_agent/prompts/__init__.py">
"""Prompt exports.

This module provides functionality for prompt exports
in the agent framework.
"""

from typing import Any, Dict, Final, List, Tuple

from react_agent.prompts.analysis import (
    ANALYSIS_PROMPT,
    TOOL_SELECTION_PROMPT,
)
from react_agent.prompts.market import (
    MARKET_DATA_PROMPT,
    MARKET_PROMPT,
)
from react_agent.prompts.reflection import (
    REFLECTION_PROMPT,
)
from react_agent.prompts.research import (
    ADDITIONAL_TOPICS_PROMPT,
    RESEARCH_AGENT_PROMPT,
    RESEARCH_BASE_PROMPT,
    TOPICS_PROMPT,
    QUERY_ANALYSIS_PROMPT,
    CLARIFICATION_PROMPT,
)

# Import all prompts from modules
from react_agent.prompts.templates import (
    ANALOGICAL_REASONING_PROMPT,
    COUNTERFACTUAL_PROMPT,
    CRITIQUE_PROMPT_TEMPLATE,
    EVALUATION_PROMPT_TEMPLATE,
    # Reflection prompts
    FEEDBACK_PROMPT_TEMPLATE,
    MAIN_PROMPT,
    METACOGNITION_PROMPT,
    NEWS_SEARCH_DESC,
    SCRAPE_DESC,
    STRUCTURED_OUTPUT_VALIDATION,
    SUMMARIZER_DESC,
    TOOL_INSTRUCTIONS,
    VALIDATION_REQUIREMENTS,
    WEB_SEARCH_DESC,
)
from react_agent.prompts.validation import (
    VALIDATION_AGENT_PROMPT,
    VALIDATION_BASE_PROMPT,
)

# Re-export everything for backward compatibility
__all__ = [
    # Templates
    "STRUCTURED_OUTPUT_VALIDATION",
    "VALIDATION_REQUIREMENTS",
    "MAIN_PROMPT",
    "WEB_SEARCH_DESC",
    "SCRAPE_DESC",
    "SUMMARIZER_DESC",
    "NEWS_SEARCH_DESC",
    "TOOL_INSTRUCTIONS",
    # Research
    "RESEARCH_BASE_PROMPT",
    "RESEARCH_AGENT_PROMPT",
    "MARKET_PROMPT",
    "TOPICS_PROMPT",
    "ADDITIONAL_TOPICS_PROMPT",
    "QUERY_ANALYSIS_PROMPT",
    "CLARIFICATION_PROMPT",
    # Validation
    "VALIDATION_BASE_PROMPT",
    "VALIDATION_AGENT_PROMPT",
    # Analysis
    "ANALYSIS_PROMPT",
    "TOOL_SELECTION_PROMPT",
    # Reflection
    "REFLECTION_PROMPT",
    # Reflection Templates
    "FEEDBACK_PROMPT_TEMPLATE",
    "EVALUATION_PROMPT_TEMPLATE",
    "CRITIQUE_PROMPT_TEMPLATE",
    "ANALOGICAL_REASONING_PROMPT",
    "COUNTERFACTUAL_PROMPT",
    "METACOGNITION_PROMPT",
    # Functions
    "get_report_template",
    "get_analysis_template",
]


# Common utility functions
def get_report_template() -> Dict[str, Any]:
    """Get the template for the final report."""
    return {
        "summary": "",
        "research_findings": {},
        "market_analysis": {},
        "generated_at": "",
    }


def get_analysis_template() -> Dict[str, Any]:
    """Get the template for research analysis."""
    return {
        "citations": [],
        "porters_five_forces": {},
        "swot_analysis": {},
        "pestel_analysis": {},
        "gap_analysis": {},
        "cost_benefit_analysis": {},
        "risk_assessment": {},
        "tco_analysis": {},
        "vendor_analysis": {},
        "benchmarking": {},
        "stakeholder_analysis": {},
        "compliance_analysis": {},
        "business_impact_analysis": {},
    }


# Required analysis topics
REQUIRED_ANALYSIS_TOPICS: List[Tuple[str, str]] = [
    ("Porter's Five Forces", "Analysis of competitive forces in the industry"),
    ("SWOT Analysis", "Strengths, weaknesses, opportunities, and threats"),
    (
        "PESTEL Analysis",
        "Political, economic, social, technological, environmental, and legal factors",
    ),
    ("GAP Analysis", "Current state vs desired state analysis"),
    ("Cost-Benefit Analysis", "Analysis of costs and benefits"),
    ("Risk Assessment", "Identification and analysis of potential risks"),
    ("Total Cost of Ownership", "Complete cost analysis including indirect costs"),
    ("Vendor Analysis", "Analysis of potential vendors and suppliers"),
    ("Benchmarking", "Comparison with industry standards and best practices"),
    ("Stakeholder Analysis", "Analysis of key stakeholders and their needs"),
    ("Compliance Analysis", "Analysis of regulatory and compliance requirements"),
    ("Business Impact Analysis", "Analysis of business impact and strategic alignment"),
]

# System prompts
SYSTEM_PROMPT_ANALYST: Final[str] = "You are an expert market research analyst."

# Finalization prompts
FINALIZATION_BASE_PROMPT: Final[
    str
] = """You are a Finalization Agent for RFP market analysis.
Your goal is to generate comprehensive reports and outputs from the validated research.

{STRUCTURED_OUTPUT_VALIDATION}

FINALIZATION REQUIREMENTS:
1. Research Report
   - Expand each analysis element into well-written sections
   - Maintain professional and clear writing style
   - Include supporting evidence and citations
   - Organize content logically and cohesively
   - Ensure all insights are actionable

2. Analysis Sections
   - Porter's 5 Forces analysis
   - SWOT analysis
   - PESTEL analysis
   - GAP analysis
   - Cost-benefit analysis
   - Risk assessment
   - Total cost of ownership
   - Vendor analysis
   - Benchmarking results
   - Stakeholder analysis
   - Compliance requirements
   - Business impact assessment

3. Market Basket Output
   - Generate CSV format
   - Include all line items
   - Maintain data accuracy
   - Format for easy review
   - Include citations and sources

4. Quality Requirements
   - Professional writing style
   - Clear section headings
   - Consistent formatting
   - Proper citation formatting
   - Executive summary
   - Recommendations section

RESPONSE_FORMAT:
{
    "outputs": {
        "research_report": {
            "format": "markdown",
            "content": "",
            "sections": []
        },
        "market_basket": {
            "format": "csv",
            "headers": [],
            "rows": []
        },
        "executive_summary": "",
        "recommendations": [],
        "confidence_scores": {},
        "key_findings": []
    },
    "metadata": {
        "generated_at": "",
        "version": "1.0",
        "validation_status": {
            "is_valid": false,
            "errors": [],
            "warnings": []
        }
    }
}

Current state: {state}
"""

FINALIZATION_AGENT_PROMPT: Final[str] = FINALIZATION_BASE_PROMPT.replace(
    "Your goal is to generate comprehensive reports and outputs from the validated research.\n",
    "Your goal is to generate comprehensive reports and outputs from the validated research.\n\n{STRUCTURED_OUTPUT_VALIDATION}\n",
)

ENRICHMENT_AGENT_PROMPT: Final[
    str
] = """You are an Enrichment Agent for RFP market analysis.
Enhance the following validated data while maintaining the JSON structure:
Validated Data:
{validated_data}
Required Schema:
{
    "rfp_analysis": {
        "analysis": {
            "porters_5_forces": {
                "competitive_rivalry": "",
                "threat_of_new_entrants": "",
                "threat_of_substitutes": "",
                "bargaining_power_buyers": "",
                "bargaining_power_suppliers": ""
            },
            "swot": {
                "strengths": [],
                "weaknesses": [],
                "opportunities": [],
                "threats": []
            },
            "recent_breakthroughs_and_disruptors": "",
            "cost_trends_and_projections": "",
            "typical_contract_clauses_and_pricing_nuances": "",
            "competitive_landscape": "",
            "citations": {
                "porters_5_forces": [],
                "swot": [],
                "recent_breakthroughs_and_disruptors": [],
                "cost_trends_and_projections": [],
                "typical_contract_clauses_and_pricing_nuances": [],
                "competitive_landscape": []
            }
        },
        "market_basket": [
            {
                "manufacturer_or_distributor": "",
                "item_number": "",
                "item_description": "",
                "uom": "",
                "estimated_qty_per_uom": 0.0,
                "unit_cost": 0.0,
                "citation": ""
            }
        ]
    },
    "confidence_score": 0.0
}
Enrichment Focus Areas:
1. Market Intelligence
   - Add emerging technology trends with citations
   - Include regulatory impact analysis with sources
   - Highlight market consolidation trends with references
   - Ensure at least 2 citations per section

2. Supplier Intelligence
   - Add supplier financial health indicators with sources
   - Include supplier innovation capabilities with citations
   - Note supplier market share trends with references
   - Validate supplier information from multiple sources

3. Pricing Intelligence
   - Add volume discount structures with citations
   - Include regional pricing variations with sources
   - Note seasonal pricing factors with references
   - Verify pricing data from reliable sources

4. Risk Analysis
   - Add supply chain risk factors with citations
   - Include mitigation strategies with sources
   - Note alternative sourcing options with references
   - Cross-reference risk data from multiple sources

5. Citation Requirements
   - Each analysis section must have at least 2 citations
   - Market basket items must each have a valid citation
   - Citations must be from reliable industry sources
   - Avoid using the same citation across multiple sections

CONFIDENCE_SCORING:
- Start with a base score of 0.5
- Add 0.1 for each section with 2+ unique citations
- Add 0.1 for each market basket item with verified pricing
- Subtract 0.1 for any section with fewer than 2 citations
- Maximum score is 0.95 until all data is fully verified

RESPONSE_REQUIREMENTS:
1. Output must be valid JSON only
2. All fields must be populated with enriched data
3. No explanatory text or comments
4. Include enrichment notes in "enrichment_details" if needed
5. Ensure complete, untruncated JSON output
6. Every section must have multiple citations
7. Market basket items must have verified sources

Current enrichment state: {current_state}
Conversation history:
{chat_history}
"""

# Export finalization prompts
__all__.extend(
    [
        "FINALIZATION_BASE_PROMPT",
        "FINALIZATION_AGENT_PROMPT",
        "ENRICHMENT_AGENT_PROMPT",
        "REQUIRED_ANALYSIS_TOPICS",
        "SYSTEM_PROMPT_ANALYST",
    ]
)
</file>

<file path="src/react_agent/prompts/analysis.py">
"""Analysis-specific prompts.

This module provides functionality for analysis-specific prompts
in the agent framework.
"""

from typing import Final

# Prompt for tool selection
TOOL_SELECTION_PROMPT: Final[str] = (
    """What information do we need to research about {current_topic}?"""
)

# Prompt for analysis of tool results
ANALYSIS_PROMPT: Final[str] = """
Based on the following research about {current_topic}, provide a comprehensive analysis:

{formatted_results}

Your analysis should include:
1. Key insights from the research
2. Patterns or trends identified
3. Implications for the business
4. Recommendations based on the findings
"""

# Prompt for analysis plan formulation
ANALYSIS_PLAN_PROMPT: Final[str] = """
Analysis Task: {task}
Available Data:
{data_summary}

Create a comprehensive plan for analyzing this data that will address the task.
Your plan should include:
1. Data preparation steps needed (e.g., cleaning, transformation)
2. Analysis methods to apply (e.g., descriptive statistics, correlation, regression)
3. Visualizations to create (e.g., histograms, scatter plots, bar charts)
4. Hypotheses to test (if applicable)
5. Statistical methods to use (e.g., t-tests, ANOVA, chi-squared)
6. Expected insights (what do you expect to learn from the analysis?)

Format your response as a JSON object with these sections.
"""

# Prompt for results interpretation
INTERPRET_RESULTS_PROMPT: Final[str] = """
Analysis Task: {task}
Analysis Results:
{analysis_results}
Analysis Plan:
{analysis_plan}

Interpret these results in the context of the original task.
Your interpretation should include:
1. Key findings and insights
2. Patterns and trends identified
3. Anomalies or unexpected results
4. Limitations of the analysis
5. Answers to specific questions in the task (if any)
6. Business implications (if applicable)

Format your response as a JSON object with these sections.
"""

# Prompt for report compilation
COMPILE_REPORT_PROMPT: Final[str] = """
Analysis Task: {task}
Analysis Results:
{analysis_results}
Interpretations:
{interpretations}
Visualizations:
{visualization_metadata}

Compile a comprehensive analysis report addressing the original task.
The report should:
1. Start with an executive summary of key findings.
2. Include an introduction explaining the context and objectives.
3. Describe the methodology and data sources.
4. Present the detailed findings with references to visualizations.
5. Discuss implications and recommendations.
6. Note limitations and potential future analysis.

Format the report as markdown with proper headings, lists, and sections.
"""
</file>

<file path="src/react_agent/prompts/market.py">
"""Market-specific prompts.

This module provides functionality for market data processing prompts
in the agent framework.
"""

from typing import Final

# Market data processing prompt
MARKET_DATA_PROMPT: Final[str] = """You are a Market Data Processor specialized in extracting pricing and sourcing information.

Your task is to analyze the following item and identify potential market sources, pricing, and manufacturer information.

INSTRUCTIONS:
1. Analyze the provided item description
2. Identify potential manufacturers or distributors
3. Find item numbers, descriptions, and pricing information
4. Format the response as a structured JSON object

RESPONSE FORMAT:
{
    "market_items": [
        {
            "manufacturer": "Name of manufacturer or distributor",
            "item_number": "Product/catalog number",
            "item_description": "Detailed description of the item",
            "unit_of_measure": "Each, Box, Case, etc.",
            "unit_cost": 0.00,
            "source": "Where this information was found"
        }
    ],
    "confidence_score": 0.0,
    "notes": "Any additional information or context"
}

IMPORTANT:
- Include multiple sources if available
- Provide accurate pricing information
- Include detailed item descriptions
- Assign a confidence score (0.0-1.0) based on data reliability
- Only include items that match the original description

Item to process: {state}
""" 

# Market research prompt
MARKET_PROMPT: Final[str] = """You are a Market Research Agent focused on building comprehensive market baskets.

Your task is to analyze the market for the following items and provide detailed market research information.

INSTRUCTIONS:
1. Analyze the market for each item
2. Identify market trends and dynamics
3. Research pricing and availability
4. Find potential suppliers and manufacturers
5. Analyze market competition
6. Identify regulatory requirements
7. Provide market forecasts and insights

RESPONSE FORMAT:
{
    "market_analysis": {
        "market_size": "Total market size with units",
        "growth_rate": "Annual growth rate",
        "trends": ["Key market trends"],
        "competition": ["Major competitors"],
        "regulations": ["Relevant regulations"]
    },
    "items": [
        {
            "item_name": "Name of item",
            "market_price": "Price range",
            "suppliers": ["List of suppliers"],
            "availability": "Supply status",
            "quality_metrics": ["Quality indicators"]
        }
    ],
    "confidence_score": 0.0,
    "notes": "Additional market insights"
}

IMPORTANT:
- Provide accurate market data with sources
- Include recent market trends and forecasts
- Consider both local and global market factors
- Note any market risks or uncertainties
- Assign confidence scores based on data reliability
"""
</file>

<file path="src/react_agent/prompts/query.py">
"""Enhanced query optimization to improve search relevance.

This enhances the query optimization to include more procurement-specific terms
and domain-specific vocabularies to increase search precision.
"""

from typing import Dict, List, Set, Optional
import re

# Domain-specific keyword repositories
PROCUREMENT_TERMS = {
    "sourcing": ["strategic sourcing", "supplier selection", "supplier qualification", "vendor selection"],
    "contracts": ["contract terms", "contract management", "agreement", "obligations", "SLAs", "KPIs"],
    "pricing": ["volume discounts", "rebates", "bulk discounts", "pricing models", "cost-plus", "fixed price"],
    "rfp": ["request for proposal", "request for information", "request for quote", "bid", "tender"],
    "procurement": ["procurement strategy", "procurement process", "buying", "purchasing"],
    "payment": ["invoicing", "payment terms", "purchase orders", "net-30", "net-60"],
    "suppliers": ["vendors", "distributors", "manufacturers", "providers", "supply base"],
    "strategy": ["category strategy", "category management", "spend analysis", "cost reduction"],
    "risk": ["risk management", "risk mitigation", "compliance", "qualifications", "certifications"],
    "process": ["auction", "reverse auction", "e-procurement", "p2p", "procure to pay"]
}

# Industry vertical specializations - can be expanded as needed
INDUSTRY_VERTICALS = {
    "education": ["university", "college", "campus", "academic", "educational"],
    "healthcare": ["hospital", "clinic", "medical", "patient care", "healthcare"],
    "manufacturing": ["factory", "industrial", "production", "assembly", "plant"],
    "government": ["public sector", "government agency", "municipal", "federal", "state"],
    "retail": ["retail operations", "store", "outlet", "retail chain", "merchandising"],
    "utilities": ["energy", "water", "electricity", "gas", "utility provider"]
}

# Maintenance categories
MAINTENANCE_CATEGORIES = {
    "preventive": ["preventive maintenance", "scheduled maintenance", "routine service"],
    "corrective": ["corrective maintenance", "repair", "fix", "troubleshooting"],
    "predictive": ["predictive maintenance", "condition monitoring", "predictive analytics"],
    "supplies": ["consumables", "disposables", "tools", "equipment", "spare parts"]
}

def detect_vertical(query: str) -> str:
    """Detect the industry vertical from the query."""
    query_lower = query.lower()

    return next(
        (
            vertical
            for vertical, keywords in INDUSTRY_VERTICALS.items()
            if any(keyword in query_lower for keyword in keywords)
        ),
        "general",
    )

def expand_acronyms(query: str) -> str:
    """Expand common industry acronyms in the query."""
    acronyms = {
        "mro": "maintenance repair operations",
        "rfp": "request for proposal",
        "rfq": "request for quote",
        "rfi": "request for information",
        "eam": "enterprise asset management",
        "cmms": "computerized maintenance management system",
        "kpi": "key performance indicator",
        "sla": "service level agreement",
        "tcoo": "total cost of ownership",
        "p2p": "procure to pay"
    }
    
    words = query.split()
    for i, word in enumerate(words):
        word_lower = word.lower().strip(",.;:()[]{}\"'")
        if word_lower in acronyms:
            # Replace acronym with expansion while preserving original casing and punctuation
            prefix = ""
            suffix = ""
            if not word.isalnum():
                prefix = word[:len(word) - len(word.lstrip(",.;:()[]{}\"'"))]
                suffix = word[len(word.rstrip(",.;:()[]{}\"'")):]
            words[i] = prefix + acronyms[word_lower] + " (" + word.strip(",.;:()[]{}\"'") + ")" + suffix
    
    return " ".join(words)

def optimize_query(
    original_query: str, 
    category: str, 
    vertical: Optional[str] = None,
    include_all_keywords: bool = False
) -> str:
    """Create optimized queries for specific research categories with enhanced domain-specific terms."""
    # Clean the original query
    original_query = original_query.split("Additional context:")[0].strip()
    
    # Expanded acronyms for better search results
    expanded_query = expand_acronyms(original_query)
    
    # Detect vertical if not provided
    if vertical is None:
        vertical = detect_vertical(original_query)
    
    # Get vertical-specific terms
    vertical_terms = INDUSTRY_VERTICALS.get(vertical, [""])
    
    # Define enhanced category-specific query templates with keyword banks
    query_templates = {
        "market_dynamics": {
            "template": "{primary_terms} market trends",
            "keyword_groups": ["contracts", "procurement", "strategy"]
        },
        "provider_landscape": {
            "template": "{primary_terms} vendors suppliers",
            "keyword_groups": ["suppliers", "rfp", "strategy"]
        },
        "technical_requirements": {
            "template": "{primary_terms} technical specifications",
            "keyword_groups": ["rfp", "procurement", "risk"]
        },
        "regulatory_landscape": {
            "template": "{primary_terms} regulations compliance",
            "keyword_groups": ["contracts", "risk", "process"]
        },
        "cost_considerations": {
            "template": "{primary_terms} pricing cost budget",
            "keyword_groups": ["pricing", "payment", "strategy"]
        },
        "best_practices": {
            "template": "{primary_terms} best practices",
            "keyword_groups": ["strategy", "risk", "process"]
        },
        "implementation_factors": {
            "template": "{primary_terms} implementation factors",
            "keyword_groups": ["procurement", "suppliers", "risk"]
        }
    }
    
    # Extract primary terms from the query (filter out common words)
    words = expanded_query.split()
    stop_words = ["help", "me", "research", "find", "information", "about", "on", "for", 
                  "the", "and", "or", "in", "to", "with", "by", "is", "are"]
    
    primary_terms = []
    for word in words:
        if word.lower() not in stop_words and len(word) > 3:
            primary_terms.append(word)
            # Limit to first 3-4 meaningful terms
            if len(primary_terms) >= 4:
                break
    
    # If no primary terms found, use the whole query up to a limit
    if not primary_terms:
        primary_terms = words[:3]
    
    # Get template for this category
    if category not in query_templates:
        return " ".join(primary_terms)
    
    template = query_templates[category]["template"]
    
    # Get procurement terms for this category (but use fewer to keep query simple)
    procurement_terms = []
    if include_all_keywords:
        # Include more keywords for comprehensive searches but still limit
        for group, terms in PROCUREMENT_TERMS.items():
            if group in query_templates[category]["keyword_groups"]:
                procurement_terms.append(terms[0])  # Just top 1 term from each group
    else:
        # Include only minimal keyword groups
        keyword_groups = query_templates[category]["keyword_groups"]
        if keyword_groups:
            top_group = keyword_groups[0]
            if top_group in PROCUREMENT_TERMS:
                procurement_terms.append(PROCUREMENT_TERMS[top_group][0])  # Just top term
    
    # Format the template with just essential terms
    primary_terms_str = " ".join(primary_terms)
    
    optimized_query = template.format(
        primary_terms=primary_terms_str
    )
    
    # Add at most one procurement term if we need to for context
    if procurement_terms:
        optimized_query += " " + procurement_terms[0]
        
    # Add at most one vertical term if needed
    if vertical != "general" and vertical_terms:
        optimized_query += " " + vertical_terms[0]
    
    # Ensure the query isn't too long for search engines
    if len(optimized_query) > 100:
        optimized_query = optimized_query[:100].rsplit(' ', 1)[0]
    
    return optimized_query
</file>

<file path="src/react_agent/prompts/reflection.py">
"""Reflection and critique prompts.

This module provides functionality for reflection and critique prompts
in the agent framework.
"""

from typing import Final

# Reflection prompt
# Parameters:
#   current_state: The current state of the agent
#   validation_targets: List of targets to validate
REFLECTION_PROMPT: Final[
    str
] = """You are a Reflection Agent responsible for validating research findings and preventing hallucinations.
Your tasks include:

1. Citation Validation
- Check all URLs for validity (no 404s)
- Verify source credibility
- Ensure citation dates are recent

2. Confidence Scoring
- Evaluate research findings confidence (threshold: 98%)
- Score market data reliability
- Assess source quality

3. Structured Output Validation
- Verify all required fields are populated
- Check data format consistency
- Validate numerical values

4. Quality Control
- Flag potential hallucinations
- Identify data gaps
- Request additional research if needed

Current state: {current_state}
Validation targets: {validation_targets}
"""
</file>

<file path="src/react_agent/prompts/research.py">
"""Enhanced research-specific prompts.

This module provides specialized prompts for different research categories
to improve extraction quality and relevance.
"""

from typing import Final, Dict, List, Any, Optional, Union
import json
from datetime import datetime

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight
from react_agent.utils.defaults import get_default_extraction_result

# Initialize logger
logger = get_logger(__name__)

# Base templates for common validation requirements
STRUCTURED_OUTPUT_VALIDATION: Final[str] = """CRITICAL: All responses MUST:
1. Be valid JSON only - no additional text or comments
2. Follow the exact schema provided
3. Never return empty or null values - use empty strings or arrays instead
4. Include all required fields
5. Use proper data types (strings, numbers, arrays)
6. Maintain proper JSON syntax
7. Include citations for all data points
8. Pass JSON schema validation

Any response that fails these requirements will be rejected."""

# Enhanced query analysis prompt with improved categorization and structure
QUERY_ANALYSIS_PROMPT: Final[str] = """Analyze the following research query to generate targeted search terms.

Query: {query}

TASK:
Break down this query into precise search components following these rules:
1. Use the UNSPSC taxonomy to identify relevant procurement categories
2. Extract no more than 3-5 focused keywords per category
3. Prioritize specificity over quantity
4. Identify the specific industry verticals, markets, and sectors
5. Determine geographical scope if relevant

FORMAT YOUR RESPONSE AS JSON:
{{
    "unspsc_categories": [
        {{"code": "code", "name": "category name", "relevance": 0.0-1.0}}
    ],
    "search_components": {{
        "primary_topic": "", 
        "industry": "",
        "product_type": "",
        "geographical_focus": ""
    }},
    "search_terms": {{
        "market_dynamics": [],
        "provider_landscape": [],
        "technical_requirements": [],
        "regulatory_landscape": [],
        "cost_considerations": [],
        "best_practices": [],
        "implementation_factors": []
    }},
    "boolean_query": "",
    "missing_context": []
}}

IMPORTANT: 
- Keep each category to a MAXIMUM of 5 focused search terms
- Only include truly essential items in "missing_context" - make reasonable assumptions
- For "boolean_query" create a precise search string using AND/OR operators
- Assign relevance scores (0.0-1.0) to each UNSPSC category
- Your response must be valid JSON with all fields present
- Do not include any comments or additional text in the JSON response
"""

# Specialized extraction prompts for different research categories
EXTRACTION_PROMPTS: Dict[str, str] = {
    "market_dynamics": """Extract factual information about MARKET DYNAMICS from this content about {query}.

URL: {url}

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about market size, growth rates, trends, forecasts, competitive dynamics, and procurement patterns
2. Format each fact with:
   - The fact statement
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If the document doesn't contain relevant market data, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_facts": [
    {{
      "fact": "Clear factual statement about market dynamics",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low",
      "data_type": "market_size/growth_rate/trend/forecast/competitive/procurement_pattern"
    }}
  ],
  "market_metrics": {{
    "market_size": null,  // Include if available with units
    "growth_rate": null,  // Include if available with time period
    "forecast_period": null,  // Include if available
    "procurement_volume": null,  // Include if available
    "contract_value": null  // Include if available
  }},
  "relevance_score": 0.0-1.0
}}
""",

    "provider_landscape": """Extract factual information about PROVIDERS/VENDORS from this content about {query}.

URL: {url}

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about vendors, suppliers, service providers, manufacturers, distributors, and market players
2. Format each fact with:
   - The vendor name and specific details
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no vendor information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_vendors": [
    {{
      "vendor_name": "Name of vendor",
      "description": "What they provide",
      "market_position": "leader/challenger/niche",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low",
      "contract_status": "active/expired/pending",
      "contract_terms": "Key contract terms if available",
      "pricing_model": "Pricing structure if available"
    }}
  ],
  "vendor_relationships": [
    {{
      "relationship_type": "partnership/competition/acquisition/contract",
      "entities": ["vendor1", "vendor2"],
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low",
      "contract_details": "Contract details if available"
    }}
  ],
  "relevance_score": 0.0-1.0
}}
""",

    "technical_requirements": """Extract factual information about TECHNICAL REQUIREMENTS from this content about {query}.

URL: {url}

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about specifications, standards, technologies, and requirements
2. Format each fact with:
   - The technical requirement or specification
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no technical information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_requirements": [
    {{
      "requirement": "Specific technical requirement",
      "category": "hardware/software/compliance/integration/performance",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "standards": [
    {{
      "standard_name": "Name of standard or protocol",
      "description": "Brief description",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}
""",

    "regulatory_landscape": """Extract factual information about REGULATIONS & COMPLIANCE from this content about {query}.

URL: {url}

CRITICAL: Your response must be a valid JSON object starting with '{{' and ending with '}}'. Do not include any additional text, explanations, or markdown formatting.

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about regulations, laws, compliance requirements, and standards
2. Format each regulation with:
   - The regulation name and jurisdiction
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no regulatory information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_regulations": [
    {{
      "regulation": "Name of regulation/law/standard",
      "jurisdiction": "Geographical or industry scope",
      "description": "Brief description of requirement",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "compliance_requirements": [
    {{
      "requirement": "Specific compliance requirement",
      "description": "What must be done",
      "source_text": "Direct quote from content", 
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}

CRITICAL REQUIREMENTS:
1. Response must be a valid JSON object
2. All fields must be present (even if empty)
3. Use proper data types (strings, numbers, arrays)
4. Never use null values - use empty arrays instead
5. Include source_text for every regulation and requirement
6. Assign confidence ratings for every item
7. Calculate relevance_score based on content quality
8. Do not include any text outside the JSON object
9. Do not include comments or trailing commas
10. Use double quotes for all strings
11. Ensure all JSON objects are properly closed with matching braces
12. Do not include any line breaks or whitespace outside the JSON structure
13. Do not include any markdown code block markers (```)
14. Do not include any explanatory text before or after the JSON
15. The response must start with '{{' and end with '}}' only
16. Do not include any text that would make the JSON invalid
17. Do not include any text that would make the JSON parsing fail
18. Do not include any text that would make the JSON validation fail
19. Do not include any text that would make the JSON schema validation fail
20. Do not include any text that would make the JSON structure invalid

EXAMPLE OF VALID RESPONSE:
{{
  "extracted_regulations": [
    {{
      "regulation": "Example Regulation",
      "jurisdiction": "Example Jurisdiction",
      "description": "Example description",
      "source_text": "Example quote",
      "confidence": "high"
    }}
  ],
  "compliance_requirements": [
    {{
      "requirement": "Example requirement",
      "description": "Example description",
      "source_text": "Example quote",
      "confidence": "high"
    }}
  ],
  "relevance_score": 0.8
}}

CRITICAL: Your response must be ONLY the JSON object above, with no additional text, comments, or formatting. The response must start with '{{' and end with '}}' only.""",

    "cost_considerations": """Extract factual information about COSTS & PRICING from this content about {query}.

URL: {url}

CRITICAL: Your response must be a valid JSON object starting with '{{' and ending with '}}'. Do not include any additional text, explanations, or markdown formatting.

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about pricing, costs, budgets, TCO, ROI, and financial considerations
2. Format each fact with:
   - The specific cost information
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no cost information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_costs": [
    {{
      "cost_item": "Specific cost element",
      "amount": null,  // Include if available with currency
      "context": "Description of pricing context",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "pricing_models": [
    {{
      "model_type": "subscription/one-time/usage-based/etc",
      "description": "How the pricing works",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}

CRITICAL REQUIREMENTS:
1. Response must be a valid JSON object
2. All fields must be present (even if empty)
3. Use proper data types (strings, numbers, arrays)
4. Never use null values - use empty arrays instead
5. Include source_text for every cost and pricing model
6. Assign confidence ratings for every item
7. Calculate relevance_score based on content quality
8. Do not include any text outside the JSON object
9. Do not include comments or trailing commas
10. Use double quotes for all strings
""",

    "best_practices": """Extract factual information about BEST PRACTICES from this content about {query}.

URL: {url}

CRITICAL: Your response must be a valid JSON object starting with '{{' and ending with '}}'. Do not include any additional text, explanations, or markdown formatting.

INSTRUCTIONS:
1. ONLY extract VERIFIED best practices, methodologies, and success factors
2. Format each practice with:
   - The specific practice or methodology
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no best practices are found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_practices": [
    {{
      "practice": "Specific best practice or methodology",
      "description": "Detailed description",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "methodologies": [
    {{
      "methodology": "Name of methodology",
      "description": "How it works",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}

CRITICAL REQUIREMENTS:
1. Response must be a valid JSON object
2. All fields must be present (even if empty)
3. Use proper data types (strings, numbers, arrays)
4. Never use null values - use empty arrays instead
5. Include source_text for every practice and methodology
6. Assign confidence ratings for every item
7. Calculate relevance_score based on content quality
8. Do not include any text outside the JSON object
9. Do not include comments or trailing commas
10. Use double quotes for all strings
""",

    "implementation_factors": """Extract factual information about IMPLEMENTATION FACTORS from this content about {query}.

URL: {url}

INSTRUCTIONS:
1. ONLY extract VERIFIED facts about implementation challenges, success factors, and considerations
2. Format each factor with:
   - The specific implementation factor
   - Direct quote from the content supporting the fact
   - Confidence rating (high/medium/low)
3. If no implementation information is found, indicate this

CONTENT:
{content}

FORMAT YOUR RESPONSE AS JSON:
{{
  "extracted_factors": [
    {{
      "factor": "Specific implementation factor",
      "description": "Detailed description",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "challenges": [
    {{
      "challenge": "Specific implementation challenge",
      "description": "What makes it challenging",
      "source_text": "Direct quote from content",
      "confidence": "high/medium/low"
    }}
  ],
  "relevance_score": 0.0-1.0
}}
"""
}

# Enhanced synthesis prompt with better structure
SYNTHESIS_PROMPT: Final[str] = """Create a comprehensive synthesis of research findings for: {query}

REQUIREMENTS:
1. Structure your synthesis with these EXACT sections:
   - Domain Overview: Essential context and background
   - Market Dynamics: Size, growth, trends, competition, procurement patterns
   - Provider Landscape: Key vendors, manufacturers, distributors, and their positioning
   - Technical Requirements: Specifications, standards, and procurement requirements
   - Regulatory Landscape: Compliance, legal requirements, and procurement regulations
   - Implementation Factors: Resources, process, challenges, and procurement considerations
   - Cost Analysis: Pricing, ROI, financial factors, volume discounts, and contract terms
   - Best Practices: Recommended approaches for procurement and sourcing
   - Contract & Procurement Strategy: Contract terms, negotiation strategies, and procurement processes

2. For EACH section:
   - Synthesize insights from multiple sources when available
   - Address inconsistencies and note knowledge gaps
   - Include SPECIFIC facts with proper citations
   - Present balanced perspectives where there are differences
   - Prioritize VERIFIED information from authoritative sources
   - If information is missing for any section, explicitly note this

3. For "Confidence Assessment":
   - Evaluate information completeness for each section
   - Identify potential biases in the sources
   - Note limitations in the research
   - Assign justified confidence scores by section

AVAILABLE RESEARCH:
{research_json}

FORMAT RESPONSE AS JSON:
{{
  "synthesis": {{
    "domain_overview": {{ "content": "", "citations": [] }},
    "market_dynamics": {{ "content": "", "citations": [] }},
    "provider_landscape": {{ "content": "", "citations": [] }},
    "technical_requirements": {{ "content": "", "citations": [] }},
    "regulatory_landscape": {{ "content": "", "citations": [] }},
    "implementation_factors": {{ "content": "", "citations": [] }},
    "cost_considerations": {{ "content": "", "citations": [] }},
    "best_practices": {{ "content": "", "citations": [] }},
    "contract_procurement_strategy": {{ "content": "", "citations": [] }}
  }},
  "confidence_assessment": {{
    "overall_score": 0.0-1.0,
    "section_scores": {{
      "domain_overview": 0.0-1.0,
      "market_dynamics": 0.0-1.0,
      "provider_landscape": 0.0-1.0,
      "technical_requirements": 0.0-1.0,
      "regulatory_landscape": 0.0-1.0,
      "implementation_factors": 0.0-1.0,
      "cost_considerations": 0.0-1.0,
      "best_practices": 0.0-1.0,
      "contract_procurement_strategy": 0.0-1.0
    }},
    "limitations": [],
    "knowledge_gaps": []
  }}
}}

REMEMBER:
- Prioritize factual accuracy over comprehensiveness
- Only include claims that are supported by the research
- Use clear, concise language focused on business impact
- Highlight conflicting information when present
- Pay special attention to procurement and sourcing-related insights
"""

# Enhanced validation prompt with adaptive thresholds
VALIDATION_PROMPT: Final[str] = """Validate the research synthesis against these criteria:

VALIDATION CRITERIA:
1. Factual Accuracy
   - Does each claim have proper citation?
   - Are the citations from credible sources?
   - Are claims consistent with the source material?

2. Comprehensive Coverage
   - Are all required sections populated?
   - Is the depth appropriate for each section?
   - Are there any significant knowledge gaps?

3. Source Quality
   - Are sources diverse and authoritative?
   - Are recent sources used where appropriate?
   - Is there over-reliance on any single source?

4. Overall Quality
   - Is confidence assessment realistic?
   - Are limitations properly acknowledged?
   - Is the synthesis balanced and objective?

RESEARCH SYNTHESIS TO VALIDATE:
{synthesis_json}

FORMAT RESPONSE AS JSON:
{{
  "validation_results": {{
    "is_valid": true/false,
    "validation_score": 0.0-1.0,
    "section_validations": {{
      "domain_overview": {{ "is_valid": true/false, "issues": [] }},
      "market_dynamics": {{ "is_valid": true/false, "issues": [] }},
      "provider_landscape": {{ "is_valid": true/false, "issues": [] }},
      "technical_requirements": {{ "is_valid": true/false, "issues": [] }},
      "regulatory_landscape": {{ "is_valid": true/false, "issues": [] }},
      "implementation_factors": {{ "is_valid": true/false, "issues": [] }},
      "cost_considerations": {{ "is_valid": true/false, "issues": [] }},
      "best_practices": {{ "is_valid": true/false, "issues": [] }}
    }},
    "critical_issues": [],
    "improvement_suggestions": []
  }},
  "adaptive_threshold": {{
    "minimum_valid_sections": 0-8,
    "required_sections": [],
    "section_weights": {{
      "domain_overview": 0.0-1.0,
      "market_dynamics": 0.0-1.0,
      "provider_landscape": 0.0-1.0,
      "technical_requirements": 0.0-1.0,
      "regulatory_landscape": 0.0-1.0,
      "implementation_factors": 0.0-1.0,
      "cost_considerations": 0.0-1.0,
      "best_practices": 0.0-1.0
    }}
  }}
}}

IMPORTANT:
- Calculate "minimum_valid_sections" based on query complexity and available data
- Identify critical sections as "required_sections" based on query intent
- Assign weights to sections based on importance to the query
- A synthesis can be valid even with some sections incomplete if priority sections are solid
- Flag fabricated or unsupported claims as critical issues
"""

# Enhanced report template with executive summary format
REPORT_TEMPLATE: Final[str] = """
# Research Report: {query}

## Executive Summary
{executive_summary}

## Key Findings
{key_findings}

## Detailed Analysis

### Market Dynamics
{market_dynamics}

### Provider Landscape
{provider_landscape}

### Technical Requirements
{technical_requirements}

### Regulatory Landscape
{regulatory_landscape}

### Implementation Considerations
{implementation_factors}

### Cost Analysis
{cost_considerations}

### Best Practices
{best_practices}

### Contract & Procurement Strategy
{contract_procurement_strategy}

## Recommendations
{recommendations}

## Sources and Citations
{sources}

---
Confidence Score: {confidence_score}
Generated: {generation_date}
"""

# Enhanced clarity request prompt
CLARIFICATION_PROMPT: Final[str] = """I'm analyzing your research request: "{query}"

Based on my initial analysis, I need some additional context to provide you with the most relevant research.

What I understand so far:
- Product/Service Focus: {product_vs_service}
- Industry Context: {industry_context}
- Geographical Scope: {geographical_focus}

To deliver more precise and comprehensive research, I need clarification on:

{missing_sections}

Could you please provide these additional details? This will help me focus the research on your specific needs rather than making assumptions.

Even with partial clarification, I can begin the research process and refine as we go."""

# Category-specific search quality thresholds
SEARCH_QUALITY_THRESHOLDS: Dict[str, Dict[str, float]] = {
    "market_dynamics": {
        "min_sources": 3,
        "min_facts": 5,
        "recency_threshold_days": 180,  # Market data needs to be recent
        "authoritative_source_ratio": 0.5  # At least half from authoritative sources
    },
    "provider_landscape": {
        "min_sources": 3,
        "min_facts": 3,
        "recency_threshold_days": 365,
        "authoritative_source_ratio": 0.3
    },
    "technical_requirements": {
        "min_sources": 2,
        "min_facts": 3,
        "recency_threshold_days": 730,  # Technical specs can be older
        "authoritative_source_ratio": 0.7  # Need highly authoritative sources
    },
    "regulatory_landscape": {
        "min_sources": 2,
        "min_facts": 2,
        "recency_threshold_days": 730,
        "authoritative_source_ratio": 0.8  # Regulatory info needs official sources
    },
    "cost_considerations": {
        "min_sources": 2,
        "min_facts": 3,
        "recency_threshold_days": 365,  # Pricing should be recent
        "authoritative_source_ratio": 0.4
    },
    "best_practices": {
        "min_sources": 2,
        "min_facts": 3,
        "recency_threshold_days": 730,
        "authoritative_source_ratio": 0.5
    },
    "implementation_factors": {
        "min_sources": 2,
        "min_facts": 3,
        "recency_threshold_days": 730,
        "authoritative_source_ratio": 0.4
    }
}

# Helper function for creating category-specific search prompts
def get_extraction_prompt(category: str, query: str, url: str, content: str) -> str:
    """Get the appropriate extraction prompt for a specific category."""
    if category in EXTRACTION_PROMPTS:
        prompt = EXTRACTION_PROMPTS[category]
        return prompt.format(
            query=query,
            url=url,
            content=content
        )
    else:
        # Fallback to general extraction prompt
        prompt = EXTRACTION_PROMPTS["market_dynamics"]
        return prompt.format(
            query=query,
            url=url,
            content=content
        )

# Prompt for identifying additional research topics
ADDITIONAL_TOPICS_PROMPT: Final[str] = """Based on the current research findings, identify additional topics that would enhance the analysis.

Current Research:
{current_research}

Consider:
1. Related market segments or industries
2. Emerging technologies or trends
3. Regulatory or compliance areas
4. Implementation considerations
5. Cost factors
6. Best practices

Format your response as JSON:
{
    "additional_topics": [
        {
            "topic": "Topic name",
            "relevance": 0.0-1.0,
            "rationale": "Why this topic is important"
        }
    ],
    "priority_order": ["topic1", "topic2", ...],
    "estimated_effort": {
        "topic1": "high/medium/low",
        "topic2": "high/medium/low",
        ...
    }
}"""

# Base research prompt
RESEARCH_BASE_PROMPT: Final[str] = """You are a Research Agent focused on gathering comprehensive market intelligence.

Your task is to analyze the following query and provide detailed research findings.

Query: {query}

INSTRUCTIONS:
1. Break down the query into research components
2. Identify key areas for investigation
3. Gather relevant market data
4. Analyze trends and patterns
5. Synthesize findings

RESPONSE FORMAT:
{
    "research_components": ["Component 1", "Component 2"],
    "key_findings": ["Finding 1", "Finding 2"],
    "sources": ["Source 1", "Source 2"],
    "confidence_score": 0.0
}"""

# Research agent prompt
RESEARCH_AGENT_PROMPT: Final[str] = """You are an advanced Research Agent specialized in market analysis.

Your task is to conduct comprehensive research on the following topic.

Topic: {topic}

INSTRUCTIONS:
1. Identify key research areas
2. Gather market intelligence
3. Analyze trends and patterns
4. Evaluate sources and credibility
5. Synthesize findings

RESPONSE FORMAT:
{
    "research_areas": ["Area 1", "Area 2"],
    "findings": ["Finding 1", "Finding 2"],
    "sources": ["Source 1", "Source 2"],
    "confidence_score": 0.0
}"""

# Topics prompt
TOPICS_PROMPT: Final[str] = """Analyze the following query to identify key research topics.

Query: {query}

INSTRUCTIONS:
1. Break down the query into main topics
2. Identify subtopics for each main topic
3. Prioritize topics by relevance
4. Consider industry context
5. Note any specialized areas

RESPONSE FORMAT:
{
    "main_topics": ["Topic 1", "Topic 2"],
    "subtopics": {
        "Topic 1": ["Subtopic 1", "Subtopic 2"],
        "Topic 2": ["Subtopic 1", "Subtopic 2"]
    },
    "priority_order": ["Topic 1", "Topic 2"],
    "specialized_areas": ["Area 1", "Area 2"]
}"""

# Export all prompts and utilities
__all__ = [
    "STRUCTURED_OUTPUT_VALIDATION",
    "QUERY_ANALYSIS_PROMPT",
    "EXTRACTION_PROMPTS",
    "SYNTHESIS_PROMPT",
    "VALIDATION_PROMPT",
    "REPORT_TEMPLATE",
    "CLARIFICATION_PROMPT",
    "SEARCH_QUALITY_THRESHOLDS",
    "get_extraction_prompt",
    "ADDITIONAL_TOPICS_PROMPT",
    "RESEARCH_BASE_PROMPT",
    "RESEARCH_AGENT_PROMPT",
    "TOPICS_PROMPT"
]
</file>

<file path="src/react_agent/prompts/synthesis.py">
"""Enhanced synthesis and output module.

This module provides improved synthesis and output formatting capabilities
to create more thorough, insightful and verbose research reports with 
better statistics integration and citation handling.
"""

from typing import Dict, List, Any, Optional, Union, Tuple
import json
from datetime import datetime, timezone
import re

from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableConfig

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight, error_highlight
from react_agent.utils.llm import call_model_json
from react_agent.utils.extraction import extract_statistics
from react_agent.utils.defaults import get_default_extraction_result
from react_agent.utils.cache import ProcessorCache, create_checkpoint, load_checkpoint
from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver

# Initialize logger
logger = get_logger(__name__)

# Initialize processor cache for synthesis
synthesis_cache = ProcessorCache(thread_id="synthesis")

# Initialize memory saver for caching
memory_saver = MemorySaver()

# Enhanced synthesis prompt template
ENHANCED_SYNTHESIS_PROMPT = """Create a comprehensive synthesis of research findings for: {query}

REQUIREMENTS:
1. Structure your synthesis with these EXACT sections:
   - Executive Summary: Concise overview of key findings with critical statistics
   - Domain Overview: Essential context and background with industry statistics
   - Market Dynamics: Size, growth, trends, competition, procurement patterns with market statistics
   - Provider Landscape: Key vendors, manufacturers, distributors, and their positioning with market share data
   - Technical Requirements: Specifications, standards, and procurement requirements with technical statistics
   - Regulatory Landscape: Compliance, legal requirements, and procurement regulations with compliance data
   - Implementation Factors: Resources, process, challenges, and procurement considerations with implementation statistics
   - Cost Analysis: Pricing, ROI, financial factors, volume discounts, and contract terms with financial metrics
   - Best Practices: Recommended approaches for procurement and sourcing with adoption statistics
   - Contract & Procurement Strategy: Contract terms, negotiation strategies, and procurement processes with benchmarks

2. For EACH section:
   - Synthesize insights from multiple sources when available
   - Prioritize STATISTICAL information and NUMERICAL data
   - Include specific PERCENTAGES, AMOUNTS, and METRICS
   - Address inconsistencies and note knowledge gaps
   - Include SPECIFIC facts with proper citations
   - Present balanced perspectives where there are differences
   - Prioritize VERIFIED information from authoritative sources
   - If information is missing for any section, explicitly note this

3. For "Confidence Assessment":
   - Evaluate information completeness for each section
   - Identify potential biases in the sources
   - Note limitations in the research
   - Assign justified confidence scores by section
   - Provide specific reasons for confidence ratings

AVAILABLE RESEARCH:
{research_json}

FORMAT RESPONSE AS JSON:
{{
  "synthesis": {{
    "executive_summary": {{ "content": "", "citations": [], "statistics": [] }},
    "domain_overview": {{ "content": "", "citations": [], "statistics": [] }},
    "market_dynamics": {{ "content": "", "citations": [], "statistics": [] }},
    "provider_landscape": {{ "content": "", "citations": [], "statistics": [] }},
    "technical_requirements": {{ "content": "", "citations": [], "statistics": [] }},
    "regulatory_landscape": {{ "content": "", "citations": [], "statistics": [] }},
    "implementation_factors": {{ "content": "", "citations": [], "statistics": [] }},
    "cost_considerations": {{ "content": "", "citations": [], "statistics": [] }},
    "best_practices": {{ "content": "", "citations": [], "statistics": [] }},
    "contract_procurement_strategy": {{ "content": "", "citations": [], "statistics": [] }}
  }},
  "confidence_assessment": {{
    "overall_score": 0.0-1.0,
    "section_scores": {{
      "executive_summary": 0.0-1.0,
      "domain_overview": 0.0-1.0,
      "market_dynamics": 0.0-1.0,
      "provider_landscape": 0.0-1.0,
      "technical_requirements": 0.0-1.0,
      "regulatory_landscape": 0.0-1.0,
      "implementation_factors": 0.0-1.0,
      "cost_considerations": 0.0-1.0,
      "best_practices": 0.0-1.0,
      "contract_procurement_strategy": 0.0-1.0
    }},
    "limitations": [],
    "knowledge_gaps": [],
    "confidence_justifications": {{
      "executive_summary": "",
      "domain_overview": "",
      "market_dynamics": "",
      "provider_landscape": "",
      "technical_requirements": "",
      "regulatory_landscape": "",
      "implementation_factors": "",
      "cost_considerations": "",
      "best_practices": "",
      "contract_procurement_strategy": ""
    }}
  }}
}}

REMEMBER:
- Prioritize statistical data and numerical findings
- Include specific numbers, percentages, and metrics
- Emphasize recent studies, surveys, and market reports
- Highlight data from industry-leading sources
- Only include claims that are supported by the research
- Use clear, concise language focused on business impact
- Highlight conflicting information when present
- Pay special attention to procurement and sourcing-related insights
"""

# Enhanced report template with better statistics and citations
ENHANCED_REPORT_TEMPLATE = """
# {title}

## Executive Summary
{executive_summary}

## Key Findings & Statistics
{key_statistics}

## Domain Overview
{domain_overview}

## Market Analysis
### Market Size & Growth
{market_size}

### Competitive Landscape
{competitive_landscape}

### Trends & Developments
{market_trends}

## Provider Landscape
### Key Vendors
{key_vendors}

### Vendor Comparison
{vendor_comparison}

## Technical Requirements
{technical_requirements}

## Regulatory Considerations
{regulatory_landscape}

## Implementation Strategy
{implementation_factors}

## Cost Analysis
### Cost Structure
{cost_structure}

### Pricing Models
{pricing_models}

### ROI Considerations
{roi_considerations}

## Best Practices
{best_practices}

## Procurement Strategy
{procurement_strategy}

## Recommendations
{recommendations}

## Sources & Citations
{sources}

---
**Research Confidence:** {confidence_score}/1.0  
**Date Generated:** {generation_date}  
{confidence_notes}
"""

def extract_all_statistics(synthesis: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Extract all statistics from synthesis results."""
    all_stats = []
    
    for section_name, section_data in synthesis.items():
        if isinstance(section_data, dict) and "statistics" in section_data:
            stats = section_data.get("statistics", [])
            if stats and isinstance(stats, list):
                for stat in stats:
                    if isinstance(stat, dict):
                        # Add section name to statistic
                        stat["section"] = section_name
                        all_stats.append(stat)
    
    # Sort by quality score if available
    sorted_stats = sorted(
        all_stats,
        key=lambda x: x.get("quality_score", 0),
        reverse=True
    )
    
    return sorted_stats

def format_citation(citation: Dict[str, Any]) -> str:
    """Format a citation for inclusion in the report."""
    if not citation or not isinstance(citation, dict):
        return ""
        
    title = citation.get("title", "")
    source = citation.get("source", "")
    url = citation.get("url", "")
    date = citation.get("date", "")
    
    if title and source:
        return f"{title} ({source}{', ' + date if date else ''})"
    elif title:
        return title
    elif source:
        return source
    elif url:
        return url
    else:
        return "Unnamed source"

def format_statistic(stat: Dict[str, Any]) -> str:
    """Format a statistic for inclusion in the report."""
    if not stat or not isinstance(stat, dict):
        return ""
        
    text = stat.get("text", "")
    citation = ""
    
    # Add citation if available
    citations = stat.get("citations", [])
    if citations and isinstance(citations, list) and len(citations) > 0:
        first_citation = citations[0]
        if isinstance(first_citation, dict):
            source = first_citation.get("source", "")
            if source:
                citation = f" ({source})"
    
    return f"{text}{citation}"

def highlight_statistics_in_content(content: str, statistics: List[Dict[str, Any]]) -> str:
    """Highlight statistics in content with bold formatting."""
    if not content or not statistics:
        return content
        
    highlighted_content = content
    
    for stat in statistics:
        if isinstance(stat, dict) and "text" in stat:
            text = stat.get("text", "")
            if text and text in highlighted_content:
                # Highlight the statistic with bold formatting
                highlighted_content = highlighted_content.replace(text, f"**{text}**")
    
    return highlighted_content

async def synthesize_research(
    state: Dict[str, Any],
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Synthesize all research data into a comprehensive result with enhanced statistics focus."""
    info_highlight("Synthesizing research results with enhanced statistics focus")
    
    try:
        # Check cache with TTL
        cache_key = f"synthesize_research_{hash(str(state))}"
        if cached_state := synthesis_cache.get(cache_key):
            if cached_state.get("data"):
                return cached_state["data"]
        
        categories = state["categories"]
        original_query = state["original_query"]
        
        # Prepare research data for synthesis
        research_data = {}
        for category, category_state in categories.items():
            # Extract statistics from facts
            statistics = []
            for fact in category_state.get("extracted_facts", []):
                if "statistics" in fact:
                    statistics.extend(fact["statistics"])
                elif "source_text" in fact:
                    # Extract statistics from source text
                    extracted_stats = extract_statistics(fact["source_text"])
                    statistics.extend(extracted_stats)
            
            research_data[category] = {
                "facts": category_state["extracted_facts"],
                "sources": category_state["sources"],
                "quality_score": category_state["quality_score"],
                "statistics": statistics  # Add extracted statistics
            }
        
        # Generate prompt
        synthesis_prompt = ENHANCED_SYNTHESIS_PROMPT.format(
            query=original_query,
            research_json=json.dumps(research_data, indent=2)
        )
        
        # Call model for synthesis
        synthesis_result = await call_model_json(
            messages=[{"role": "human", "content": synthesis_prompt}],
            config=config
        )
        
        # Extract key statistics for reference
        synthesis_sections = synthesis_result.get("synthesis", {})
        all_statistics = extract_all_statistics(synthesis_sections)
        synthesis_result["key_statistics"] = all_statistics[:10]  # Top 10 statistics
        
        overall_score = synthesis_result.get("confidence_assessment", {}).get("overall_score", 0.0)
        info_highlight(f"Research synthesis complete with confidence score: {overall_score:.2f}")
        
        result = {
            "synthesis": synthesis_result,
            "status": "synthesized"
        }
        
        # Cache result with TTL
        synthesis_cache.put(
            cache_key,
            {
                "data": result,
                "timestamp": datetime.now(timezone.utc).isoformat()
            },
            ttl=3600  # 1 hour TTL
        )
        
        return result
        
    except Exception as e:
        error_highlight(f"Error in research synthesis: {str(e)}")
        return {"error": {"message": f"Error in research synthesis: {str(e)}", "phase": "synthesis"}}

async def prepare_enhanced_response(
    state: Dict[str, Any],
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Prepare an enhanced final response with detailed statistics and citations."""
    info_highlight("Preparing enhanced final response")
    
    synthesis = state.get("synthesis", {})
    validation = state.get("validation_result", {})
    original_query = state["original_query"]
    
    # Get synthesis content
    synthesis_content = synthesis.get("synthesis", {}) if synthesis else {}
    confidence = synthesis.get("confidence_assessment", {}) if synthesis else {}
    
    # Get all statistics
    all_statistics = synthesis.get("key_statistics", []) if synthesis else []
    
    # Format the title with proper capitalization
    title = "Research Results: " + original_query.capitalize()
    
    # Format each section with highlighted statistics
    sections = {}
    for section_name, section_data in synthesis_content.items():
        if isinstance(section_data, dict) and "content" in section_data:
            content = section_data.get("content", "")
            statistics = section_data.get("statistics", [])
            
            # Highlight statistics in content
            highlighted_content = highlight_statistics_in_content(content, statistics)
            
            # Add citations
            citations = section_data.get("citations", [])
            if citations and isinstance(citations, list) and len(citations) > 0:
                citation_text = "\n\n**Sources:** " + ", ".join(
                    format_citation(citation) for citation in citations if citation
                )
                sections[section_name] = highlighted_content + citation_text
            else:
                sections[section_name] = highlighted_content
    
    # Format key statistics section
    key_stats_formatted = []
    for stat in all_statistics:
        formatted_stat = format_statistic(stat)
        if formatted_stat:
            key_stats_formatted.append(f"- {formatted_stat}")
    
    key_statistics_section = "\n".join(key_stats_formatted) if key_stats_formatted else "No key statistics available."
    
    # Format sources section
    sources_set = set()
    for section_data in synthesis_content.values():
        if isinstance(section_data, dict) and "citations" in section_data:
            citations = section_data.get("citations", [])
            for citation in citations:
                if isinstance(citation, dict):
                    formatted = format_citation(citation)
                    if formatted:
                        sources_set.add(formatted)
    
    sources_list = sorted(list(sources_set))
    sources_section = "\n".join(f"- {source}" for source in sources_list) if sources_list else "No sources available."
    
    # Get confidence information
    confidence_score = confidence.get("overall_score", 0.0)
    limitations = confidence.get("limitations", [])
    knowledge_gaps = confidence.get("knowledge_gaps", [])
    
    # Format confidence notes
    confidence_notes = []
    if limitations:
        confidence_notes.append("**Limitations:** " + ", ".join(limitations))
    if knowledge_gaps:
        confidence_notes.append("**Knowledge Gaps:** " + ", ".join(knowledge_gaps))
    
    confidence_notes_text = "\n".join(confidence_notes)
    
    # Generate recommendations based on synthesis
    recommendations = await generate_recommendations(synthesis_content, original_query, config)
    
    # Fill in the template
    report_content = ENHANCED_REPORT_TEMPLATE.format(
        title=title,
        executive_summary=sections.get("executive_summary", "No executive summary available."),
        key_statistics=key_statistics_section,
        domain_overview=sections.get("domain_overview", "No domain overview available."),
        market_size=sections.get("market_dynamics", "No market dynamics information available."),
        competitive_landscape=sections.get("provider_landscape", "No provider landscape information available."),
        market_trends=sections.get("market_dynamics", "No market trends information available."),
        key_vendors=sections.get("provider_landscape", "No vendor information available."),
        vendor_comparison=sections.get("provider_landscape", "No vendor comparison available."),
        technical_requirements=sections.get("technical_requirements", "No technical requirements information available."),
        regulatory_landscape=sections.get("regulatory_landscape", "No regulatory information available."),
        implementation_factors=sections.get("implementation_factors", "No implementation information available."),
        cost_structure=sections.get("cost_considerations", "No cost structure information available."),
        pricing_models=sections.get("cost_considerations", "No pricing models information available."),
        roi_considerations=sections.get("cost_considerations", "No ROI considerations available."),
        best_practices=sections.get("best_practices", "No best practices information available."),
        procurement_strategy=sections.get("contract_procurement_strategy", "No procurement strategy information available."),
        recommendations=recommendations,
        sources=sources_section,
        confidence_score=f"{confidence_score:.2f}",
        generation_date=datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC"),
        confidence_notes=confidence_notes_text
    )
    
    # Create the response message
    response_message = AIMessage(content=report_content)
    
    return {
        "messages": [response_message],
        "status": "complete",
        "complete": True
    }

async def generate_recommendations(
    synthesis_content: Dict[str, Any],
    query: str,
    config: Optional[RunnableConfig] = None
) -> str:
    """Generate recommendations based on synthesis content."""
    info_highlight("Generating recommendations based on synthesis")
    
    if not synthesis_content:
        return "No recommendations available due to insufficient data."
    
    # Create a prompt for recommendations
    recommendations_prompt = f"""
    Based on the research synthesis for "{query}", generate 5-7 specific, actionable recommendations.
    Each recommendation should:
    1. Be specific and actionable
    2. Reference relevant statistics or findings when available
    3. Address a key need or gap identified in the research
    4. Be practical and implementable
    5. Include expected benefits or outcomes
    
    Synthesis data:
    {json.dumps(synthesis_content, indent=2)}
    
    FORMAT:
    Return a markdown list of recommendations with brief explanations.
    """
    
    try:
        response = await call_model_json(
            messages=[{"role": "human", "content": recommendations_prompt}],
            config=config
        )
        
        if isinstance(response, dict) and "recommendations" in response:
            return response["recommendations"]
        elif isinstance(response, dict) and "content" in response:
            return response["content"]
        elif isinstance(response, str):
            return response
        else:
            # Default format if response structure is unexpected
            return "Recommendations could not be generated due to unexpected response format."
    except Exception as e:
        error_highlight(f"Error generating recommendations: {str(e)}")
        return "Recommendations could not be generated due to an error."

__all__ = [
    "chunk_text",
    "preprocess_content",
    "estimate_tokens",
    "should_skip_content",
    "merge_chunk_results",
    "validate_content",
    "detect_content_type"
]
</file>

<file path="src/react_agent/prompts/templates.py">
"""Main prompt templates.

This module provides functionality for main prompt templates
in the agent framework.
"""

from typing import Final

# Common validation template used across multiple prompts
STRUCTURED_OUTPUT_VALIDATION: Final[str] = """CRITICAL: All responses MUST:
1. Be valid JSON only - no additional text or comments
2. Follow the exact schema provided
3. Never return empty or null values
4. Include all required fields
5. Use proper data types (strings, numbers, arrays)
6. Maintain proper JSON syntax
7. Include citations for all data points
8. Pass JSON schema validation

Any response that fails these requirements will be rejected."""

# Validation requirements component - reusable across prompts
VALIDATION_REQUIREMENTS: Final[str] = """VALIDATION REQUIREMENTS:
1. Structural Validation
   - Verify JSON syntax is valid
   - Check all required fields are present
   - Ensure no empty or null values
   - Validate data types match schema
   - Check array elements follow required format

2. Citation Validation
   - Verify each citation URL exists and is accessible
   - Ensure at least 2 citations per analysis section
   - Validate source credibility and relevance
   - Cross-reference data points across sources"""

# Main prompt for the primary agent
MAIN_PROMPT: Final[
    str
] = """You are conducting web research for RFP category analysis and market basket development.
Your goal is to produce a structured JSON response following this exact schema:
{
    "rfp_analysis": {
        "analysis": {
            "porters_5_forces": {
                "competitive_rivalry": "",
                "threat_of_new_entrants": "",
                "threat_of_substitutes": "",
                "bargaining_power_buyers": "",
                "bargaining_power_suppliers": ""
            },
            "swot": {
                "strengths": [],
                "weaknesses": [],
                "opportunities": [],
                "threats": []
            },
            "recent_breakthroughs_and_disruptors": "",
            "cost_trends_and_projections": "",
            "typical_contract_clauses_and_pricing_nuances": "",
            "competitive_landscape": ""
        },
        "market_basket": [
            {
                "manufacturer_or_distributor": "",
                "item_number": "",
                "item_description": "",
                "uom": "",
                "estimated_qty_per_uom": 0.0,
                "unit_cost": 0.0
            }
        ]
    },
    "confidence_score": 0.0
}
Category to analyze: {topic}
IMPORTANT INSTRUCTIONS:
1. Your response must be ONLY valid JSON - no additional text, comments or explanations
2. Every field must be populated - no empty strings or null values
3. If you cannot structure some information, include it under a "raw_findings" key
4. Do not truncate or leave responses incomplete
5. Ensure all JSON syntax is valid (quotes, commas, brackets)
Available tools:
1. Search: Query search engines for industry and market information
2. ScrapeWebsite: Extract structured data from industry sources
3. SummarizeResearch: Generate AI-powered summaries for complex topics
4. SearchNews: Find recent news articles and industry developments
5. Info: Compile and format final findings
"""

# Tool descriptions
WEB_SEARCH_DESC: Final[str] = """Search the web for information about a topic.
Input should be a search query string.
Returns up to 3 search results with titles, URLs, and snippets."""

SCRAPE_DESC: Final[str] = """Scrape content from a website URL.
Input should be a valid URL.
Returns the scraped content and metadata."""

# New tool descriptions for Brave Summarizer and News APIs
SUMMARIZER_DESC: Final[
    str
] = """Generate an AI-powered summary of search results for a topic.
Input should be a search query string.
Returns a comprehensive summary along with key topics and 5 source articles."""

NEWS_SEARCH_DESC: Final[str] = """Search for recent news articles related to a topic.
Input should be a search query string.
Returns 5 news articles with titles, URLs, descriptions, and sources."""

# Tool instructions for reuse across agent nodes
TOOL_INSTRUCTIONS: Final[str] = """
IMPORTANT:
1. Use the search_web tool to find relevant information (returns 3 results per query)
2. Use the search_news tool for recent developments and news (returns 5 results per query)
3. Use the scrape_website tool to extract detailed content from websites
4. Use the summarize_research tool to get AI-powered summaries of complex topics (returns 5 sources per query)
5. Always include proper citations for all information
6. Follow all research requirements in the prompt
"""

# Evaluation prompt template for content evaluation
EVALUATION_PROMPT_TEMPLATE: Final[
    str
] = """You are an evaluation system that assesses the quality of AI responses.
Review the following response and provide scores and feedback.

Task description: {task_description}

Response to evaluate:
{response}

Please evaluate this response on these criteria: {criteria}.
For each criterion, provide a score from 0.0 to 1.0 and brief feedback."""

# Reflection prompt templates
FEEDBACK_PROMPT_TEMPLATE: Final[str] = """You are an AI improvement coach.
Based on the critique and evaluation of a previous response, generate actionable feedback 
to help improve future responses.

Original task: {task}

Previous response: {response}

Critique: {critique}

Evaluation scores: {scores}

Generate specific, actionable feedback with examples of how to improve."""

CRITIQUE_PROMPT_TEMPLATE: Final[str] = """You are an expert evaluator providing critique.
Review the following response and provide detailed feedback.

Task: {task}
Response: {response}
Evaluation criteria: {criteria}

Provide specific critique points and actionable suggestions for improvement."""

ANALOGICAL_REASONING_PROMPT: Final[str] = """You are an expert at improving solutions through analogical reasoning.

Current task: {task}
Current response: {response}
Similar examples:
{examples}

Based on these examples, suggest improvements to the current response."""

COUNTERFACTUAL_PROMPT: Final[str] = """You are an expert at generating counterfactual improvements.
Consider 'what if' scenarios that could lead to better outcomes.

Current response: {response}
Areas for improvement: {areas}

Generate counterfactual scenarios and corresponding improvements."""

METACOGNITION_PROMPT: Final[str] = """You are an expert at analyzing thinking processes and cognitive patterns.
Identify patterns, biases, and potential improvements in the reasoning process.

Conversation history: {history}
Current scores: {scores}
Improvement areas: {areas}

Analyze the thinking process and suggest meta-level improvements."""

# Detailed feedback prompt templates
DETAILED_FEEDBACK_PROMPT: Final[str] = """You are an AI improvement coach providing detailed feedback.
Review the following response and generate specific, actionable feedback.

CONTEXT:
Original task: {task}
Previous response: {response}
Critique points: {critique}
Current scores: {scores}

REQUIREMENTS:
1. Provide specific examples of what could be improved
2. Suggest concrete implementation steps
3. Reference similar successful approaches
4. Highlight both strengths and areas for improvement
5. Maintain constructive and actionable tone

Generate detailed, actionable feedback that addresses:
1. Content quality and accuracy
2. Structure and organization
3. Completeness and depth
4. Implementation and practicality
5. Overall effectiveness"""

REFLECTION_FEEDBACK_PROMPT: Final[str] = """You are an AI reflection coach.
Help improve responses through structured reflection and feedback.

CONTEXT:
Task description: {task}
Current response: {response}
Evaluation scores: {scores}
Areas for improvement: {areas}

REFLECTION POINTS:
1. What worked well in the current approach?
2. What could have been done differently?
3. How can we apply lessons from similar successful cases?
4. What specific steps would lead to better outcomes?

Provide actionable feedback focusing on:
1. Strategic improvements
2. Tactical adjustments
3. Process refinements
4. Quality enhancements"""

STRUCTURED_SYSTEM_PROMPT: Final[str] = """You are a helpful assistant that can answer questions and help with tasks."""

SYSTEM_PROMPT: Final[str] = """You are a helpful assistant that can answer questions and help with tasks."""
</file>

<file path="src/react_agent/prompts/validation.py">
"""Validation-specific prompts.

This module provides functionality for validation-specific prompts
in the agent framework.
"""

from typing import Final

# Validation base prompt
VALIDATION_BASE_PROMPT: Final[
    str
] = """You are a Validation Agent for RFP market analysis.
Your goal is to prevent hallucinations and ensure data quality.

{VALIDATION_REQUIREMENTS}

3. Content Validation
   - Verify all required fields are populated
   - Check for data consistency across sections
   - Validate numerical data and calculations
   - Ensure analysis conclusions are supported by data

4. Market Basket Validation
   - Verify product information accuracy
   - Cross-check pricing against multiple sources
   - Validate manufacturer/distributor details
   - Ensure proper unit of measure conversions

5. Analysis Quality
   - Verify PESTEL factors are comprehensive
   - Check GAP analysis identifies clear needs
   - Validate cost-benefit calculations
   - Review risk assessment completeness
   - Cross-check TCO components
   - Verify vendor analysis objectivity
   - Check benchmarking methodology
   - Validate stakeholder identification
   - Ensure compliance requirements are current
   - Verify business impact assessments

CONFIDENCE SCORING:
- Start with base score of 0.4
- Add 0.1 for each validated section with 2+ citations
- Add 0.1 for each verified market basket item
- Add 0.1 for comprehensive analysis coverage
- Subtract 0.1 for each validation failure
- Reject if final score < 0.98

RESPONSE_FORMAT:
{
    "validation_results": {
        "is_valid": false,
        "errors": [],
        "warnings": [],
        "confidence_score": 0.0,
        "section_scores": {
            "structural": 0.0,
            "citations": 0.0,
            "content": 0.0,
            "market_basket": 0.0
        },
        "failed_validations": [],
        "required_fixes": []
    }
}

Current state: {state}
"""

# Validation agent prompt with structured output validation
VALIDATION_AGENT_PROMPT: Final[str] = VALIDATION_BASE_PROMPT.replace(
    "Your goal is to prevent hallucinations and ensure data quality.\n",
    "Your goal is to prevent hallucinations and ensure data quality.\n\n{STRUCTURED_OUTPUT_VALIDATION}\n",
)

# Prompt for generating validation criteria
VALIDATION_CRITERIA_PROMPT: Final[str] = """
Content Type: {content_type}
Generate appropriate validation criteria for content of this type.
The criteria should be comprehensive and tailored to the specific content type.
For example:
- For research content: factual accuracy, source credibility, logical consistency
- For analysis content: methodological soundness, statistical validity, interpretative accuracy
- For code: functional correctness, efficiency, security, readability

Format your response as a JSON object with these fields:
- primary_criteria: List of primary validation criteria (string[])
- secondary_criteria: List of secondary validation criteria (string[])
- critical_requirements: List of must-have elements (string[])
- disqualifying_factors: List of automatic disqualifiers (string[])
- scoring_weights: Dictionary mapping criteria to weights (0.0 to 1.0)
"""

# Prompt for fact checking
FACT_CHECK_CLAIMS_PROMPT: Final[str] = """
Content Type: {content_type}
Analyze the following content for factual accuracy of claims:
{content}

1. Identify claims that are factual, opinion-based, unclear, or contradictory.
2. Provide source citations for each claim.
3. Evaluate the credibility of sources.
4. Verify the accuracy of each claim.
5. Determine if the content as a whole is factually accurate.
6. Identify any potential biases or conflicts of interest.
7. Note any areas where more research is needed.

Respond in JSON format with these fields:
- factually_accurate_claims: string[] (list of factual claims)
- opinion_based_claims: string[] (list of opinion-based claims)
- unclear_claims: string[] (list of unclear claims)
- source_citations: string[] (list of source URLs for each claim)
- source_credibility: string[] (list of source credibility scores)
- verification_results: string[] (list of verification results)
- overall_accuracy: number from 0-10
- potential_biases: string[] (list of potential biases)
- areas_for_future_research: string[] (list of areas for future research)
- issues: string[] (summarizing all critical issues)
"""

# Prompt for validating individual claims
VALIDATE_CLAIM_PROMPT: Final[str] = """
Fact check the following claim:
CLAIM: {claim}

Respond in JSON format with these fields:
- accuracy: number from 0-10
- confidence: number from 0-10
- issues: string[] (empty if no issues)
- verification_notes: string
"""

# Prompt for logic validation
LOGIC_VALIDATION_PROMPT: Final[str] = """
Content Type: {content_type}
Validate the logical consistency, reasoning quality, and argument structure of the following content:
{content}

Analyze for:
1. Valid argument structure (premises, conclusions)
2. Logical fallacies (e.g., circular reasoning, false cause)
3. Consistency between claims
4. Quality of evidence and reasoning
5. Appropriate conclusions

Respond in JSON format with these fields:
- logical_structure_score: number from 0-10
- fallacies_found: string[] (empty if none)
- consistency_issues: string[] (empty if none)
- reasoning_quality: number from 0-10
- conclusion_validity: number from 0-10
- overall_score: number from 0-10
- issues: string[] (summarizing all critical issues)
"""

# Prompt for consistency checking
CONSISTENCY_CHECK_PROMPT: Final[str] = """
Content Type: {content_type}
Check the internal consistency and coherence of the following content:
{content}

Analyze for:
1. Consistency between different sections
2. Coherence of narrative or explanation
3. Presence of contradictions
4. Logical flow and structure
5. Completeness (no missing pieces in the reasoning)

Respond in JSON format with these fields:
- section_consistency: number from 0-10
- coherence_score: number from 0-10
- contradictions: string[] (empty if none)
- flow_quality: number from 0-10
- completeness: number from 0-10
- overall_score: number from 0-10
- issues: string[] (summarizing all critical issues)
- needs_human_review: boolean (true if human review is recommended)
"""

# Prompt for human feedback request
HUMAN_FEEDBACK_PROMPT: Final[str] = """
Content Type: {content_type}
Based on automated validation, the following issues were identified:
{issues}

Generate 3-5 specific questions for human reviewers to address these issues.
Questions should be clear, focused, and help improve the quality of the content.
Additionally, suggest specific sections or aspects that need human attention.

Format your response as JSON with these fields:
- questions: string[] (list of questions)
- focus_areas: string[] (specific aspects needing review)
- content_summary: string (brief summary of the content)
"""
</file>

<file path="src/react_agent/tools/jina.py">
"""Enhanced Jina AI Search Integration.

This module provides a more robust integration with Jina AI's search API
with improved error handling, result validation, and search strategies.
"""


from typing import Dict, List, Optional, Any, Union, cast, Tuple, Literal
import json
import time
import aiohttp
import asyncio
import contextlib
from urllib.parse import urljoin, quote, urlparse
import random
from datetime import datetime, timezone
import re
import os

from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import InjectedToolArg
from typing_extensions import Annotated
from pydantic import BaseModel, Field
from langchain.tools import BaseTool
from langchain_core.tools import ToolException
from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.base import Checkpoint, CheckpointMetadata

from react_agent.configuration import Configuration
from react_agent.utils.logging import get_logger, log_dict, info_highlight, warning_highlight, error_highlight
from react_agent.utils.validations import is_valid_url
from react_agent.prompts.query import optimize_query, detect_vertical, expand_acronyms
from react_agent.utils.extraction import safe_json_parse
from react_agent.utils.cache import create_checkpoint, load_checkpoint, cache_result, ProcessorCache

# Initialize logger
logger = get_logger(__name__)

# Define search types for specialized search strategies
SearchType = Literal["general", "authoritative", "recent", "comprehensive", "technical"]

# Initialize memory saver for caching
processor_cache = ProcessorCache(thread_id="jina-search")

# Add at module level after imports
_query_cache: Dict[str, Tuple[List[Document], datetime]] = {}

class RetryConfig(BaseModel):
    """Configuration for retry behavior."""
    max_retries: int = Field(default=3, description="Maximum number of retries")
    base_delay: float = Field(default=1.0, description="Base delay between retries in seconds")
    max_delay: float = Field(default=10.0, description="Maximum delay between retries in seconds")
    
    def get_delay(self, attempt: int) -> float:
        """Calculate delay with exponential backoff."""
        return min(self.max_delay, self.base_delay * (2 ** (attempt - 1)))

class SearchParams(BaseModel):
    """Parameters for search operations."""
    query: str = Field(..., description="Search query")
    search_type: SearchType = Field(default="general", description="Type of search to perform")
    max_results: Optional[int] = Field(default=None, description="Maximum number of results to return")
    min_quality_score: Optional[float] = Field(default=0.5, description="Minimum quality score for results")
    recency_days: Optional[int] = Field(default=None, description="Maximum age of results in days")
    domains: Optional[List[str]] = Field(default=None, description="List of domains to search")
    category: Optional[str] = Field(default=None, description="Category to search in")
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for API request."""
        result = {
            "q": self.query,
            "limit": self.max_results or 10,
            "min_score": self.min_quality_score or 0.5
        }
        
        if self.recency_days:
            result["recency_days"] = self.recency_days
            
        if self.domains:
            result["domains"] = ",".join(self.domains)
            
        if self.category:
            result["category"] = self.category
            
        return result

class JinaSearchClient:
    """Enhanced client for Jina AI search with retry and validation."""
    
    def __init__(
        self,
        api_key: str,
        base_url: Optional[str] = None,
        retry_config: Optional[RetryConfig] = None,
    ):
        """Initialize Jina search client.
        
        Args:
            api_key: Jina AI API key
            base_url: Optional base URL for self-hosted instances
            retry_config: Configuration for retry behavior
        """
        self.api_key = api_key
        self.base_url = base_url.rstrip('/') if base_url else "https://s.jina.ai"
        self.retry_config = retry_config or RetryConfig()
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        """Create aiohttp session."""
        self.session = aiohttp.ClientSession(
            headers={
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Close aiohttp session."""
        if self.session:
            await self.session.close()
            self.session = None

    def _get_endpoint(self, endpoint: str) -> str:
        """Get endpoint URL."""
        base = self.base_url.rstrip('/')
        if not endpoint.startswith('/'):
            endpoint = f'/{endpoint}'
        return f"{base}{endpoint}"

    async def _make_request_with_retry(
        self,
        method: str,
        endpoint: str,
        params: Optional[Dict[str, Any]] = None,
        json_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Make HTTP request to Jina API with retry logic and better error handling."""
        if not self.session:
            raise ValueError("Session not initialized")

        last_exception = None
        for attempt in range(1, self.retry_config.max_retries + 1):
            try:
                url = self._get_endpoint(endpoint)
                info_highlight(f"Making request to: {url} with params: {params}")
                
                async with self.session.request(
                    method=method,
                    url=url,
                    params=params,
                    json=json_data
                ) as response:
                    # Check for no results error (422)
                    if response.status == 422:
                        error_text = await response.text()
                        if "No search results available" in error_text:
                            warning_highlight("No search results available for this query")
                            return {"results": []}  # Return empty results rather than raising an error
                    
                    # Handle other errors
                    if response.status != 200:
                        error_text = await response.text()
                        error_highlight(f"Request failed with status {response.status}: {error_text}")
                        raise aiohttp.ClientError(f"Request failed with status {response.status}: {error_text}")
                    
                    return await response.json()
            except Exception as e:
                last_exception = e
                if attempt < self.retry_config.max_retries:
                    delay = self.retry_config.get_delay(attempt)
                    warning_highlight(
                        f"Request failed (attempt {attempt}/{self.retry_config.max_retries}): {str(last_exception)}. Retrying in {delay:.2f}s"
                    )
                    await asyncio.sleep(delay)
                else:
                    error_highlight(
                        f"Request failed after {self.retry_config.max_retries} attempts: {str(last_exception)}"
                    )
                    raise

        # This should never be reached as the last failure should raise
        assert last_exception is not None
        raise last_exception

    async def search(
        self,
        params: SearchParams
    ) -> List[Document]:
        """Search with improved parameters and result validation.
        
        Args:
            params: Search parameters
            
        Returns:
            List of documents from search results
        """
        request_params = params.to_dict()
        info_highlight(f"Executing {params.search_type} search with query: {params.query}")
        
        try:
            # Use the search endpoint
            data = await self._make_request_with_retry(
                method="GET",
                endpoint="/search",  # Use the search endpoint
                params=request_params
            )
            
            results = self._parse_search_results(data)
            info_highlight(f"Retrieved {len(results)} search results")
            
            # If no results, try simplified query
            if not results:
                # Extract main keywords (first 3 words or up to 30 chars)
                simplified_query = " ".join(params.query.split()[:3])
                if len(simplified_query) > 30:
                    simplified_query = simplified_query[:30]
                
                info_highlight(f"No results found, trying simplified query: {simplified_query}")
                request_params["query"] = simplified_query
                
                data = await self._make_request_with_retry(
                    method="GET",
                    endpoint="/search",
                    params=request_params
                )
                
                results = self._parse_search_results(data)
                info_highlight(f"Retrieved {len(results)} results with simplified query")
            
            # Convert results to Documents
            documents = self._convert_to_documents(results)
            
            # Apply quality filtering
            min_score: float = float(params.min_quality_score or 0.5)  # Explicit type conversion
            filtered_docs = self._filter_documents(documents, min_score)
            info_highlight(f"Filtered to {len(filtered_docs)} high-quality results")
            
            return filtered_docs
        except Exception as e:
            error_highlight(f"Search failed: {str(e)}")
            return []

    def _parse_search_results(self, raw_results: Union[str, List[Dict], Dict]) -> List[Dict]:
        """Parse raw results with improved error handling."""
        try:
            # Convert raw_results to string if it's not already
            if isinstance(raw_results, (list, dict)):
                raw_results = json.dumps(raw_results)
            parsed = safe_json_parse(raw_results, "search_results")
            return self._extract_results_list(parsed)
        except Exception as e:
            error_highlight(f"Error parsing search results: {str(e)}")
            return []

    def _extract_results_list(self, data: Union[List[Dict], Dict]) -> List[Dict]:
        """Extract results list from various response formats."""
        try:
            if isinstance(data, list):
                return data
            elif isinstance(data, dict):
                # Try common response formats
                for key in ['results', 'data', 'items', 'hits', 'matches', 'documents', 'response']:
                    if key in data:
                        if isinstance(data[key], list):
                            return data[key]
                        elif isinstance(data[key], dict):
                            # Try to extract from nested structure
                            for nested_key in ['results', 'data', 'items', 'hits', 'matches']:
                                if nested_key in data[key] and isinstance(data[key][nested_key], list):
                                    return data[key][nested_key]

                # If no list found, try to extract single result
                if 'result' in data:
                    return [data['result']]

                # If still no results, try to extract from nested structure
                for value in data.values():
                    if isinstance(value, list):
                        return value
                    elif isinstance(value, dict):
                        if nested_results := self._extract_results_list(value):
                            return nested_results

                # If still no results, return empty list
                return []
            return []
        except Exception as e:
            error_highlight(f"Error extracting results list: {str(e)}")
            return []

    def _extract_content(self, result: Dict) -> Optional[str]:
        """Extract content from result with fallbacks."""
        content_fields = ['snippet', 'content', 'text', 'description', 'summary', 'body', 'raw']
        for field in content_fields:
            if content := result.get(field):
                return content.strip().strip('```json').strip('```')
        return None

    def _build_metadata(self, result: Dict, content: str) -> Dict:
        """Build metadata dictionary from result."""
        field_mapping = {
            'url': ['url', 'link', 'href', 'source_url', 'web_url'],
            'title': ['title', 'name', 'heading', 'subject', 'headline'],
            'source': ['source', 'domain', 'site', 'provider', 'publisher'],
            'published_date': ['published_date', 'date', 'timestamp', 'published', 'created_at', 'publication_date']
        }
        
        metadata = {
            'quality_score': self._calculate_quality_score(result, content),
            'extraction_status': 'success',
            'extraction_timestamp': datetime.now().isoformat(),
            'original_result': result
        }
        
        for field_type, fields in field_mapping.items():
            for field in fields:
                if value := result.get(field):
                    metadata[field_type] = value
                    break
        
        if url := metadata.get('url'):
            try:
                metadata['domain'] = urlparse(url).netloc
            except Exception:
                metadata['domain'] = ""
                
        return metadata

    def _convert_to_documents(self, results: List[Dict]) -> List[Document]:
        """Convert search results to Document objects with improved metadata."""
        documents = []
        for idx, result in enumerate(results, 1):
            if not isinstance(result, dict):
                continue
                
            if not (content := self._extract_content(result)):
                warning_highlight(f"No content found for result {idx}")
                continue
                
            try:
                metadata = self._build_metadata(result, content)
                documents.append(Document(page_content=content, metadata=metadata))
                info_highlight(f"Successfully converted result {idx} to Document")
            except Exception as e:
                warning_highlight(f"Error converting result {idx} to Document: {str(e)}")
                continue
                
        return documents

    @cache_result(ttl=3600)
    def _calculate_quality_score(self, result: Dict[str, Any], content: str) -> float:
        """Calculate quality score for a search result."""
        score = 0.5  # Base score

        # Add points for authoritative domains
        authoritative_domains = [
            '.gov', '.edu', '.org', 'wikipedia.org', 
            'research', 'journal', 'university', 'association'
        ]
        url = result.get('url', '')
        if any(domain in url.lower() for domain in authoritative_domains):
            score += 0.2

        # Add points for content length (substantive content)
        if len(content) > 500:
            score += 0.1

        # Add points for having title/publication date
        if result.get('title'):
            score += 0.05
        if result.get('published_date') or result.get('date'):
            score += 0.05

        if date_field := result.get('published_date', result.get('date', '')):
            with contextlib.suppress(Exception):
                # Try to parse date
                from dateutil import parser
                published_date = parser.parse(date_field)
                current_date = datetime.now(timezone.utc)
                days_old = (current_date - published_date).days

                # Fresher content gets higher score
                if days_old < 30:  # Last month
                    score += 0.1
                elif days_old < 180:  # Last 6 months
                    score += 0.05
        return min(1.0, score)  # Cap at 1.0

    @cache_result(ttl=3600)
    def _filter_documents(self, documents: List[Document], min_score: float) -> List[Document]:
        """Filter documents based on quality score."""
        return [
            doc for doc in documents 
            if doc.metadata.get('quality_score', 0) >= min_score
        ]

class JinaSearchTool(BaseTool):
    """Enhanced Jina AI search integration with caching and parallel processing."""
    
    name: str = "jina_search"
    description: str = "Search for information using Jina AI's search engine with enhanced caching and parallel processing"
    
    config: Configuration = Field(default_factory=Configuration)
    
    def __init__(self, config: Optional[Configuration] = None):
        """Initialize the Jina search tool.
        
        Args:
            config: Optional configuration for the search tool
        """
        super().__init__()
        if config:
            self.config = config
        if not self.config.jina_api_key:
            raise ValueError("Jina API key is required")
            
    async def _arun(
        self,
        query: str,
        search_type: Optional[SearchType] = None,
        max_results: Optional[int] = None,
        min_quality_score: Optional[float] = None,
        recency_days: Optional[int] = None,
        domains: Optional[List[str]] = None,
        category: Optional[str] = None
    ) -> List[Document]:
        """Execute search with caching and parallel processing."""
        config = RunnableConfig(configurable={"jina_api_key": self.config.jina_api_key})
        return await search(
            query=query,
            search_type=search_type,
            max_results=max_results,
            min_quality=min_quality_score,
            recency_days=recency_days,
            domains=domains,
            category=category,
            config=config
        )

async def _execute_search_strategy(
    client: JinaSearchClient,
    params: SearchParams,
    category: Optional[str] = None
) -> List[List[Document]]:
    """Execute search with optional category-specific search."""
    search_tasks = [client.search(params)]

    if not category:
        if vertical := detect_vertical(params.query):
            category_params = SearchParams(**params.model_dump())
            category_params.category = vertical
            search_tasks.append(client.search(category_params))

    return await asyncio.gather(*search_tasks)

@cache_result(ttl=3600)
def _merge_and_filter_results(
    results_list: List[List[Document]],
    min_quality: float
) -> List[Document]:
    """Merge, deduplicate and filter search results."""
    seen_urls = set()
    all_results = []
    
    for results in results_list:
        for doc in results:
            url = doc.metadata.get("url", "")
            if url not in seen_urls:
                seen_urls.add(url)
                all_results.append(doc)
    
    all_results.sort(key=lambda x: x.metadata.get("quality_score", 0), reverse=True)
    return [doc for doc in all_results if doc.metadata.get("quality_score", 0) >= min_quality]

async def _load_cached_results(cache_key: str) -> Optional[List[Document]]:
    """Load and validate cached search results."""
    try:
        if cached := load_checkpoint(cache_key):
            if not isinstance(cached, dict) or "results" not in cached or "timestamp" not in cached:
                return None
                
            if (datetime.now() - datetime.fromisoformat(cached["timestamp"])).total_seconds() >= 86400:
                return None
                
            results = cached["results"]
            if not isinstance(results, list) or not all(isinstance(doc, Document) for doc in results):
                return None
                
            info_highlight(f"Retrieved {len(results)} results from cache")
            return results
    except Exception as e:
        warning_highlight(f"Error loading from cache: {str(e)}")
    return None

async def _perform_search(
    configuration: Configuration,
    params: SearchParams,
    cache_key: str
) -> List[Document]:
    """Execute search and cache results."""
    async with JinaSearchClient(
        api_key=configuration.jina_api_key or "",
        base_url=configuration.jina_url,
        retry_config=RetryConfig()
    ) as client:
        results_list = await _execute_search_strategy(client, params, params.category)
        all_results = _merge_and_filter_results(results_list, params.min_quality_score or 0.5)
        
        try:
            create_checkpoint(
                cache_key,
                {
                    "results": all_results,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "params": params.to_dict()
                },
                ttl=86400
            )
            info_highlight(f"Cached {len(all_results)} results")
        except Exception as e:
            warning_highlight(f"Error caching results: {str(e)}")
        
        return all_results

async def search(
    query: str,
    search_type: Optional[SearchType] = None,
    max_results: Optional[int] = None,
    min_quality: Optional[float] = None,
    recency_days: Optional[int] = None,
    domains: Optional[List[str]] = None,
    category: Optional[str] = None,
    *,
    config: Annotated[RunnableConfig, InjectedToolArg]
) -> List[Document]:
    """Enhanced search with multiple strategies, quality filters, and improved caching."""
    configuration = Configuration.from_runnable_config(config)
    if not configuration.jina_api_key:
        error_highlight("Jina API key is required")
        return []

    os.environ["JINA_API_KEY"] = configuration.jina_api_key
    if configuration.jina_url:
        os.environ["JINA_URL"] = configuration.jina_url

    params = SearchParams(
        query=query,
        search_type=search_type or "general",
        max_results=max_results or configuration.max_search_results,
        min_quality_score=min_quality or 0.5,
        recency_days=recency_days,
        domains=domains or (['.edu', '.gov', '.org'] if search_type in ["authoritative", None] else None),
        category=category
    )

    cache_key = f"jina_search_{params.query}_{params.search_type}_{params.max_results}"
    
    if cached_results := await _load_cached_results(cache_key):
        return cached_results

    try:
        return await _perform_search(configuration, params, cache_key)
    except Exception as e:
        error_highlight(f"Error in Jina search: {str(e)}")
        return []

# Export available tools
TOOLS = [search]
</file>

<file path="src/react_agent/utils/__init__.py">
from react_agent.utils.validations import is_valid_url
from react_agent.utils.logging import (
    get_logger,
    log_dict,
    info_highlight,
    warning_highlight,
    error_highlight,
    log_step
)

__all__ = [
    "is_valid_url",
    "get_logger",
    "log_dict",
    "info_highlight",
    "warning_highlight",
    "error_highlight",
    "log_step"
]
</file>

<file path="src/react_agent/utils/cache.py">
"""Type-safe caching and checkpointing utilities.

This module provides a unified interface for caching and checkpointing with LangGraph,
ensuring type safety and consistent behavior across the application.
"""

from typing import Any, Callable, Dict, Generic, Optional, TypeVar, Union, cast
from dataclasses import dataclass
from datetime import datetime, timezone
import hashlib
import json
import pickle
from pathlib import Path
import logging
from functools import wraps

from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.base import Checkpoint, CheckpointMetadata
from langgraph.checkpoint.memory import MemorySaver

from react_agent.utils.logging import get_logger

# Initialize logger
logger = get_logger(__name__)

# Type variables for generic type safety
T = TypeVar('T')
K = TypeVar('K')

@dataclass
class CacheEntry(Generic[T]):
    """Type-safe cache entry with metadata."""
    data: T
    timestamp: str
    ttl: int = 3600  # Default 1 hour TTL

class ProcessorCache:
    """Type-safe processor cache using LangGraph checkpointing."""
    
    def __init__(self, thread_id: str = "default-processor") -> None:
        """Initialize the cache with a specific thread ID.
        
        Args:
            thread_id: Identifier for this cache instance
        """
        self.memory_saver = MemorySaver()
        self.thread_id = thread_id

    def _get_config(self, checkpoint_id: str) -> RunnableConfig:
        """Create a RunnableConfig for checkpoint operations."""
        return RunnableConfig(configurable={
            "thread_id": self.thread_id,
            "checkpoint_id": checkpoint_id
        })

    def _create_checkpoint(
        self,
        key: str,
        data: Dict[str, Any],
        metadata: Optional[Dict[str, Any]] = None
    ) -> Checkpoint:
        """Create a checkpoint with proper typing."""
        return Checkpoint(
            id=key,
            ts=datetime.now(timezone.utc).isoformat(),
            v=1,
            channel_values=data,
            channel_versions={},
            versions_seen={},
            pending_sends=[]
        )

    def get(self, key: str) -> Optional[Dict[str, Any]]:
        """Get data from cache with type safety."""
        try:
            checkpoint = self.memory_saver.get(self._get_config(key))
            if checkpoint and isinstance(checkpoint, dict):
                return cast(Dict[str, Any], checkpoint.get("channel_values"))
            return None
        except Exception as e:
            logger.error(f"Error retrieving from cache: {str(e)}")
            return None

    def put(
        self,
        key: str,
        data: Dict[str, Any],
        ttl: int = 3600,
        metadata: Optional[Dict[str, Any]] = None
    ) -> None:
        """Store data in cache with type safety."""
        checkpoint_metadata = CheckpointMetadata(
            source="input",
            step=-1,
            writes={},
            parents={}
        )

        if metadata:
            checkpoint_metadata["writes"] = metadata

        checkpoint = self._create_checkpoint(key, data)
        
        self.memory_saver.put(
            self._get_config(key),
            checkpoint,
            metadata=checkpoint_metadata,
            new_versions={}
        )

    def cache_result(self, ttl: int = 3600) -> Callable[[Callable[..., T]], Callable[..., T]]:
        """Decorator for caching function results with type preservation."""
        def decorator(func: Callable[..., T]) -> Callable[..., T]:
            @wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> T:
                # Generate cache key
                cache_data = {'args': args, 'kwargs': kwargs, 'func': func.__name__}
                cache_str = json.dumps(cache_data, sort_keys=True)
                cache_key = hashlib.sha256(cache_str.encode()).hexdigest()

                # Check cache
                if cached := self.get(cache_key):
                    if cached.get("data") is not None:
                        timestamp = datetime.fromisoformat(cached.get("timestamp", ""))
                        if (datetime.now(timezone.utc) - timestamp).total_seconds() < cached.get("ttl", ttl):
                            return cast(T, cached["data"])

                # Execute function
                result = func(*args, **kwargs)

                # Store in cache
                self.put(
                    cache_key,
                    {
                        "data": result,
                        "timestamp": datetime.now(timezone.utc).isoformat(),
                        "ttl": ttl
                    }
                )

                return result
            return wrapper
        return decorator

# Default processor instance
default_processor = ProcessorCache()

# Type-safe utility functions
def create_checkpoint(key: str, data: Dict[str, Any], ttl: int = 3600) -> None:
    """Create a checkpoint with the default processor."""
    default_processor.put(key, data, ttl=ttl)

def load_checkpoint(key: str) -> Optional[Dict[str, Any]]:
    """Load a checkpoint with the default processor."""
    return default_processor.get(key)

def cache_result(ttl: int = 3600) -> Callable[[Callable[..., T]], Callable[..., T]]:
    """Decorator for caching results with the default processor."""
    return default_processor.cache_result(ttl=ttl)
</file>

<file path="src/react_agent/utils/content.py">
"""Content processing utilities for the research agent.

This module provides utilities for processing and validating content,
including chunking, preprocessing, and content type detection.
"""

from typing import List, Dict, Any, Optional, Union, Tuple, TypedDict, Set, Literal
import re
from urllib.parse import urlparse, unquote
import logging
from datetime import datetime, timezone
import json
import contextlib
import urllib.parse
from react_agent.utils.logging import (
    get_logger,
    info_highlight, 
    warning_highlight,
    error_highlight,
    log_progress,
    log_performance_metrics
)
from react_agent.utils.extraction import safe_json_parse
from react_agent.utils.defaults import ChunkConfig, get_default_extraction_result, get_category_merge_mapping
from langgraph.graph import StateGraph
from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.base import Checkpoint, CheckpointMetadata
from react_agent.utils.cache import create_checkpoint, load_checkpoint, cache_result

# Initialize logger
logger = get_logger(__name__)

class ContentState(TypedDict):
    """Type definition for content state in the graph."""
    content: str
    url: str
    content_type: str
    chunks: List[str]
    metadata: Dict[str, Any]
    timestamp: str

def create_checkpoint(key: str, data: Dict[str, Any], ttl: int = 3600) -> None:
    """Create a checkpoint using the caching system.
    
    Args:
        key: Unique identifier for the checkpoint
        data: Data to store in the checkpoint
        ttl: Time to live in seconds (default 1 hour)
        
    Examples:
        >>> data = {
        ...     "content": "Sample content",
        ...     "url": "example.com",
        ...     "content_type": "text"
        ... }
        >>> create_checkpoint("test_key", data)
        
        >>> create_checkpoint("temp_key", data, ttl=1800)  # 30 minute TTL
    """
    # Use the imported create_checkpoint function
    create_checkpoint(key, data, ttl)

def load_checkpoint(key: str) -> Optional[Dict[str, Any]]:
    """Load a checkpoint using the caching system.
    
    Args:
        key: Unique identifier for the checkpoint
        
    Returns:
        Channel values if found, None otherwise
        
    Examples:
        >>> # Assuming checkpoint exists
        >>> load_checkpoint("test_key")
        {'content': 'Sample content', 'url': 'example.com', 'content_type': 'text'}
        
        >>> # Non-existent checkpoint
        >>> load_checkpoint("missing_key")
        None
    """
    # Use the imported load_checkpoint function
    return load_checkpoint(key)

# Constants
DEFAULT_CHUNK_SIZE: int = 40000
DEFAULT_OVERLAP: int = 5000
MAX_CONTENT_LENGTH: int = 100000
TOKEN_CHAR_RATIO: float = 4.0

# Problematic content patterns
PROBLEMATIC_PATTERNS: List[str] = [
    r'\.pdf(\?|$)',  # Modified to catch PDF URLs with query params
    r'\.docx?$',
    r'\.xlsx?$',
    r'\.ppt$',
    r'\.zip$',
    r'\.rar$',
    r'\.exe$',
    r'\.dmg$',
    r'\.iso$',
    r'\.tar$',
    r'\.gz$'
]

# Known problematic sites
PROBLEMATIC_SITES: List[str] = [
    'iaeme.com',
    'scribd.com',
    'slideshare.net',
    'academia.edu'
]

@cache_result(ttl=3600)
def chunk_text(
    text: str,
    chunk_size: Optional[int] = None,
    overlap: Optional[int] = None,
    use_large_chunks: bool = False,
    min_chunk_size: int = 100
) -> List[str]:
    """Split text into overlapping chunks with enhanced robustness and caching.
    
    Args:
        text: Text to chunk
        chunk_size: Size of each chunk (defaults to ChunkConfig values)
        overlap: Overlap between chunks (defaults to ChunkConfig values)
        use_large_chunks: Whether to use large chunk sizes
        min_chunk_size: Minimum size for a chunk to avoid too small chunks
        
    Returns:
        List of text chunks
        
    Examples:
        >>> text = "This is a sample text that needs to be chunked into smaller pieces."
        >>> chunk_text(text, chunk_size=20, overlap=5)
        ['This is a sample', 'sample text that', 'that needs to be', 'be chunked into', 'into smaller pieces.']
        
        >>> # Using large chunks
        >>> chunk_text(text, use_large_chunks=True)
        ['This is a sample text that needs to be chunked into smaller pieces.']
        
        >>> # Empty input
        >>> chunk_text("")
        []
    """
    if not text or text.isspace():
        warning_highlight("Empty or whitespace-only text provided")
        return []

    # Set chunk parameters
    chunk_size = max(min_chunk_size, chunk_size or (
        ChunkConfig.LARGE_CHUNK_SIZE if use_large_chunks 
        else ChunkConfig.DEFAULT_CHUNK_SIZE
    ))
    overlap = min(chunk_size - 1, max(0, overlap or (
        ChunkConfig.LARGE_OVERLAP if use_large_chunks 
        else ChunkConfig.DEFAULT_OVERLAP
    )))

    chunks: List[str] = []
    start = 0
    text_length = len(text)

    while start < text_length:
        # Calculate chunk boundaries
        end = min(start + chunk_size, text_length)

        # Find natural break point
        if end < text_length:
            end = text.rfind(' ', start + min_chunk_size, end) or end

        if chunk := text[start:end].strip():
            if chunks and len(chunk) < min_chunk_size:
                chunks[-1] = f"{chunks[-1]} {chunk}"
            else:
                chunks.append(chunk)

        start = end - overlap

    info_highlight(f"Created {len(chunks)} chunks", category="chunking")
    return chunks

def detect_html(content: str) -> Optional[str]:
    """Detect if content is HTML.
    
    Args:
        content: String content to analyze
        
    Returns:
        'html' if HTML is detected, None otherwise
        
    Examples:
        >>> detect_html("<html><body>Hello</body></html>")
        'html'
        
        >>> detect_html("<!doctype html><div>Content</div>")
        'html'
        
        >>> detect_html("Plain text content")
        None
        
        >>> detect_html("")
        None
    """
    if not content:
        return None
    content_lower = content.strip().lower()
    if content_lower.startswith('<!doctype html') or content_lower.startswith('<html'):
        return 'html'
    if '<body' in content_lower and '</body>' in content_lower:
        return 'html'
    if '<div' in content_lower and '</div>' in content_lower:
        return 'html'
    return None

def detect_json(content: str) -> Optional[str]:
    """Detect if content is JSON.
    
    Args:
        content: String content to analyze
        
    Returns:
        'json' if valid JSON is detected, None otherwise
        
    Examples:
        >>> detect_json('{"key": "value"}')
        'json'
        
        >>> detect_json('[1, 2, 3]')
        'json'
        
        >>> detect_json('Invalid content')
        None
        
        >>> detect_json('')
        None
    """
    if not content:
        return None
    content = content.strip()
    if (content.startswith('{') and content.endswith('}')) or (content.startswith('[') and content.endswith(']')):
        with contextlib.suppress(json.JSONDecodeError):
            json.loads(content)
            return 'json'
    return None

def detect_from_url_extension(url: str) -> Optional[str]:
    """Detect content type from URL file extension.
    
    Args:
        url: URL to analyze
        
    Returns:
        Content type based on file extension, or None if not detected
        
    Examples:
        >>> detect_from_url_extension('document.pdf')
        'pdf'
        
        >>> detect_from_url_extension('page.html')
        'html'
        
        >>> detect_from_url_extension('data.json')
        'json'
        
        >>> detect_from_url_extension('noextension')
        None
    """
    if not url:
        return None
        
    extensions = {
        '.pdf': 'pdf', '.doc': 'doc', '.docx': 'doc', '.xls': 'excel', '.xlsx': 'excel',
        '.ppt': 'presentation', '.pptx': 'presentation', '.txt': 'text', '.md': 'text',
        '.rst': 'text', '.html': 'html', '.htm': 'html', '.json': 'json', '.xml': 'xml',
        '.csv': 'data'
    }
    
    try:
        ext = f".{url.lower().split('.')[-1]}"
        return extensions.get(ext)
    except IndexError:
        return None

def detect_from_url_path(url: str) -> Optional[str]:
    """Detect content type from URL path patterns."""
    if not url:
        return None
        
    html_path_patterns = (
        '/wiki/', '/articles/', '/blog/', '/news/',
        '/docs/', '/help/', '/support/', '/pages/',
        '/product/', '/service/', '/consumers/',
        '/detail/', '/view/', '/content/'
    )
    return 'html' if any(pattern in url.lower() for pattern in html_path_patterns) else None

def detect_from_content_heuristics(content: str) -> Optional[str]:
    """Detect content type from content patterns."""
    if not content or len(content) < 50:
        return None

    content = content.strip()
    if content.startswith('<?xml') or (content.startswith('<') and '>' in content):
        return 'xml'
    if '{' in content and '}' in content and '"' in content and ':' in content:
        return 'data'
    return 'text' if '\n\n' in content and len(content) > 200 else None

def detect_from_url_domain(url: str) -> Optional[str]:
    """Detect content type from URL domain."""
    if not url:
        return None
        
    common_web_domains = ('.gov', '.org', '.edu', '.com', '.net', '.io')
    parsed_url = urlparse(url)
    domain = parsed_url.netloc.lower()
    if any(domain.endswith(d) for d in common_web_domains) and (parsed_url.path and '.' not in parsed_url.path.split('/')[-1]):
        return 'html'
    return None

def detect_content_type(url: str, content: str) -> str:
    """Detect content type from URL and content using a modular approach.
    
    Args:
        url: URL of the content
        content: Content to analyze
        
    Returns:
        Detected content type string
        
    Examples:
        >>> detect_content_type('page.html', '<html><body>Content</body></html>')
        'html'
        
        >>> detect_content_type('data.json', '{"key": "value"}')
        'json'
        
        >>> detect_content_type('article', 'Plain text content\n\nMore content')
        'text'
        
        >>> detect_content_type('', '')
        'unknown'
    """
    info_highlight(f"Detecting content type for URL: {url}", category="content_type")
    
    detector_functions = [
        (detect_html, content),
        (detect_json, content),
        (detect_from_url_extension, url),
        (detect_from_url_path, url),
        (detect_from_content_heuristics, content),
        (detect_from_url_domain, url)
    ]

    for detector, arg in detector_functions:
        if result := detector(arg):
            info_highlight(f"Detected content type: {result}", category="content_type")
            return result

    # Fallback detection
    result = fallback_detection(url, content)
    info_highlight(f"Using fallback detection, type: {result}", category="content_type")
    return result

def fallback_detection(url: str, content: str) -> str:
    """Fallback detection logic."""
    if content and content.strip():
        return 'text'
    return 'html' if url and url.startswith(('http://', 'https://')) else 'unknown'

def preprocess_content(content: str, url: str) -> str:
    """Clean and preprocess content before sending to model with improved performance.
    
    Args:
        content: Content to preprocess
        url: URL of the content
        
    Returns:
        Preprocessed content string
        
    Examples:
        >>> content = "Copyright © 2024 Example Corp. All rights reserved.\nActual content here"
        >>> preprocess_content(content, "example.com")
        'Actual content here'
        
        >>> content = "Please enable JavaScript to continue.\nImportant content"
        >>> preprocess_content(content, "example.com")
        'Important content'
        
        >>> preprocess_content("", "example.com")
        ''
    """
    if not content:
        warning_highlight("Empty content provided", category="preprocessing")
        return ""

    info_highlight(f"Preprocessing content from {url}", category="preprocessing")
    info_highlight(f"Initial content length: {len(content)}", category="preprocessing")

    # Generate cache key
    cache_key = f"preprocess_content_{hash(f'{content}_{url}')}"
    
    # Check cache with TTL
    if cached_state := load_checkpoint(cache_key):
        cached_result = cached_state.get("result")
        if isinstance(cached_result, dict) and cached_result:
            timestamp = datetime.fromisoformat(cached_state.get("timestamp", ""))
            if (datetime.now() - timestamp).total_seconds() < 3600:  # 1 hour TTL
                return cached_result.get("content", "")

    # Compile regex patterns once
    boilerplate_patterns = [
        (re.compile(r'Copyright © \d{4}.*?reserved\.', re.IGNORECASE | re.DOTALL), ''),
        (re.compile(r'Terms of Service.*?Privacy Policy', re.IGNORECASE | re.DOTALL), ''),
        (re.compile(r'Please enable JavaScript.*?continue', re.IGNORECASE | re.DOTALL), '')
    ]
    
    # Apply boilerplate removal patterns
    for pattern, replacement in boilerplate_patterns:
        content = pattern.sub(replacement, content)

    # Remove redundant whitespace efficiently
    content = ' '.join(content.split())

    # Site-specific cleaning with compiled pattern
    domain = urlparse(url).netloc.lower()
    if 'iaeme.com' in domain:
        iaeme_pattern = re.compile(r'International Journal.*?Indexing', re.IGNORECASE | re.DOTALL)
        content = iaeme_pattern.sub('', content)

    # Truncate if too long
    if len(content) > MAX_CONTENT_LENGTH:
        warning_highlight(
            f"Content exceeds {MAX_CONTENT_LENGTH} characters, truncating",
            category="preprocessing"
        )
        content = f"{content[:MAX_CONTENT_LENGTH]}..."

    # Save to cache with TTL
    create_checkpoint(
        cache_key,
        {
            "result": {"content": content},
            "timestamp": datetime.now(timezone.utc).isoformat()
        },
        ttl=3600  # 1 hour TTL
    )
    
    info_highlight(f"Final content length: {len(content)}", category="preprocessing")
    return content

def estimate_tokens(text: str) -> int:
    """Estimate number of tokens in text using character ratio.
    
    Args:
        text: Text to estimate tokens for
        
    Returns:
        Estimated number of tokens
        
    Examples:
        >>> # Assuming TOKEN_CHAR_RATIO = 4.0
        >>> estimate_tokens("This is a test string")
        5  # ~20 characters / 4.0 = 5 tokens
        
        >>> estimate_tokens("")
        0
        
        >>> estimate_tokens("Short")
        1  # ~5 characters / 4.0 = 1 token
    """
    return int(len(text) / TOKEN_CHAR_RATIO) if text else 0

def should_skip_content(url: str) -> bool:
    """Check if content should be skipped based on URL patterns.
    
    Args:
        url: URL to check
        
    Returns:
        True if content should be skipped, False otherwise
        
    Examples:
        >>> should_skip_content("http://example.com/document.pdf")
        True
        
        >>> should_skip_content("http://example.com/page.html")
        False
        
        >>> should_skip_content("http://scribd.com/document")
        True  # Problematic site
        
        >>> should_skip_content("")
        True
    """
    try:
        decoded_url = urllib.parse.unquote(url).lower()
    except Exception as e:
        error_highlight(f"Error decoding URL: {str(e)}", category="validation")
        decoded_url = url.lower()

    # Enhanced PDF detection
    if any(p in decoded_url for p in ('.pdf', '%2Fpdf', '%3Fpdf')):
        info_highlight(f"Skipping PDF content: {url}", category="validation")
        return True

    # Add MIME-type pattern detection
    mime_patterns = [
        r'application/pdf',
        r'application/\w+?pdf',
        r'content-type:.*pdf'
    ]
    if any(re.match(p, decoded_url) for p in mime_patterns):
        info_highlight(f"Skipping PDF MIME-type pattern: {url}", category="validation")
        return True

    if not url:
        return True

    url_lower = url.lower()

    # Check for problematic file types
    for pattern in PROBLEMATIC_PATTERNS:
        if re.search(pattern, url_lower):
            info_highlight(f"Skipping content with pattern {pattern}: {url}", category="validation")
            return True

    # Check for problematic sites
    domain = urlparse(url).netloc.lower()
    for site in PROBLEMATIC_SITES:
        if site in domain:
            info_highlight(
                f"Skipping content from problematic site {site}: {url}",
                category="validation"
            )
            return True

    return False

def _merge_field(merged: Dict[str, Any], result: Dict[str, Any], field: str, operation: str, seen_items: set) -> None:
    """Helper function to merge a single field based on operation type.
    
    Args:
        merged: Dictionary containing the merged results
        result: Dictionary containing the current result to merge
        field: Field name to merge
        operation: Type of merge operation ('extend', 'update', 'max', 'min', 'avg')
        seen_items: Set of already seen items to prevent duplicates
        
    Examples:
        # Extend operation
        >>> merged = {'items': [1, 2]}
        >>> result = {'items': [3, 4]}
        >>> seen_items = set()
        >>> _merge_field(merged, result, 'items', 'extend', seen_items)
        >>> merged
        {'items': [1, 2, 3, 4]}

        # Update operation
        >>> merged = {'counts': {'a': 1}}
        >>> result = {'counts': {'b': 2}}
        >>> _merge_field(merged, result, 'counts', 'update', seen_items)
        >>> merged
        {'counts': {'a': 1, 'b': 2}}

        # Max operation
        >>> merged = {'score': 5}
        >>> result = {'score': 8}
        >>> _merge_field(merged, result, 'score', 'max', seen_items)
        >>> merged
        {'score': 8}
    """
    if field not in result:
        return

    value = result[field]
    if operation == "extend":
        merged[field] = merged.get(field, [])
        item_key = json.dumps(value, sort_keys=True)
        if item_key not in seen_items:
            merged[field].append(value)
            seen_items.add(item_key)
    elif operation == "update":
        merged[field] = merged.get(field, {})
        merged[field].update(value)
    elif operation in {"max", "min"}:
        if field not in merged or (operation == "max" and value > merged[field]) or (operation == "min" and value < merged[field]):
            merged[field] = value
    elif operation == "avg":
        merged[field] = merged.get(field, [])
        merged[field].append(value)

def merge_chunk_results(results: List[Dict[str, Any]], category: str, merge_strategy: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
    """Merge results from multiple chunks into a single result based on merge strategy.
    
    Args:
        results: List of dictionaries containing results from each chunk
        category: Category of the extraction (e.g., 'research', 'summary')
        merge_strategy: Optional dictionary mapping fields to merge operations
                       ('extend', 'update', 'max', 'min', 'avg')
        
    Returns:
        Dictionary containing the merged results
        
    Examples:
        >>> results = [
        ...     {'findings': ['finding1'], 'score': 5},
        ...     {'findings': ['finding2'], 'score': 8}
        ... ]
        >>> merge_strategy = {'findings': 'extend', 'score': 'max'}
        >>> merge_chunk_results(results, 'research', merge_strategy)
        {
            'findings': ['finding1', 'finding2'],
            'score': 8
        }

        # With average operation
        >>> results = [
        ...     {'rating': 4.0},
        ...     {'rating': 6.0}
        ... ]
        >>> merge_strategy = {'rating': 'avg'}
        >>> merge_chunk_results(results, 'review', merge_strategy)
        {
            'rating': 5.0
        }
    """
    if not results:
        warning_highlight("No results to merge", category=category)
        return get_default_extraction_result(category)

    info_highlight(f"Merging {len(results)} chunk results", category=category)
    merged = get_default_extraction_result(category)
    merge_mappings = merge_strategy or get_category_merge_mapping(category)
    seen_items = set()

    for result in results:
        if "content" in result:
            try:
                result = safe_json_parse(result["content"], category)
            except Exception as e:
                error_highlight(f"Error parsing content: {str(e)}", category=category)
                continue
                
        for field, operation in merge_mappings.items():
            _merge_field(merged, result, field, operation, seen_items)
    
    # Calculate averages
    for field, operation in merge_mappings.items():
        if operation == "avg" and field in merged:
            merged[field] = sum(merged[field]) / len(results)

    info_highlight(f"Merged {len(results)} results successfully", category=category)
    return merged

def validate_content(content: str) -> bool:
    """Validate content before processing.
    
    Args:
        content: Content string to validate
        
    Returns:
        True if content is valid (non-empty string with minimum length),
        False otherwise
        
    Examples:
        >>> validate_content("This is valid content")
        True
        
        >>> validate_content("")  # Empty string
        False
        
        >>> validate_content("Hi")  # Too short
        False
        
        >>> validate_content(None)  # Invalid type
        False
    """
    if not content or not isinstance(content, str):
        warning_highlight("Invalid content type or empty content", category="validation")
        return False
        
    if len(content) < 10:  # Minimum content length
        warning_highlight(
            f"Content too short: {len(content)} characters",
            category="validation"
        )
        return False
        
    return True

__all__ = [
    "chunk_text",
    "preprocess_content",
    "estimate_tokens",
    "should_skip_content",
    "merge_chunk_results",
    "validate_content",
    "detect_content_type"
]
</file>

<file path="src/react_agent/utils/defaults.py">
"""Default values and configurations for the research agent.

This module consolidates all default values, configurations, and common structures
used across the research agent to maintain consistency and reduce duplication.
"""

from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass

# Default chunking configurations
@dataclass
class ChunkConfig:
    """Configuration for text chunking operations."""
    DEFAULT_CHUNK_SIZE: int = 4000
    DEFAULT_OVERLAP: int = 500
    LARGE_CHUNK_SIZE: int = 40000
    LARGE_OVERLAP: int = 5000

# Default extraction result structure
DEFAULT_EXTRACTION_RESULTS = {
    "market_dynamics": {
        "extracted_facts": [],
        "market_metrics": {
            "market_size": None,
            "growth_rate": None,
            "forecast_period": None
        },
        "relevance_score": 0.0
    },
    "provider_landscape": {
        "extracted_vendors": [],
        "vendor_relationships": [],
        "relevance_score": 0.0
    },
    "technical_requirements": {
        "extracted_requirements": [],
        "standards": [],
        "relevance_score": 0.0
    },
    "regulatory_landscape": {
        "extracted_regulations": [],
        "compliance_requirements": [],
        "relevance_score": 0.0
    },
    "cost_considerations": {
        "extracted_costs": [],
        "pricing_models": [],
        "relevance_score": 0.0
    },
    "best_practices": {
        "extracted_practices": [],
        "methodologies": [],
        "relevance_score": 0.0
    },
    "implementation_factors": {
        "extracted_factors": [],
        "challenges": [],
        "relevance_score": 0.0
    }
}

# Category-specific merge mappings
CATEGORY_MERGE_MAPPINGS = {
    "market_dynamics": {
        "extracted_facts": "extend",
        "market_metrics": "update"
    },
    "provider_landscape": {
        "extracted_vendors": "extend",
        "vendor_relationships": "extend"
    },
    "technical_requirements": {
        "extracted_requirements": "extend",
        "standards": "extend"
    },
    "regulatory_landscape": {
        "extracted_regulations": "extend",
        "compliance_requirements": "extend"
    },
    "cost_considerations": {
        "extracted_costs": "extend",
        "pricing_models": "extend"
    },
    "best_practices": {
        "extracted_practices": "extend",
        "methodologies": "extend"
    },
    "implementation_factors": {
        "extracted_factors": "extend",
        "challenges": "extend"
    }
}

def get_default_extraction_result(category: str) -> Dict[str, Any]:
    """Get a default empty extraction result when parsing fails.
    
    Args:
        category: Research category
        
    Returns:
        Default empty result dictionary
    """
    return DEFAULT_EXTRACTION_RESULTS.get(category, {"extracted_facts": [], "relevance_score": 0.0})

def get_category_merge_mapping(category: str) -> Dict[str, str]:
    """Get the merge mapping for a specific category.
    
    Args:
        category: Research category
        
    Returns:
        Dictionary mapping field names to merge operations
    """
    return CATEGORY_MERGE_MAPPINGS.get(category, {})

# Export all defaults
__all__ = [
    "ChunkConfig",
    "DEFAULT_EXTRACTION_RESULTS",
    "CATEGORY_MERGE_MAPPINGS",
    "get_default_extraction_result",
    "get_category_merge_mapping"
]
</file>

<file path="src/react_agent/utils/extraction.py">
"""Enhanced extraction module for research categories with statistics focus.

This module improves the extraction of facts and statistics from search results,
with a particular emphasis on numerical data, trends, and statistical information.

Examples:
    Input text example:
        >>> text = '''
        According to a recent survey by TechCorp, 75% of enterprises adopted cloud 
        computing in 2023, up from 60% in 2022. The global cloud market reached 
        $483.3 billion in revenue, with AWS maintaining a 32% market share. 
        A separate study by MarketWatch revealed that cybersecurity spending 
        increased by 15% year-over-year.
        '''

    Extracting citations:
        >>> citations = extract_citations(text)
        >>> citations
        [
            {
                "source": "TechCorp",
                "context": "According to a recent survey by TechCorp, 75% of enterprises"
            },
            {
                "source": "MarketWatch",
                "context": "A separate study by MarketWatch revealed that cybersecurity"
            }
        ]

    Extracting statistics:
        >>> stats = extract_statistics(text)
        >>> stats
        [
            {
                "text": "75% of enterprises adopted cloud computing in 2023",
                "type": "percentage",
                "citations": [{"source": "TechCorp", "context": "...survey by TechCorp, 75% of enterprises..."}],
                "quality_score": 0.95,
                "year_mentioned": 2023
            },
            {
                "text": "global cloud market reached $483.3 billion in revenue",
                "type": "financial",
                "citations": [],
                "quality_score": 0.85,
                "year_mentioned": None
            },
            {
                "text": "AWS maintaining a 32% market share",
                "type": "market",
                "citations": [],
                "quality_score": 0.75,
                "year_mentioned": None
            }
        ]

    Rating statistic quality:
        >>> stat_text = "According to Gartner's 2023 survey, 78.5% of Fortune 500 companies..."
        >>> quality = rate_statistic_quality(stat_text)
        >>> quality
        0.95  # High score due to specific percentage, citation, and year

    Inferring statistic type:
        >>> text1 = "Market share increased to 45%"
        >>> infer_statistic_type(text1)
        'percentage'
        >>> text2 = "$50 million in revenue"
        >>> infer_statistic_type(text2)
        'financial'

    Extracting year:
        >>> text_with_year = "In 2023, cloud adoption grew by 25%"
        >>> extract_year(text_with_year)
        2023

    Finding JSON objects:
        >>> text_with_json = 'Some text {"key": "value", "nested": {"data": 123}} more text'
        >>> json_obj = find_json_object(text_with_json)
        >>> json_obj
        '{"key": "value", "nested": {"data": 123}}'

    Enriching extracted facts:
        >>> fact = {
        ...     "text": "Cloud adoption grew by 25% in 2023",
        ...     "confidence": 0.8,
        ...     "source_text": "According to AWS, cloud adoption grew by 25% in 2023"
        ... }
        >>> enriched = enrich_extracted_fact(
        ...     fact,
        ...     url="https://example.com/report",
        ...     source_title="Cloud Market Report"
        ... )
        >>> enriched
        {
            "text": "Cloud adoption grew by 25% in 2023",
            "confidence": 0.8,
            "source_text": "According to AWS, cloud adoption grew by 25% in 2023",
            "source_url": "https://example.com/report",
            "source_title": "Cloud Market Report",
            "source_domain": "example.com",
            "extraction_timestamp": "2024-03-14T10:30:00",
            "statistics": [...],
            "additional_citations": [...],
            "confidence_score": 0.9  # Adjusted based on statistics and citations
        }

    Full category information extraction:
        >>> facts, relevance = await extract_category_information(
        ...     content=text,
        ...     url="https://example.com/cloud-report",
        ...     title="Cloud Computing Trends 2023",
        ...     category="market_dynamics",
        ...     original_query="cloud computing adoption trends",
        ...     prompt_template="...",
        ...     extraction_model=model
        ... )
        >>> facts
        [
            {
                "type": "fact",
                "data": {
                    "text": "Enterprise cloud adoption increased to 75% in 2023",
                    "source_url": "https://example.com/cloud-report",
                    "source_title": "Cloud Computing Trends 2023",
                    "source_domain": "example.com",
                    "extraction_timestamp": "2024-03-14T10:30:00",
                    "confidence_score": 0.9,
                    "statistics": [...],
                    "additional_citations": [...]
                }
            }
        ]
        >>> relevance
        0.95

Functions:
    extract_citations(text: str) -> List[Dict[str, str]]
        Extract citation information from text.

    extract_statistics(text: str, url: str = "", source_title: str = "") -> List[Dict[str, Any]]
        Extract statistics and numerical data from text.

    rate_statistic_quality(stat_text: str) -> float
        Rate the quality of a statistic on a scale of 0.0 to 1.0.

    extract_year(text: str) -> Optional[int]
        Extract year mentioned in text, if any.

    infer_statistic_type(text: str) -> str
        Infer the type of statistic mentioned.

    enrich_extracted_fact(fact: Dict[str, Any], url: str, source_title: str) -> Dict[str, Any]
        Enrich an extracted fact with additional context and metadata.

    find_json_object(text: str) -> Optional[str]
        Find a JSON object in text using balanced brace matching.

    extract_category_information(content: str, url: str, title: str, category: str,
                               original_query: str, prompt_template: str,
                               extraction_model: Any) -> Tuple[List[Dict[str, Any]], float]
        Extract information from content for a specific category with enhanced statistics focus.
"""


import contextlib
from typing import Dict, List, Any, Optional, Union, Tuple
import re
import json
from datetime import datetime, timezone
from urllib.parse import urlparse

from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight, error_highlight
from react_agent.utils.validations import is_valid_url
from react_agent.utils.content import chunk_text, preprocess_content, merge_chunk_results
from react_agent.utils.defaults import get_default_extraction_result
from react_agent.utils.cache import ProcessorCache, create_checkpoint, load_checkpoint, cache_result
from langgraph.graph import StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langgraph.checkpoint.base import Checkpoint, CheckpointMetadata
from react_agent.utils.statistics import (
    calculate_category_quality_score,
    assess_authoritative_sources,
    HIGH_CREDIBILITY_TERMS
)

# Initialize logger and cache
logger = get_logger(__name__)
json_cache = ProcessorCache(thread_id="json_parser")

# Regular expressions for identifying statistical content
STAT_PATTERNS = [
    r'\d+%',  # Percentage
    r'\$\d+(?:,\d+)*(?:\.\d+)?(?:\s?(?:million|billion|trillion))?',  # Currency
    r'\d+(?:\.\d+)?(?:\s?(?:million|billion|trillion))?',  # Numbers with scale
    r'increased by|decreased by|grew by|reduced by|rose|fell',  # Trend language
    r'majority|minority|fraction|proportion|ratio',  # Proportion language
    r'survey|respondents|participants|study found',  # Research language
    r'statistics show|data indicates|report reveals',  # Statistical citation
    r'market share|growth rate|adoption rate|satisfaction score',  # Business metrics
    r'average|mean|median|mode|range|standard deviation'  # Statistical terms
]

COMPILED_STAT_PATTERNS = [re.compile(pattern, re.IGNORECASE) for pattern in STAT_PATTERNS]

def extract_citations(text: str) -> List[Dict[str, str]]:
    """Extract citation information from text."""
    citations = []
    
    # Find patterns like (Source: X), [X], cited from X, etc.
    citation_patterns = [
        r'\(Source:?\s+([^)]+)\)',
        r'\[([^]]+)\]',
        r'cited\s+from\s+([^,.;]+)',
        r'according\s+to\s+([^,.;]+)',
        r'reported\s+by\s+([^,.;]+)',
        r'([^,.;]+)\s+reports'
    ]
    
    for pattern in citation_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            citation = match.group(1).strip()
            # Skip dates that might be captured in brackets like [2022]
            if not re.match(r'^(19|20)\d{2}$', citation):
                citations.append({
                    "source": citation,
                    "context": text[max(0, match.start() - 50):min(len(text), match.end() + 50)]
                })
    
    return citations

def extract_statistics(text: str, url: str = "", source_title: str = "") -> List[Dict[str, Any]]:
    """Extract statistics and numerical data from text.
    
    This function identifies and extracts statistical information from text,
    including percentages, currency amounts, numerical data, and their context.
    It also assesses the quality of each statistic based on various factors.
    
    Args:
        text: The input text to extract statistics from
        url: Optional URL source of the statistics
        source_title: Optional title of the source document
        
    Returns:
        List of dictionaries containing extracted statistics with metadata
    """
    statistics = []
    sentences = re.split(r'(?<=[.!?])\s+', text)

    for sentence in sentences:
        for pattern in COMPILED_STAT_PATTERNS:
            if pattern.search(sentence):
                # Avoid duplicate statistics
                stat_text = sentence.strip()
                if all(s["text"] != stat_text for s in statistics):
                    # Create statistic object with enhanced metadata
                    statistic = {
                        "text": stat_text,
                        "type": infer_statistic_type(stat_text),
                        "citations": extract_citations(stat_text),
                        "year_mentioned": extract_year(stat_text),
                        "source_quality": assess_source_quality(stat_text),
                        "quality_score": rate_statistic_quality(stat_text),
                        "credibility_terms": extract_credibility_terms(stat_text)
                    }

                    if enriched := enrich_extracted_fact(
                        statistic, url, source_title
                    ):
                        statistic |= enriched

                    statistics.append(statistic)
                break

    return statistics

def rate_statistic_quality(stat_text: str) -> float:
    """Rate the quality of a statistic on a scale of 0.0 to 1.0."""
    score = 0.5  # Base score
    
    # Higher quality if includes specific numbers
    if re.search(r'\d+(?:\.\d+)?%', stat_text):
        score += 0.15  # Specific percentage
    elif re.search(r'\$\d+(?:,\d+)*(?:\.\d+)?', stat_text):
        score += 0.15  # Specific currency amount
    
    # Higher quality if cites source
    if re.search(r'according to|reported by|cited from|source|study|survey', stat_text, re.IGNORECASE):
        score += 0.2
    
    # Higher quality if includes year
    if extract_year(stat_text):
        score += 0.1
    
    # Lower quality if uses vague language
    if re.search(r'may|might|could|possibly|potentially|estimated', stat_text, re.IGNORECASE):
        score -= 0.1
    
    # Cap between 0.0 and 1.0
    return max(0.0, min(1.0, score))

def extract_year(text: str) -> Optional[int]:
    """Extract year mentioned in text, if any."""
    year_match = re.search(r'\b(19\d{2}|20\d{2})\b', text)
    return int(year_match[1]) if year_match else None

def infer_statistic_type(text: str) -> str:
    """Infer the type of statistic mentioned."""
    if re.search(r'%|percent|percentage', text, re.IGNORECASE):
        return "percentage"
    elif re.search(r'\$|\beuro\b|\beur\b|\bgbp\b|\bjpy\b|cost|price|spend|budget', text, re.IGNORECASE):
        return "financial"
    elif re.search(r'time|duration|period|year|month|week|day|hour', text, re.IGNORECASE):
        return "temporal"
    elif re.search(r'ratio|proportion|fraction', text, re.IGNORECASE):
        return "ratio"
    elif re.search(r'increase|decrease|growth|decline|trend', text, re.IGNORECASE):
        return "trend"
    elif re.search(r'survey|respondent|participant', text, re.IGNORECASE):
        return "survey"
    elif re.search(r'market share|market size', text, re.IGNORECASE):
        return "market"
    else:
        return "general"

def enrich_extracted_fact(fact: Dict[str, Any], url: str, source_title: str) -> Dict[str, Any]:
    """Enrich an extracted fact with additional context and metadata."""
    # Add source information
    fact["source_url"] = url
    fact["source_title"] = source_title

    # Extract domain from URL
    try:
        domain = urlparse(url).netloc
        fact["source_domain"] = domain
    except Exception:
        fact["source_domain"] = ""

    # Add timestamp
    fact["extraction_timestamp"] = datetime.now().isoformat()

    # Extract statistics if present in source_text
    if "source_text" in fact and isinstance(fact["source_text"], str):
        if statistics := extract_statistics(fact["source_text"], url, source_title):
            fact["statistics"] = statistics

    # Extract citations if present in source_text
    if "source_text" in fact and isinstance(fact["source_text"], str):
        if citations := extract_citations(fact["source_text"]):
            fact["additional_citations"] = citations

    # Add confidence score based on enriched data
    confidence_score = fact.get("confidence", 0.5)
    if isinstance(confidence_score, str):
        # Convert string confidence to float
        if confidence_score.lower() == "high":
            confidence_score = 0.9
        elif confidence_score.lower() == "medium":
            confidence_score = 0.7
        elif confidence_score.lower() == "low":
            confidence_score = 0.4
        else:
            confidence_score = 0.5

    # Adjust confidence based on statistics and citations
    if "statistics" in fact and len(fact["statistics"]) > 0:
        confidence_score = min(1.0, confidence_score + 0.1)

    if "additional_citations" in fact and len(fact["additional_citations"]) > 0:
        confidence_score = min(1.0, confidence_score + 0.1)

    fact["confidence_score"] = confidence_score

    return fact

def find_json_object(text: str) -> Optional[str]:
    """Find a JSON object in text using balanced brace matching.
    
    This is more robust than simple regex for finding JSON objects as it:
    1. Handles nested braces correctly
    2. Supports both objects and arrays as root elements
    3. Finds the longest valid JSON-like structure
    
    Args:
        text: Text that may contain a JSON object
        
    Returns:
        Extracted JSON-like text or None if not found
    """
    # Look for both object and array patterns
    for start_char, end_char in [('{', '}'), ('[', ']')]:
        # Find potential starting positions
        start_positions = [pos for pos, char in enumerate(text) if char == start_char]
        
        for start_pos in start_positions:
            # Track nesting level
            level = 0
            # Track position
            pos = start_pos
            
            # Scan through text tracking brace/bracket balance
            while pos < len(text):
                char = text[pos]
                if char == start_char:
                    level += 1
                elif char == end_char:
                    level -= 1
                    # If we've found a balanced structure, extract it
                    if level == 0:
                        return text[start_pos:pos+1]
                pos += 1
    
    # No balanced JSON-like structure found
    return None

def _check_cache(response, category):
    # Clean response string
    response = _clean_json_string(response)

    # Check cache
    cache_key = f"json_parse_{hash(response)}"
    if cached_result := json_cache.get(cache_key):
        if isinstance(cached_result, dict) and cached_result.get("data"):
            return cached_result["data"]

    # Find and parse JSON
    json_text = find_json_object(response)
    if not json_text:
        return get_default_extraction_result(category)

    parsed = json.loads(json_text)
    result = _merge_with_default(parsed, category)

    # Cache result
    json_cache.put(
        cache_key,
        {
            "data": result,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "ttl": 3600
        }
    )

    return result

def _clean_json_string(text: str) -> str:
    """Clean and normalize JSON string."""
    text = re.sub(r'```(?:json)?\s*|\s*```', '', text)  # Remove code blocks
    text = text.strip()
    text = text.replace("'", '"')  # Handle single quotes
    text = re.sub(r',(\s*[}\]])', r'\1', text)  # Handle trailing commas
    
    # Add missing braces
    if not text.startswith('{'): text = '{' + text
    if not text.endswith('}'): text = text + '}'
    
    return re.sub(r'"(\w+)":\s*"', r'"\1": []', text)  # Handle quoted keys without values

def _merge_with_default(parsed: Dict[str, Any], category: str) -> Dict[str, Any]:
    """Merge parsed result with default template."""
    result = get_default_extraction_result(category)
    
    for key, value in parsed.items():
        if key not in result:
            continue
            
        if isinstance(value, type(result[key])):
            if isinstance(value, (list, dict)):
                result[key] = value
            elif isinstance(value, (int, float)) and key in ['relevance_score', 'confidence_score']:
                result[key] = float(value)
    
    return result

def assess_source_quality(text: str) -> float:
    """Assess the quality of the source mentioned in the statistic."""
    score = 0.5  # Base score

    # Check for credibility terms
    credibility_count = sum(
        term.lower() in text.lower() for term in HIGH_CREDIBILITY_TERMS
    )

    # Add points for credibility terms
    if credibility_count >= 2:
        score += 0.3
    elif credibility_count == 1:
        score += 0.15

    # Add points for specific source citations
    if re.search(r'according to|reported by|cited from|source|study|survey', text, re.IGNORECASE):
        score += 0.2

    # Add points for authoritative sources
    if any(domain in text.lower() for domain in ['.gov', '.edu', '.org', 'research', 'university']):
        score += 0.2

    return min(1.0, score)

def extract_credibility_terms(text: str) -> List[str]:
    """Extract credibility-indicating terms from the text."""
    return [
        term for term in HIGH_CREDIBILITY_TERMS
        if term.lower() in text.lower()
    ]

async def extract_category_information(
    content: str,
    url: str,
    title: str,
    category: str,
    original_query: str,
    prompt_template: str,
    extraction_model: Any,
    config: Optional[RunnableConfig] = None
) -> Tuple[List[Dict[str, Any]], float]:
    """Extract information from content for a specific category with enhanced statistics focus."""
    if not _validate_inputs(content, url):
        return [], 0.0

    info_highlight(f"Extracting from {url} for {category}")
    
    try:
        content = preprocess_content(content, url)
        prompt = prompt_template.format(query=original_query, url=url, content=content)
        
        extraction_result = await _process_content(
            content=content,
            prompt=prompt,
            category=category,
            extraction_model=extraction_model,
            config=config,
            url=url,
            title=title
        )
        
        facts = _get_category_facts(category, extraction_result)
        enriched_facts = [enrich_extracted_fact(fact, url, title) for fact in facts]
        
        return sorted(
            enriched_facts,
            key=lambda x: x.get("confidence_score", 0),
            reverse=True
        ), extraction_result.get("relevance_score", 0.0)
        
    except Exception as e:
        error_highlight(f"Error extracting from {url}: {str(e)}")
        return [], 0.0

def _validate_inputs(content: str, url: str) -> bool:
    """Validate input content and URL."""
    if not content or not url or not is_valid_url(url):
        warning_highlight(f"Invalid content or URL for extraction: {url}")
        return False
    return True

async def _process_content(
    content: str,
    prompt: str,
    category: str,
    extraction_model: Any,
    config: Optional[RunnableConfig],
    url: str,
    title: str
) -> Dict[str, Any]:
    """Process content and extract information using the model."""
    if len(content) > 40000:
        return await _process_chunked_content(content, prompt, category, extraction_model, config, url, title)
    
    model_response = await extraction_model(
        messages=[{"role": "human", "content": prompt}],
        config=config
    )
    
    extraction_result = safe_json_parse(model_response, category)
    if statistics := extract_statistics(content, url, title):
        extraction_result["statistics"] = statistics
        
    return extraction_result

async def _process_chunked_content(
    content: str,
    prompt: str,
    category: str,
    extraction_model: Any,
    config: Optional[RunnableConfig],
    url: str,
    title: str
) -> Dict[str, Any]:
    """Process content in chunks when it exceeds size limit."""
    info_highlight(f"Content too large ({len(content)} chars), chunking...")
    chunks = chunk_text(content)
    
    all_statistics = []
    chunk_results = []
    
    for chunk_idx, chunk in enumerate(chunks):
        info_highlight(f"Processing chunk {chunk_idx + 1}/{len(chunks)}")
        chunk_prompt = prompt.format(content=chunk)
        
        chunk_response = await extraction_model(
            messages=[{"role": "human", "content": chunk_prompt}],
            config=config
        )
        
        if chunk_result := safe_json_parse(chunk_response, category):
            chunk_statistics = extract_statistics(chunk, url, title)
            all_statistics.extend(chunk_statistics)
            chunk_results.append(chunk_result)
    
    result = merge_chunk_results(chunk_results, category)
    if all_statistics:
        result["statistics"] = all_statistics
        
    return result

def _get_category_facts(category: str, extraction_result: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Extract facts from extraction result based on category structure."""
    if not extraction_result or not isinstance(extraction_result, dict):
        return []

    # Mapping of categories to their corresponding fact types and keys
    category_mapping = {
        "market_dynamics": [("fact", "extracted_facts")],
        "provider_landscape": [
            ("vendor", "extracted_vendors"),
            ("relationship", "vendor_relationships")
        ],
        "technical_requirements": [
            ("requirement", "extracted_requirements"),
            ("standard", "standards")
        ],
        "regulatory_landscape": [
            ("regulation", "extracted_regulations"),
            ("compliance", "compliance_requirements")
        ],
        "cost_considerations": [
            ("cost", "extracted_costs"),
            ("pricing_model", "pricing_models")
        ],
        "best_practices": [
            ("practice", "extracted_practices"),
            ("methodology", "methodologies")
        ],
        "implementation_factors": [
            ("factor", "extracted_factors"),
            ("challenge", "challenges")
        ]
    }

    facts = []
    for fact_type, key in category_mapping.get(category, [("fact", "extracted_facts")]):
        items = extraction_result.get(key, [])
        facts.extend([{"type": fact_type, "data": item} for item in items])
    
    return facts

@cache_result(ttl=3600)
def safe_json_parse(response: Union[str, Dict[str, Any]], category: str) -> Dict[str, Any]:
    """Safely parse JSON response with enhanced error handling and cleanup."""
    # Return if already a dict
    if isinstance(response, dict):
        return response

    # Handle non-string input
    if not isinstance(response, str):
        return get_default_extraction_result(category)

    try:
        return _check_cache(response, category)
    except Exception as e:
        error_highlight(f"Error parsing JSON: {str(e)}")
        return get_default_extraction_result(category)
</file>

<file path="src/react_agent/utils/llm.py">
"""LLM utility functions for handling model calls and content processing.

This module provides utilities for interacting with language models,
including content length management and error handling.

Examples:
    Basic usage of call_model:
    
    ```python
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
    response = await call_model(messages)
    # Returns: {"content": "The capital of France is Paris."}
    ```

    Using call_model_json with chunking:
    
    ```python
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Analyze this long text: " + large_document}
    ]
    config = {"configurable": {"model": "openai/gpt-4"}}
    response = await call_model_json(
        messages, 
        config=config,
        chunk_size=1000,  # Process in 1000-token chunks
        overlap=100       # 100-token overlap between chunks
    )
    # Returns: {
    #     "analysis": "Key points from the document...",
    #     "summary": "Overall summary...",
    #     "topics": ["topic1", "topic2", ...]
    # }
    ```
"""

import json
import logging
import os
import re
from datetime import datetime, timezone
import os
import asyncio
from typing import Any, Dict, List, Optional, Union, cast

from anthropic import AsyncAnthropic
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_core.runnables import RunnableConfig, ensure_config
from openai import AsyncClient
from openai.types.chat import ChatCompletionMessageParam

from react_agent.configuration import Configuration
from react_agent.utils.content import (
    chunk_text,
    detect_content_type,
    estimate_tokens,
    merge_chunk_results,
    preprocess_content,
    validate_content,
)
from react_agent.configuration import Configuration
from react_agent.utils.extraction import safe_json_parse
from react_agent.utils.defaults import ChunkConfig
from react_agent.utils.logging import (
    get_logger,
    error_highlight,
    info_highlight,
    warning_highlight
)

# Initialize logger
logger = get_logger(__name__)

# Constants
MAX_TOKENS: int = 16000  # Reduced from 100000 to stay within model limits
MAX_SUMMARY_TOKENS: int = 2000  # New constant for summary model

# Initialize API clients
openai_client = AsyncClient(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_BASE")
)
anthropic_client = AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

async def summarize_content(input_content: str, max_tokens: int = MAX_SUMMARY_TOKENS) -> str:
    """Summarize content using a more efficient model.
    
    Args:
        input_content: The content to summarize
        max_tokens: Maximum tokens for the summary
        
    Returns:
        Summarized content
    """
    try:
        # Use a more efficient model for summarization
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",  # Use a more efficient model for summarization
            messages=[
                {"role": "system", "content": "You are a helpful assistant that creates concise summaries. Focus on key points and maintain factual accuracy."},
                {"role": "user", "content": f"Please summarize the following content concisely:\n\n{input_content}"}
            ],
            max_tokens=max_tokens,
            temperature=0.3  # Lower temperature for more focused summaries
        )
        response_content = cast(Optional[str], response.choices[0].message.content)
        return response_content if response_content is not None else ""
    except Exception as e:
        error_highlight(f"Error in summarize_content: {str(e)}")
        return input_content  # Return original content if summarization fails

async def _format_openai_messages(
    messages: List[Dict[str, str]], 
    system_prompt: str,
    max_tokens: Optional[int] = None
) -> List[ChatCompletionMessageParam]:
    """Format messages for OpenAI API with enhanced content handling.
    
    Args:
        messages: List of message dictionaries
        system_prompt: System prompt to use
        max_tokens: Optional maximum tokens per message
        
    Returns:
        Formatted messages for OpenAI API
    """
    if not messages:
        return [{"role": "system", "content": system_prompt}]
        
    formatted_messages: List[ChatCompletionMessageParam] = []
    max_tokens = max_tokens or MAX_TOKENS
    
    for msg in messages:
        if msg["role"] == "system":
            formatted_messages.append({"role": "system", "content": msg["content"]})
        else:
            content = msg["content"]
            if estimate_tokens(content) > max_tokens:
                info_highlight("Content too long, summarizing...")
                content = await summarize_content(content, max_tokens)
            formatted_messages.append({"role": "user", "content": content})

    if all(msg["role"] != "system" for msg in formatted_messages):
        formatted_messages.insert(0, {"role": "system", "content": system_prompt})
    
    return formatted_messages

async def _call_openai_api(model: str, messages: List[ChatCompletionMessageParam]) -> Dict[str, Any]:
    """Call OpenAI API with formatted messages."""
    try:
        response = await openai_client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=MAX_TOKENS,
            temperature=0.7
        )
        content = response.choices[0].message.content
        return {"content": content} if content else {}
    except Exception as e:
        error_highlight(f"Error in _call_openai_api: {str(e)}")
        return {}

async def call_model(
    messages: List[Dict[str, str]],
    config: Optional[RunnableConfig] = None
) -> Dict[str, Any]:
    """Call the language model with the given messages.
    
    Args:
        messages: List of message dictionaries. Each message should have 'role' 
                 (system/user/assistant) and 'content' keys.
        config: Optional configuration for the model call. Can include model selection,
               temperature, and other parameters.

    Returns:
        Dict containing the model's response. The response will always have a 'content'
        key with the model's output.

    Examples:
        ```python
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is 2+2?"}
        ]
        
        response = await call_model(messages)
        # Returns: {"content": "2 + 2 = 4"}
        
        # With custom configuration
        config = {
            "configurable": {
                "model": "anthropic/claude-3",
                "temperature": 0.7
            }
        }
        response = await call_model(messages, config)
        ```
    """
    if not messages:
        error_highlight("No messages provided to call_model")
        return {}

    try:
        config = config or {}
        configurable = config.get("configurable", {})
        configurable["timestamp"] = datetime.now(timezone.utc).isoformat()
        config = {**config, "configurable": configurable}

        configuration = Configuration.from_runnable_config(config)
        logger.info(f"Calling model with {len(messages)} messages")
        logger.debug(f"Config: {config}")

        provider, model = configuration.model.split("/", 1)

        if provider == "openai":
            formatted_messages = await _format_openai_messages(messages, configuration.system_prompt)
            return await _call_openai_api(model, formatted_messages)
        elif provider == "anthropic":
            # Handle system prompt properly
            has_system_message = any(msg["role"] == "system" for msg in messages)

            formatted_messages = []
            # Add system message if not already present in messages
            if not has_system_message:
                formatted_messages.append({"role": "system", "content": configuration.system_prompt})

            # Process all messages, preserving their roles correctly
            formatted_messages.extend(
                (
                    {"role": msg["role"], "content": msg["content"]}
                    if msg["role"] == "system"
                    else {
                        "role": (
                            "user" if msg["role"] == "user" else "assistant"
                        ),
                        "content": msg["content"],
                    }
                )
                for msg in messages
                if msg["role"] in ["user", "assistant", "system"]
            )
            formatted_messages = [msg for msg in formatted_messages if msg is not None]

            response = await anthropic_client.messages.create(
                model=model,
                messages=[{"role": msg["role"], "content": msg["content"]} for msg in formatted_messages],
                max_tokens=MAX_TOKENS,
                temperature=0.7
            )
            return {"content": str(response.content[0])}
        else:
            error_highlight(f"Unsupported model provider: {provider}")
            return {}

    except Exception as e:
        error_highlight(f"Error in call_model: {str(e)}")
        return {}

async def _process_chunk(
    chunk: str,
    previous_messages: List[Dict[str, str]],
    config: Optional[RunnableConfig] = None,
    model: Optional[str] = None
) -> Dict[str, Any]:
    """Process a single chunk of content with enhanced error handling.
    
    Args:
        chunk: Content chunk to process
        previous_messages: Previous messages in the conversation
        config: Optional configuration
        model: Optional model to use for processing
        
    Returns:
        Processed chunk result
    """
    if not chunk or not previous_messages:
        return {}

    try:
        # Create messages with chunk
        messages = previous_messages + [{"role": "human", "content": chunk}]
        
        # Call model with retries
        max_retries = 3
        retry_delay = 1
        
        for attempt in range(max_retries):
            try:
                response = await call_model(messages, config)
                if not response or not response.get("content"):
                    error_highlight("Empty response from model")
                    return {}
                    
                # Parse JSON response with enhanced error handling
                parsed = safe_json_parse(response["content"], "model_response")
                return parsed or {}
                
            except Exception as e:
                if attempt < max_retries - 1:
                    warning_highlight(f"Attempt {attempt + 1} failed: {str(e)}. Retrying...")
                    await asyncio.sleep(retry_delay * (attempt + 1))
                else:
                    error_highlight(f"All retry attempts failed: {str(e)}")
                    return {}
                    
    except Exception as e:
        error_highlight(f"Error processing chunk: {str(e)}")
        return {}
        
    return {}  # Fallback return to satisfy type checker

async def call_model_json(
    messages: List[Dict[str, str]],
    config: Optional[RunnableConfig] = None,
    chunk_size: Optional[int] = None,
    overlap: Optional[int] = None
) -> Dict[str, Any]:
    """Call the model with JSON output format and enhanced chunking.
    
    This function handles large inputs by automatically chunking them and
    merging the results. It expects and returns JSON-formatted data.
    
    Args:
        messages: List of message dictionaries. Each message should have 'role'
                 and 'content' keys.
        config: Optional configuration for the model call.
        chunk_size: Optional custom chunk size in tokens. If not provided,
                   uses default chunking configuration.
        overlap: Optional overlap size between chunks in tokens.

    Returns:
        Dict containing the merged JSON response from all chunks.

    Examples:
        ```python
        # Simple query expecting JSON response
        messages = [
            {"role": "user", "content": "List 3 capitals in JSON format"}
        ]
        response = await call_model_json(messages)
        # Returns: {
        #     "capitals": [
        #         {"city": "Paris", "country": "France"},
        #         {"city": "Tokyo", "country": "Japan"},
        #         {"city": "Rome", "country": "Italy"}
        #     ]
        # }

        # Processing a large document with custom chunking
        messages = [
            {"role": "user", "content": "Analyze this document: " + large_text}
        ]
        response = await call_model_json(
            messages,
            chunk_size=2000,  # Process in 2000-token chunks
            overlap=200       # 200-token overlap between chunks
        )
        # Returns merged analysis from all chunks:
        # {
        #     "main_topics": ["topic1", "topic2", ...],
        #     "key_points": ["point1", "point2", ...],
        #     "summary": "Overall summary of the document..."
        # }
        ```
    """
    try:
        if not messages:
            error_highlight("No messages provided to call_model_json")
            return {}

        # Process content in chunks if needed
        content = messages[-1]["content"]

        if estimate_tokens(content) <= MAX_TOKENS:
            return await _process_chunk(content, messages[:-1], config)

        info_highlight(f"Content too large ({estimate_tokens(content)} tokens), chunking...")
        chunks = chunk_text(
            content,
            chunk_size=chunk_size,
            overlap=overlap,
            use_large_chunks=True
        )

        if len(chunks) <= 1:
            return await _process_chunk(content, messages[:-1], config)
        # Process chunks in parallel with rate limiting
        chunk_results = []
        for chunk in chunks:
            result = await _process_chunk(chunk, messages[:-1], config)
            if result:
                chunk_results.append(result)

        if not chunk_results:
            error_highlight("No valid results from chunks")
            return {}

        # Merge results with enhanced merging
        return merge_chunk_results(chunk_results, "model_response")
    except Exception as e:
        error_highlight(f"Error in call_model_json: {str(e)}")
        return {}

async def _process_chunked_content(
    messages: List[Dict[str, str]],
    content: str,
    config: Optional[RunnableConfig]
) -> Dict[str, Any]:
    """Process content that exceeds token limit by chunking."""
    try:
        chunks = chunk_text(
            content,
            chunk_size=ChunkConfig.DEFAULT_CHUNK_SIZE,
            overlap=ChunkConfig.DEFAULT_OVERLAP
        )
        chunk_results = []
        
        for i, chunk in enumerate(chunks):
            chunk_messages = messages[:-1] + [{"role": "human", "content": chunk}]
            try:
                chunk_response = await call_model(chunk_messages, config)
                if chunk_response and chunk_response.get("content"):
                    chunk_results.append(chunk_response)
            except Exception as e:
                error_highlight(f"Error processing chunk {i+1}: {str(e)}")
                continue
        
        if not chunk_results:
            error_highlight("No valid results from chunks")
            return {}
            
        return merge_chunk_results(chunk_results, "general")
    except Exception as e:
        error_highlight(f"Error in _process_chunked_content: {str(e)}")
        return {}

def _parse_json_response(response: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
    """Parse and clean JSON response from model."""
    try:
        if isinstance(response, dict):
            return response
        return safe_json_parse(response, "model_response")
    except Exception as e:
        error_highlight(f"Error in _parse_json_response: {str(e)}")
        return {}

__all__ = ["call_model_json", "call_model"]
</file>

<file path="src/react_agent/utils/logging.py">
"""Logging utilities.

This module provides enhanced logging utilities and convenience methods
for the agent framework. It builds upon the basic logging configuration
defined in log_config.py.
"""

import logging
from typing import Any, Mapping, Optional, Dict


"""Module: log_config.py. This module provides logging configuration for the enrichment agent.

-------------------------
It includes functions for setting up and configuring loggers with rich formatting.
"""

# Standard library imports
import logging
import threading
from typing import Optional  # noqa: F401

# Third-party imports
from rich.console import Console
from rich.logging import RichHandler

# Create console for rich output - use stderr for logs
console = Console(stderr=True)

# Logging configuration
LOG_FORMAT = "%(message)s"  # Added back for backward compatibility with tests
DATE_FORMAT = "[%X]"

# Logger lock for thread safety
_logger_lock = threading.Lock()


def setup_logger(
    name: str = "enrichment_agent", level: int = logging.INFO
) -> logging.Logger:
    """Set up a logger with rich formatting for beautiful console output.

    Args:
        name: Name of the logger. Defaults to "enrichment_agent".
        level: Logging level. Defaults to logging.INFO.

    Returns:
        Configured logger instance with rich formatting.
    """
    with _logger_lock:
        logger = logging.getLogger(name)

        # Only configure the logger if it hasn't been configured yet
        if not logger.handlers:
            logger.setLevel(level)

            # Create rich handler with proper parameter passing
            handler = RichHandler(
                console=console,
                rich_tracebacks=True,
                tracebacks_show_locals=True,
                show_time=True,
                show_path=True,
                markup=True,
                log_time_format=DATE_FORMAT,
                omit_repeated_times=False,
                level=level,
            )

            # Add handler and disable propagation to prevent duplicate logs
            logger.addHandler(handler)
            logger.propagate = False

        return logger


# Create a default logger instance
logger = setup_logger()

def set_level(level: int) -> None:
    """Set the logging level for the default logger and root logger.
    
    Args:
        level: The logging level to set (e.g., logging.DEBUG, logging.INFO)
    """
    with _logger_lock:
        # Set level for the default logger
        logger.setLevel(level)
        
        # Update handler level
        for handler in logger.handlers:
            handler.setLevel(level)
        
        # Also set for root logger to affect other loggers in hierarchy
        root_logger = logging.getLogger()
        root_logger.setLevel(level)


def get_logger(name: str) -> logging.Logger:
    """Get a logger instance with the specified name.

    This function returns a logger configured with rich formatting and proper
    log levels. It uses the setup_logger function from log_config to ensure
    consistent logging configuration across the application.

    Args:
        name (str): The name for the logger, typically __name__ from the calling module

    Returns:
        logging.Logger: A configured logger instance with rich formatting
    """
    return setup_logger(name)


# Convenience methods for formatted logging with Rich markup
def info_success(message: str, exc_info: bool | BaseException | None = None) -> None:
    """Log a success message with green formatting.

    Args:
        message (str): The message to log
        exc_info (bool | BaseException | None): Exception information to include in the log
    """
    logger.info("[bold green]✓ %s[/bold green]", message, exc_info=exc_info)


def info_highlight(message: str, category: Optional[str] = None, progress: Optional[str] = None, exc_info: bool | BaseException | None = None) -> None:
    """Log a highlighted informational message with optional category and progress tracking.
    
    Args:
        message: The message to log
        category: Optional category for the message
        progress: Optional progress indicator
        exc_info: Exception information to include in the log
    """
    if progress:
        message = f"[{progress}] {message}"
    if category:
        message = f"[{category}] {message}"
    logger.info("[bold blue]ℹ %s[/bold blue]", message, exc_info=exc_info)


def warning_highlight(message: str, category: Optional[str] = None, exc_info: bool | BaseException | None = None) -> None:
    """Log a highlighted warning message with optional category.
    
    Args:
        message: The message to log
        category: Optional category for the message
        exc_info: Exception information to include in the log
    """
    if category:
        message = f"[{category}] {message}"
    logger.warning("[bold yellow]⚠ %s[/bold yellow]", message, exc_info=exc_info)


def error_highlight(message: str, category: Optional[str] = None, exc_info: bool | BaseException | None = None) -> None:
    """Log a highlighted error message with optional category.
    
    Args:
        message: The message to log
        category: Optional category for the message
        exc_info: Exception information to include in the log
    """
    if category:
        message = f"[{category}] {message}"
    logger.error("[bold red]✗ %s[/bold red]", message, exc_info=exc_info)


def log_dict(
    data: Mapping[str, Any], level: int = logging.INFO, title: str | None = None
) -> None:
    """Log a dictionary with pretty formatting.

    Args:
        data (Mapping[str, Any]): The dictionary to log
        level (int): The logging level to use. Defaults to logging.INFO
        title (str | None): Optional title to display before the dictionary

    Raises:
        ValueError: If level is not a valid logging level
    """
    # Validate logging level
    if level not in (
        logging.DEBUG,
        logging.INFO,
        logging.WARNING,
        logging.ERROR,
        logging.CRITICAL,
    ):
        raise ValueError(f"Invalid logging level: {level}")

    if title:
        logger.log(level, "[bold]%s[/bold]", title)

    # Use rich's pretty printing through the logger
    for key, value in data.items():
        logger.log(level, "  [cyan]%s[/cyan]: %s", key, value)


def log_step(
    step_name: str, step_number: int | None = None, total_steps: int | None = None
) -> None:
    """Log a processing step with optional step counter.

    Args:
        step_name (str): The name of the step
        step_number (int | None): Current step number if part of a sequence
        total_steps (int | None): Total number of steps in the sequence

    Raises:
        ValueError: If step_number is provided without total_steps or vice versa
        ValueError: If step_number is greater than total_steps
    """
    # Validate step numbers
    if (step_number is None) != (total_steps is None):
        raise ValueError("Both step_number and total_steps must be provided together")

    if step_number is None or total_steps is None:
        logger.info("[bold magenta]Step:[/bold magenta] %s", step_name)

    elif not 1 <= step_number <= total_steps:
        raise ValueError(f"Invalid step numbers: {step_number}/{total_steps}")
    else:
        logger.info(
            "[bold magenta]Step %s/%s:[/bold magenta] %s",
            step_number,
            total_steps,
            step_name,
        )


def log_progress(current: int, total: int, category: str, operation: str):
    """Log progress for long-running operations.
    
    Args:
        current: Current progress value
        total: Total number of items
        category: Category being processed
        operation: Type of operation (e.g., "processing", "extracting")
    """
    if total > 0:
        percentage = (current / total) * 100
        info_highlight(
            f"{operation} {current}/{total} ({percentage:.1f}%)",
            category=category
        )


def log_performance_metrics(
    operation: str,
    start_time: float,
    end_time: float,
    category: Optional[str] = None,
    additional_info: Optional[Dict[str, Any]] = None
) -> None:
    """Log performance metrics for operations.
    
    Args:
        operation: Name of the operation being measured
        start_time: Start time of the operation
        end_time: End time of the operation
        category: Optional category for the operation
        additional_info: Optional dictionary of additional metrics
    """
    duration = end_time - start_time
    message = f"{operation} completed in {duration:.2f}s"
    
    if additional_info:
        info_parts = [f"{k}: {v}" for k, v in additional_info.items()]
        message += f" ({', '.join(info_parts)})"
    
    info_highlight(message, category=category)
</file>

<file path="src/react_agent/utils/statistics.py">
"""Improved confidence scoring with statistical validation.

This module enhances the confidence scoring logic to focus on statistical
validation, source quality assessment, and cross-validation to achieve
confidence scores above 80%.
"""

from typing import Dict, List, Any, Optional, Set
from datetime import datetime, timezone
import re
from urllib.parse import urlparse
from collections import Counter

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight, error_highlight

# Initialize logger
logger = get_logger(__name__)

# Authority domain patterns
AUTHORITY_DOMAINS = [
    r'\.gov($|/)',  # Government domains
    r'\.edu($|/)',  # Educational institutions
    r'\.org($|/)',  # Non-profit organizations
    r'research\.',  # Research organizations
    r'\.ac\.($|/)',  # Academic institutions
    r'journal\.',   # Academic journals
    r'university\.',  # Universities
    r'institute\.',  # Research institutes
    r'association\.'  # Professional associations
]

# Compiled patterns for efficiency
COMPILED_AUTHORITY_PATTERNS = [re.compile(pattern) for pattern in AUTHORITY_DOMAINS]

# High-credibility source terms
HIGH_CREDIBILITY_TERMS = [
    'study', 'research', 'survey', 'report', 'analysis',
    'journal', 'publication', 'paper', 'review', 'assessment',
    'statistics', 'data', 'findings', 'results', 'evidence'
]

def calculate_category_quality_score(
    category: str,
    extracted_facts: List[Dict[str, Any]],
    sources: List[Dict[str, Any]],
    thresholds: Dict[str, Any]
) -> float:
    """Calculate enhanced quality score for a category based on extracted data.
    
    Args:
        category: Research category
        extracted_facts: List of extracted facts
        sources: List of source information
        thresholds: Quality thresholds for this category
        
    Returns:
        Quality score (0.0-1.0)
    """
    # Start with a slightly higher base score
    score = 0.35
    
    # Get category-specific thresholds
    min_facts = thresholds.get("min_facts", 3)
    min_sources = thresholds.get("min_sources", 2)
    auth_ratio = thresholds.get("authoritative_source_ratio", 0.5)
    recency_threshold = thresholds.get("recency_threshold_days", 365)
    
    # 1. QUANTITY ASSESSMENT (0.25 max)
    # Add points for number of facts (nonlinear scale to reward comprehensiveness)
    if len(extracted_facts) >= min_facts * 3:  # Excellent quantity
        score += 0.15
    elif len(extracted_facts) >= min_facts * 2:  # Very good quantity
        score += 0.12
    elif len(extracted_facts) >= min_facts:  # Good quantity
        score += 0.08
    else:  # Below threshold
        fact_ratio = len(extracted_facts) / min_facts if min_facts else 0
        score += fact_ratio * 0.06
    
    # Add points for number of sources (reward diversity)
    if len(sources) >= min_sources * 3:  # Excellent source diversity
        score += 0.10
    elif len(sources) >= min_sources * 2:  # Very good source diversity
        score += 0.08
    elif len(sources) >= min_sources:  # Good source diversity
        score += 0.05
    else:  # Below threshold
        source_ratio = len(sources) / min_sources if min_sources else 0
        score += source_ratio * 0.03
    
    # 2. STATISTICAL CONTENT (0.20 max)
    # Reward statistical content
    stat_facts = [
        f for f in extracted_facts
        if "statistics" in f or
        (isinstance(f.get("source_text"), str) and
         re.search(r'\d+', f.get("source_text", "")) is not None)
    ]
    
    if stat_facts:
        stat_ratio = len(stat_facts) / len(extracted_facts) if extracted_facts else 0
        if stat_ratio >= 0.5:  # Excellent statistical content
            score += 0.20
        elif stat_ratio >= 0.3:  # Good statistical content
            score += 0.15
        elif stat_ratio >= 0.1:  # Some statistical content
            score += 0.10
        else:  # Minimal statistical content
            score += 0.05
    
    # 3. SOURCE QUALITY (0.25 max)
    # Assess authoritative sources
    authoritative_sources = assess_authoritative_sources(sources)
    if sources:
        auth_source_ratio = len(authoritative_sources) / len(sources)
        
        if auth_source_ratio >= auth_ratio * 1.5:  # Excellent authority
            score += 0.25
        elif auth_source_ratio >= auth_ratio:  # Good authority
            score += 0.20
        elif auth_source_ratio >= auth_ratio * 0.7:  # Adequate authority
            score += 0.15
        elif auth_source_ratio >= auth_ratio * 0.5:  # Minimal authority
            score += 0.10
        else:  # Poor authority
            score += 0.05
    
    # 4. RECENCY (0.15 max)
    # Assess source recency
    recent_sources = count_recent_sources(sources, recency_threshold)
    if sources:
        recency_ratio = recent_sources / len(sources)
        
        if recency_ratio >= 0.8:  # Very recent sources
            score += 0.15
        elif recency_ratio >= 0.6:  # Mostly recent sources
            score += 0.12
        elif recency_ratio >= 0.4:  # Some recent sources
            score += 0.08
        elif recency_ratio >= 0.2:  # Few recent sources
            score += 0.05
        else:  # Outdated sources
            score += 0.02
    
    # 5. CONSISTENCY AND CROSS-VALIDATION (0.15 max)
    # Existing consistency assessment
    consistency_score = assess_fact_consistency(extracted_facts)
    
    # New statistical validation to check numeric data consistency among facts
    stat_validation_score = perform_statistical_validation(extracted_facts)
    
    # We keep the total weighting for this section at 0.15,
    # distributing it between consistency and validation.
    # For example, 0.10 for consistency and 0.05 for statistical validation:
    combined_cross_val_score = (consistency_score * 0.10) + (stat_validation_score * 0.05)
    score += combined_cross_val_score
    
    # Log detailed scoring breakdown
    info_highlight(f"Category {category} quality score breakdown:")
    info_highlight(f"  - Facts: {len(extracted_facts)}/{min_facts} min")
    info_highlight(f"  - Sources: {len(sources)}/{min_sources} min")
    info_highlight(f"  - Statistical content: {len(stat_facts)}/{len(extracted_facts)} facts")
    info_highlight(f"  - Authoritative sources: {len(authoritative_sources)}/{len(sources)} sources")
    info_highlight(f"  - Recent sources: {recent_sources}/{len(sources)} sources")
    info_highlight(f"  - Consistency score: {consistency_score:.2f}")
    info_highlight(f"  - Statistical validation score: {stat_validation_score:.2f}")
    info_highlight(f"  - Final category score: {min(1.0, score):.2f}")
    
    return min(1.0, score)

def assess_authoritative_sources(sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Assess which sources are from authoritative domains."""
    authoritative_sources = []
    
    for source in sources:
        url = source.get("url", "")
        quality_score = source.get("quality_score", 0)
        
        # Check domain patterns
        is_authoritative_domain = any(
            pattern.search(url) for pattern in COMPILED_AUTHORITY_PATTERNS
        )
        
        # Check title and source for credibility terms
        title = source.get("title", "").lower()
        source_name = source.get("source", "").lower()
        
        credibility_term_count = sum(
            1 for term in HIGH_CREDIBILITY_TERMS
            if term in title or term in source_name
        )
        
        # Consider authoritative if domain matches or high quality score or credibility terms
        if (
            is_authoritative_domain or
            quality_score >= 0.8 or
            credibility_term_count >= 2
        ):
            authoritative_sources.append(source)
    
    return authoritative_sources

def count_recent_sources(sources: List[Dict[str, Any]], recency_threshold: int) -> int:
    """Count how many sources are recent according to threshold."""
    recent_count = 0
    current_time = datetime.now().replace(tzinfo=timezone.utc)
    
    for source in sources:
        published_date = source.get("published_date")
        if not published_date:
            continue
            
        try:
            # First try direct datetime parsing
            try:
                date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
            except (ValueError, AttributeError):
                # Try other common formats
                from dateutil import parser
                date = parser.parse(published_date)
                
            # Make naive datetime timezone-aware if needed
            if hasattr(date, 'tzinfo') and date.tzinfo is None:
                date = date.replace(tzinfo=timezone.utc)
                
            days_old = (current_time - date).days
            if days_old <= recency_threshold:
                recent_count += 1
        except Exception:
            # If unparseable, don't count as recent
            pass
    
    return recent_count

def assess_fact_consistency(facts: List[Dict[str, Any]]) -> float:
    """Assess consistency among extracted facts.
    
    Returns:
        Score from 0.0-1.0 representing consistency
    """
    if not facts or len(facts) < 2:
        return 0.5  # Neutral score if insufficient facts
    
    # Extract key topics/entities mentioned across facts
    topics = extract_topics_from_facts(facts)
    
    # Count topic occurrences
    topic_counts = Counter(topics)
    
    # Calculate consistency based on topic distribution
    if not topic_counts:
        return 0.5
    
    # Get top topics (those mentioned multiple times)
    recurring_topics = {topic for topic, count in topic_counts.items() if count > 1}
    
    if not recurring_topics:
        return 0.5
    
    # Calculate what percentage of facts mention recurring topics
    facts_with_recurring = 0
    for fact in facts:
        fact_topics = get_topics_in_fact(fact)
        if any(topic in recurring_topics for topic in fact_topics):
            facts_with_recurring += 1
    
    return min(1.0, facts_with_recurring / len(facts))

def extract_topics_from_facts(facts: List[Dict[str, Any]]) -> List[str]:
    """Extract key topics/entities from facts."""
    all_topics = []
    
    for fact in facts:
        fact_topics = get_topics_in_fact(fact)
        all_topics.extend(fact_topics)
    
    return all_topics

def get_topics_in_fact(fact: Dict[str, Any]) -> Set[str]:
    """Extract topics from a single fact."""
    topics = set()
    
    # Extract from different fact formats
    if "data" in fact and isinstance(fact["data"], dict):
        data = fact["data"]
        # Handle different types of facts
        if fact.get("type") == "vendor":
            if "vendor_name" in data:
                topics.add(data["vendor_name"].lower())
        elif fact.get("type") == "relationship":
            entities = data.get("entities", [])
            for entity in entities:
                if isinstance(entity, str):
                    topics.add(entity.lower())
        elif fact.get("type") in ["requirement", "standard", "regulation", "compliance"]:
            if "description" in data:
                extract_noun_phrases(data["description"], topics)
    
    # Extract from source text if available
    if "source_text" in fact and isinstance(fact["source_text"], str):
        extract_noun_phrases(fact["source_text"], topics)
    
    return topics

def extract_noun_phrases(text: str, topics: Set[str]) -> None:
    """Extract potential noun phrases from text and add to topics set."""
    if not text:
        return
    
    # Simple noun phrase extraction (can be enhanced with NLP)
    # Currently just extracts capitalized multi-word sequences
    for match in re.finditer(r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)', text):
        topics.add(match.group(1).lower())
    
    # Also extract technical terms and acronyms
    for match in re.finditer(r'\b([A-Z]{2,})\b', text):
        topics.add(match.group(1).lower())

def perform_statistical_validation(facts: List[Dict[str, Any]]) -> float:
    """
    Check for numeric data in the extracted facts and evaluate how consistent
    or valid those values are. Returns a score in the range [0.0 - 1.0].
    
    This function:
      - Extracts numeric values from each fact (either from `source_text` or 
        from known numeric fields in `data`).
      - Calculates the mean and standard deviation of these values.
      - If the standard deviation is low relative to the mean (and enough data
        points exist), it indicates higher consistency among facts.
      - Rewards having multiple consistent numeric references.
    """
    # Gather numeric values
    numeric_values = []
    pattern = re.compile(r"\b\d+(?:\.\d+)?\b")
    
    for fact in facts:
        # Check source_text for numeric values
        source_text = fact.get("source_text", "")
        if isinstance(source_text, str):
            found_numbers = pattern.findall(source_text)
            numeric_values.extend(float(n) for n in found_numbers)
        
        # Check fact["data"] if it contains numeric fields
        if "data" in fact and isinstance(fact["data"], dict):
            for key, val in fact["data"].items():
                if isinstance(val, (int, float)):
                    numeric_values.append(float(val))
                elif isinstance(val, str):
                    # Attempt to parse if it's a numeric string
                    match = pattern.search(val)
                    if match:
                        numeric_values.append(float(match.group(0)))
    
    # If fewer than 3 numeric values, not enough to judge consistency well
    if len(numeric_values) < 3:
        return 0.5  # Neutral score
    
    import statistics
    
    try:
        mean_val = statistics.mean(numeric_values)
        stdev_val = statistics.pstdev(numeric_values)  # population stdev
        
        # If mean is ~0, skip ratio-based check to avoid division by zero
        if abs(mean_val) < 1e-9:
            # If everything is zero or near zero
            if all(abs(x) < 1e-9 for x in numeric_values):
                return 1.0  # Perfectly consistent, all zero
            return 0.5
        
        # Relative stdev: smaller means more consistent
        rel_stdev = stdev_val / abs(mean_val)
        
        # Score logic:
        # - If rel_stdev is very low (< 0.1), very high consistency
        # - If rel_stdev is moderate (< 0.3), good consistency
        # - If rel_stdev is high, penalize
        if rel_stdev < 0.1:
            return 1.0
        elif rel_stdev < 0.3:
            return 0.8
        elif rel_stdev < 0.6:
            return 0.6
        else:
            return 0.4
    except statistics.StatisticsError:
        # Fallback if something goes wrong
        return 0.5

def calculate_overall_confidence(
    category_scores: Dict[str, float],
    synthesis_quality: float,
    validation_score: float
) -> float:
    """Calculate overall confidence score from multiple inputs.
    
    Args:
        category_scores: Quality scores for each research category
        synthesis_quality: Quality score for synthesis
        validation_score: Score from validation process
        
    Returns:
        Overall confidence score (0.0-1.0)
    """
    # If no scores, return low confidence
    if not category_scores:
        return 0.3
    
    # Calculate average category score
    avg_category_score = sum(category_scores.values()) / len(category_scores)
    
    # Base confidence on weighted components
    base_score = (
        avg_category_score * 0.5 +  # Category quality is 50% of score
        synthesis_quality * 0.3 +   # Synthesis quality is 30% of score
        validation_score * 0.2      # Validation score is 20% of score
    )
    
    # Apply modifiers based on coverage
    # Full coverage of all categories gets a boost
    if len(category_scores) >= 7 and all(score >= 0.6 for score in category_scores.values()):
        base_score += 0.1
    
    # Strong statistical content gets a boost
    stats_categories = sum(
        1
        for cat, score in category_scores.items()
        if cat in ['market_dynamics', 'cost_considerations'] and score >= 0.7
    )
    if stats_categories >= 2:
        base_score += 0.05
    
    return min(1.0, base_score)

def assess_synthesis_quality(synthesis: Dict[str, Any]) -> float:
    """Assess the quality of synthesis output.
    
    Args:
        synthesis: Synthesis output
        
    Returns:
        Quality score (0.0-1.0)
    """
    if not synthesis:
        return 0.3
    
    # Base score
    score = 0.5
    
    # Check for presence of synthesis sections
    synthesis_content = synthesis.get("synthesis", {})
    if not synthesis_content:
        return 0.3
    
    # Count sections with content
    sections_with_content = sum(
        1 for section in synthesis_content.values()
        if isinstance(section, dict) and
        section.get("content") and
        len(section.get("content", "")) > 50
    )
    
    # Add points for section coverage
    section_ratio = sections_with_content / max(1, len(synthesis_content))
    score += section_ratio * 0.2
    
    # Count sections with citations
    sections_with_citations = sum(
        1 for section in synthesis_content.values()
        if isinstance(section, dict) and
        section.get("citations") and
        len(section.get("citations", [])) > 0
    )
    
    # Add points for citation coverage
    citation_ratio = sections_with_citations / max(1, len(synthesis_content))
    score += citation_ratio * 0.15
    
    # Count sections with statistics
    sections_with_stats = sum(
        1 for section in synthesis_content.values()
        if isinstance(section, dict) and
        section.get("statistics") and
        len(section.get("statistics", [])) > 0
    )
    
    # Add points for statistics coverage
    stats_ratio = sections_with_stats / max(1, len(synthesis_content))
    score += stats_ratio * 0.15
    
    return min(1.0, score)
</file>

<file path="src/react_agent/utils/validations.py">
import re
import urllib
from urllib.parse import urlparse, unquote

def is_valid_url(url: str) -> bool:
    """Validate if a URL is properly formatted."""
    # Add PDF detection to URL validation
    decoded_url = unquote(url).lower()
    if '.pdf' in decoded_url:
        return False
    
    # Enhanced fake URL detection
    fake_patterns = [
        r'example\.(com|org|net)',
        r'\b(test|sample|dummy|placeholder)\.',
        r'\b(mock|fake|staging|dev)\.(com|org|net)\b'
    ]
    if any(re.search(p, url, re.IGNORECASE) for p in fake_patterns):
        return False
    
    if not url:
        return False

    # Check for example/fake URLs
    fake_url_patterns = [
        r'example\.com',
        r'sample\.org',
        r'test\.net',
        r'domain\.com',
        r'yourcompany\.com',
        r'acme\.com',
        r'widget\.com',
        r'placeholder\.net',
        r'company\.org'
    ]

    for pattern in fake_url_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            return False

    # Basic URL validation
    try:
        result = urlparse(url)
        return all([result.scheme in ('http', 'https'), result.netloc])
    except Exception:
        return False
</file>

<file path="src/react_agent/__init__.py">
"""React Agent.

This module defines a custom reasoning and action agent graph.
It invokes tools in a simple loop.
"""

from react_agent.graphs.graph import graph

__all__ = ["graph"]
</file>

<file path="src/react_agent/configuration.py">
"""Define the configurable parameters for the agent."""

from __future__ import annotations

import os
from dataclasses import dataclass, field, fields
from typing import Annotated, Optional

from langchain_core.runnables import RunnableConfig, ensure_config

from react_agent.prompts import templates


@dataclass(kw_only=True)
class Configuration:
    """The configuration for the agent."""

    system_prompt: str = field(
        default=templates.SYSTEM_PROMPT,
        metadata={
            "description": "The system prompt to use for the agent's interactions. "
            "This prompt sets the context and behavior for the agent."
        },
    )

    model: Annotated[str, {"__template_metadata__": {"kind": "llm"}}] = field(
        default="openai/gpt-4o",
        metadata={
            "description": "The name of the language model to use for the agent's main interactions. "
            "Should be in the form: provider/model-name."
        },
    )

    max_search_results: int = field(
        default=10,
        metadata={
            "description": "The maximum number of search results to return for each search query."
        },
    )

    firecrawl_api_key: Optional[str] = field(
        default_factory=lambda: os.getenv("FIRECRAWL_API_KEY"),
        metadata={
            "description": "API key for the FireCrawl service. Required for web scraping and crawling."
        },
    )

    firecrawl_url: Optional[str] = field(
        default_factory=lambda: os.getenv("FIRECRAWL_URL"),
        metadata={
            "description": "Base URL for the FireCrawl service. Use this for self-hosted instances."
        },
    )

    jina_api_key: Optional[str] = field(
        default_factory=lambda: os.getenv("JINA_API_KEY"),
        metadata={
            "description": "API key for the Jina AI service. Required for web search and summarization."
        },
    )

    jina_url: Optional[str] = field(
        default_factory=lambda: os.getenv("JINA_URL", "https://s.jina.ai"),
        metadata={
            "description": "Base URL for the Jina AI service. Use this for self-hosted instances."
        },
    )

    @classmethod
    def from_runnable_config(
        cls, config: Optional[RunnableConfig] = None
    ) -> Configuration:
        """Create a Configuration instance from a RunnableConfig object."""
        config = ensure_config(config)
        configurable = config.get("configurable") or {}
        
        # Get environment variables for FireCrawl
        env_config = {
            "firecrawl_api_key": os.getenv("FIRECRAWL_API_KEY"),
            "firecrawl_url": os.getenv("FIRECRAWL_URL"),
        }
        
        # Merge environment variables with configurable, giving priority to configurable
        merged_config = {**env_config, **configurable}
        
        _fields = {f.name for f in fields(cls) if f.init}
        return cls(**{k: v for k, v in merged_config.items() if k in _fields})
</file>

<file path="src/react_agent/state.py">
"""Define the state structures for the agent."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Sequence

from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages
from langgraph.managed import IsLastStep
from typing_extensions import Annotated


@dataclass
class InputState:
    """Defines the input state for the agent, representing a narrower interface to the outside world.

    This class is used to define the initial state and structure of incoming data.
    """

    messages: Annotated[Sequence[AnyMessage], add_messages] = field(
        default_factory=list
    )
    """
    Messages tracking the primary execution state of the agent.

    Typically accumulates a pattern of:
    1. HumanMessage - user input
    2. AIMessage with .tool_calls - agent picking tool(s) to use to collect information
    3. ToolMessage(s) - the responses (or errors) from the executed tools
    4. AIMessage without .tool_calls - agent responding in unstructured format to the user
    5. HumanMessage - user responds with the next conversational turn

    Steps 2-5 may repeat as needed.

    The `add_messages` annotation ensures that new messages are merged with existing ones,
    updating by ID to maintain an "append-only" state unless a message with the same ID is provided.
    """


@dataclass
class State(InputState):
    """Represents the complete state of the agent, extending InputState with additional attributes.

    This class can be used to store any information needed throughout the agent's lifecycle.
    """

    is_last_step: IsLastStep = field(default=False)
    """
    Indicates whether the current step is the last one before the graph raises an error.

    This is a 'managed' variable, controlled by the state machine rather than user code.
    It is set to 'True' when the step count reaches recursion_limit - 1.
    """

    # Additional attributes can be added here as needed.
    # Common examples include:
    # retrieved_documents: List[Document] = field(default_factory=list)
    # extracted_entities: Dict[str, Any] = field(default_factory=dict)
    # api_connections: Dict[str, Any] = field(default_factory=dict)
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
uv.lock

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST
.langgraph_api/
# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/latest/usage/project/#working-with-version-control
.pdm.toml
.pdm-python
.pdm-build/

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/
.qodo
</file>

<file path=".repomixignore">
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
.venv/
src/react_agent/tools/tavily.py
src/react_agent/tools/firecrawl.py
src/react_agent/graphs/error.py
src/react_agent/graphs/graph.py
src/react_agent/graphs/analysis.py
tests/cassettes/**
repomix-output.*
repomix.config.json
tests/**
static/**
</file>

<file path="langgraph.json">
{
  "dockerfile_lines": [],
  "graphs": {
    "agent": "src/react_agent/graphs/graph.py:graph",
    "analysis": "src/react_agent/graphs/analysis.py:graph",
    "research": "src/react_agent/graphs/research.py:research_graph"
  },
  "env": ".env",
  "python_version": "3.11",
  "dependencies": [
    "."
  ]
}
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 LangChain

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="Makefile">
.PHONY: all format lint test tests test_watch integration_tests docker_tests help extended_tests

# Default target executed when no arguments are given to make.
all: help

# Define a variable for the test file path.
TEST_FILE ?= tests/unit_tests/

test:
	python -m pytest $(TEST_FILE)

test_watch:
	python -m ptw --snapshot-update --now . -- -vv tests/unit_tests

test_profile:
	python -m pytest -vv tests/unit_tests/ --profile-svg

extended_tests:
	python -m pytest --only-extended $(TEST_FILE)


######################
# LINTING AND FORMATTING
######################

# Define a variable for Python and notebook files.
PYTHON_FILES=src/
MYPY_CACHE=.mypy_cache
lint format: PYTHON_FILES=.
lint_diff format_diff: PYTHON_FILES=$(shell git diff --name-only --diff-filter=d main | grep -E '\.py$$|\.ipynb$$')
lint_package: PYTHON_FILES=src
lint_tests: PYTHON_FILES=tests
lint_tests: MYPY_CACHE=.mypy_cache_test

lint lint_diff lint_package lint_tests:
	python -m ruff check .
	[ "$(PYTHON_FILES)" = "" ] || python -m ruff format $(PYTHON_FILES) --diff
	[ "$(PYTHON_FILES)" = "" ] || python -m ruff check --select I $(PYTHON_FILES)
	[ "$(PYTHON_FILES)" = "" ] || python -m mypy --strict $(PYTHON_FILES)
	[ "$(PYTHON_FILES)" = "" ] || mkdir -p $(MYPY_CACHE) && python -m mypy --strict $(PYTHON_FILES) --cache-dir $(MYPY_CACHE)

format format_diff:
	ruff format $(PYTHON_FILES)
	ruff check --select I --fix $(PYTHON_FILES)

spell_check:
	codespell --toml pyproject.toml

spell_fix:
	codespell --toml pyproject.toml -w

######################
# HELP
######################

help:
	@echo '----'
	@echo 'format                       - run code formatters'
	@echo 'lint                         - run linters'
	@echo 'test                         - run unit tests'
	@echo 'tests                        - run unit tests'
	@echo 'test TEST_FILE=<test_file>   - run all tests in file'
	@echo 'test_watch                   - run unit tests in watch mode'
</file>

<file path="pyproject.toml">
[project]
name = "react-agent"
version = "0.0.1"
description = "Starter template for making a custom Reasoning and Action agent (using tool calling) in LangGraph."
authors = [
    { name = "William Fu-Hinthorn", email = "13333726+hinthornw@users.noreply.github.com" },
]
readme = "README.md"
license = { text = "MIT" }
requires-python = ">=3.11,<4.0"
dependencies = [
    "langgraph==0.3.18",
    "langchain-openai>=0.1.22",
    "langchain-anthropic>=0.1.23",
    "langchain>=0.2.14",
    "langchain-fireworks>=0.1.7",
    "langgraph-checkpoint>=0.1.1",
    "python-dotenv>=1.0.1",
    "langchain-community>=0.2.17",
    "tavily-python>=0.4.0",
    "pandas",
    "firecrawl-py",
    "rich",
    "langgraph-cli[inmem]>=0.1.79",
    "langgraph-utils==0.0.3",
    "langgraph-api==0.0.33",
    "langgraph-checkpoint-sqlite==2.0.6",
    "cursor-rules>=0.5.1",
]


[project.optional-dependencies]
dev = ["mypy>=1.11.1", "ruff>=0.6.1"]

[build-system]
requires = ["setuptools>=73.0.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = ["langgraph.templates.react_agent", "react_agent"]
[tool.setuptools.package-dir]
"langgraph.templates.react_agent" = "src/react_agent"
"react_agent" = "src/react_agent"


[tool.setuptools.package-data]
"*" = ["py.typed"]

[tool.ruff]
lint.select = [
    "E",    # pycodestyle
    "F",    # pyflakes
    "I",    # isort
    "D",    # pydocstyle
    "D401", # First line should be in imperative mood
    "T201",
    "UP",
]
lint.ignore = [
    "UP006",
    "UP007",
    # We actually do want to import from typing_extensions
    "UP035",
    # Relax the convention by _not_ requiring documentation for every function parameter.
    "D417",
    "E501",
]
[tool.ruff.lint.per-file-ignores]
"tests/*" = ["D", "UP"]
[tool.ruff.lint.pydocstyle]
convention = "google"

[dependency-groups]
dev = [
    "langgraph-cli[inmem]>=0.1.71",
]
</file>

<file path="README.md">
# LangGraph ReAct Agent Template

[![CI](https://github.com/langchain-ai/react-agent/actions/workflows/unit-tests.yml/badge.svg)](https://github.com/langchain-ai/react-agent/actions/workflows/unit-tests.yml)
[![Integration Tests](https://github.com/langchain-ai/react-agent/actions/workflows/integration-tests.yml/badge.svg)](https://github.com/langchain-ai/react-agent/actions/workflows/integration-tests.yml)
[![Open in - LangGraph Studio](https://img.shields.io/badge/Open_in-LangGraph_Studio-00324d.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI4NS4zMzMiIGhlaWdodD0iODUuMzMzIiB2ZXJzaW9uPSIxLjAiIHZpZXdCb3g9IjAgMCA2NCA2NCI+PHBhdGggZD0iTTEzIDcuOGMtNi4zIDMuMS03LjEgNi4zLTYuOCAyNS43LjQgMjQuNi4zIDI0LjUgMjUuOSAyNC41QzU3LjUgNTggNTggNTcuNSA1OCAzMi4zIDU4IDcuMyA1Ni43IDYgMzIgNmMtMTIuOCAwLTE2LjEuMy0xOSAxLjhtMzcuNiAxNi42YzIuOCAyLjggMy40IDQuMiAzLjQgNy42cy0uNiA0LjgtMy40IDcuNkw0Ny4yIDQzSDE2LjhsLTMuNC0zLjRjLTQuOC00LjgtNC44LTEwLjQgMC0xNS4ybDMuNC0zLjRoMzAuNHoiLz48cGF0aCBkPSJNMTguOSAyNS42Yy0xLjEgMS4zLTEgMS43LjQgMi41LjkuNiAxLjcgMS44IDEuNyAyLjcgMCAxIC43IDIuOCAxLjYgNC4xIDEuNCAxLjkgMS40IDIuNS4zIDMuMi0xIC42LS42LjkgMS40LjkgMS41IDAgMi43LS41IDIuNy0xIDAtLjYgMS4xLS44IDIuNi0uNGwyLjYuNy0xLjgtMi45Yy01LjktOS4zLTkuNC0xMi4zLTExLjUtOS44TTM5IDI2YzAgMS4xLS45IDIuNS0yIDMuMi0yLjQgMS41LTIuNiAzLjQtLjUgNC4yLjguMyAyIDEuNyAyLjUgMy4xLjYgMS41IDEuNCAyLjMgMiAyIDEuNS0uOSAxLjItMy41LS40LTMuNS0yLjEgMC0yLjgtMi44LS44LTMuMyAxLjYtLjQgMS42LS41IDAtLjYtMS4xLS4xLTEuNS0uNi0xLjItMS42LjctMS43IDMuMy0yLjEgMy41LS41LjEuNS4yIDEuNi4zIDIuMiAwIC43LjkgMS40IDEuOSAxLjYgMi4xLjQgMi4zLTIuMy4yLTMuMi0uOC0uMy0yLTEuNy0yLjUtMy4xLTEuMS0zLTMtMy4zLTMtLjUiLz48L3N2Zz4=)](https://langgraph-studio.vercel.app/templates/open?githubUrl=https://github.com/langchain-ai/react-agent)

This template showcases a [ReAct agent](https://arxiv.org/abs/2210.03629) implemented using [LangGraph](https://github.com/langchain-ai/langgraph), designed for [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio). ReAct agents are uncomplicated, prototypical agents that can be flexibly extended to many tools.

![Graph view in LangGraph studio UI](./static/studio_ui.png)

The core logic, defined in `src/react_agent/graph.py`, demonstrates a flexible ReAct agent that iteratively reasons about user queries and executes actions, showcasing the power of this approach for complex problem-solving tasks.

## What it does

The ReAct agent:

1. Takes a user **query** as input
2. Reasons about the query and decides on an action
3. Executes the chosen action using available tools
4. Observes the result of the action
5. Repeats steps 2-4 until it can provide a final answer

By default, it's set up with a basic set of tools, but can be easily extended with custom tools to suit various use cases.

## Getting Started

Assuming you have already [installed LangGraph Studio](https://github.com/langchain-ai/langgraph-studio?tab=readme-ov-file#download), to set up:

1. Create a `.env` file.

```bash
cp .env.example .env
```

2. Define required API keys in your `.env` file.

The primary [search tool](./src/react_agent/tools.py) [^1] used is [Tavily](https://tavily.com/). Create an API key [here](https://app.tavily.com/sign-in).

<!--
Setup instruction auto-generated by `langgraph template lock`. DO NOT EDIT MANUALLY.
-->

### Setup Model

The defaults values for `model` are shown below:

```yaml
model: anthropic/claude-3-5-sonnet-20240620
```

Follow the instructions below to get set up, or pick one of the additional options.

#### Anthropic

To use Anthropic's chat models:

1. Sign up for an [Anthropic API key](https://console.anthropic.com/) if you haven't already.
2. Once you have your API key, add it to your `.env` file:

```
ANTHROPIC_API_KEY=your-api-key
```
#### OpenAI

To use OpenAI's chat models:

1. Sign up for an [OpenAI API key](https://platform.openai.com/signup).
2. Once you have your API key, add it to your `.env` file:
```
OPENAI_API_KEY=your-api-key
```





<!--
End setup instructions
-->


3. Customize whatever you'd like in the code.
4. Open the folder LangGraph Studio!

## How to customize

1. **Add new tools**: Extend the agent's capabilities by adding new tools in [tools.py](./src/react_agent/tools.py). These can be any Python functions that perform specific tasks.
2. **Select a different model**: We default to Anthropic's Claude 3 Sonnet. You can select a compatible chat model using `provider/model-name` via configuration. Example: `openai/gpt-4-turbo-preview`.
3. **Customize the prompt**: We provide a default system prompt in [prompts.py](./src/react_agent/prompts.py). You can easily update this via configuration in the studio.

You can also quickly extend this template by:

- Modifying the agent's reasoning process in [graph.py](./src/react_agent/graph.py).
- Adjusting the ReAct loop or adding additional steps to the agent's decision-making process.

## Development

While iterating on your graph, you can edit past state and rerun your app from past states to debug specific nodes. Local changes will be automatically applied via hot reload. Try adding an interrupt before the agent calls tools, updating the default system message in `src/react_agent/configuration.py` to take on a persona, or adding additional nodes and edges!

Follow up requests will be appended to the same thread. You can create an entirely new thread, clearing previous history, using the `+` button in the top right.

You can find the latest (under construction) docs on [LangGraph](https://github.com/langchain-ai/langgraph) here, including examples and other references. Using those guides can help you pick the right patterns to adapt here for your use case.

LangGraph Studio also integrates with [LangSmith](https://smith.langchain.com/) for more in-depth tracing and collaboration with teammates.

[^1]: https://python.langchain.com/docs/concepts/#tools

<!--
Configuration auto-generated by `langgraph template lock`. DO NOT EDIT MANUALLY.
{
  "config_schemas": {
    "agent": {
      "type": "object",
      "properties": {
        "model": {
          "type": "string",
          "default": "anthropic/claude-3-5-sonnet-20240620",
          "description": "The name of the language model to use for the agent's main interactions. Should be in the form: provider/model-name.",
          "environment": [
            {
              "value": "anthropic/claude-1.2",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-2.0",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-2.1",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-3-5-sonnet-20240620",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-3-haiku-20240307",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-3-opus-20240229",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-3-sonnet-20240229",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "anthropic/claude-instant-1.2",
              "variables": "ANTHROPIC_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-0125",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-0301",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-0613",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-1106",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-16k",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-3.5-turbo-16k-0613",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-0125-preview",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-0314",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-0613",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-1106-preview",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-32k",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-32k-0314",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-32k-0613",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-turbo",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-turbo-preview",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4-vision-preview",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4o",
              "variables": "OPENAI_API_KEY"
            },
            {
              "value": "openai/gpt-4o-mini",
              "variables": "OPENAI_API_KEY"
            }
          ]
        }
      },
      "environment": [
        "TAVILY_API_KEY"
      ]
    }
  }
}
-->
</file>

<file path="rules.json">
{
    "libraries": [
      {
        "name": "react",
        "tags": ["frontend", "framework", "javascript"]
      },
      {
        "name": "react-native",
        "tags": ["frontend", "framework", "javascript", "mobile", "cross-platform"]
      },
      {
        "name": "react-query",
        "tags": ["frontend", "javascript", "data-fetching"]
      },
      {
        "name": "react-redux",
        "tags": ["frontend", "javascript", "state-management"]
      },
      {
        "name": "react-mobx",
        "tags": ["frontend", "javascript", "state-management"]
      },
      {
        "name": "next-js",
        "tags": ["frontend", "framework", "javascript", "react", "ssr"]
      },
      {
        "name": "vue",
        "tags": ["frontend", "framework", "javascript"]
      },
      {
        "name": "vue3",
        "tags": ["frontend", "framework", "javascript"]
      },
      {
        "name": "nuxt",
        "tags": ["frontend", "framework", "javascript", "vue", "ssr"]
      },
      {
        "name": "angular",
        "tags": ["frontend", "framework", "javascript", "typescript"]
      },
      {
        "name": "svelte",
        "tags": ["frontend", "framework", "javascript"]
      },
      {
        "name": "sveltekit",
        "tags": ["frontend", "framework", "javascript", "svelte", "ssr"]
      },
      {
        "name": "solidjs",
        "tags": ["frontend", "framework", "javascript"]
      },
      {
        "name": "qwik",
        "tags": ["frontend", "framework", "javascript"]
      },
      {
        "name": "express",
        "tags": ["backend", "framework", "javascript", "nodejs"]
      },
      {
        "name": "nestjs",
        "tags": ["backend", "framework", "javascript", "typescript", "nodejs"]
      },
      {
        "name": "bun",
        "tags": ["backend", "javascript", "runtime", "nodejs-alternative"]
      },
      {
        "name": "django",
        "tags": ["backend", "framework", "python", "orm", "full-stack"]
      },
      {
        "name": "flask",
        "tags": ["backend", "framework", "python", "microframework"]
      },
      {
        "name": "fastapi",
        "tags": ["backend", "framework", "python", "api", "async"]
      },
      {
        "name": "pyramid",
        "tags": ["backend", "framework", "python"]
      },
      {
        "name": "tornado",
        "tags": ["backend", "framework", "python", "async"]
      },
      {
        "name": "sanic",
        "tags": ["backend", "framework", "python", "async"]
      },
      {
        "name": "bottle",
        "tags": ["backend", "framework", "python", "microframework"]
      },
      {
        "name": "laravel",
        "tags": ["backend", "framework", "php"]
      },
      {
        "name": "springboot",
        "tags": ["backend", "framework", "java"]
      },
      {
        "name": "fiber",
        "tags": ["backend", "framework", "go"]
      },
      {
        "name": "servemux",
        "tags": ["backend", "framework", "go"]
      },
      {
        "name": "phoenix",
        "tags": ["backend", "framework", "elixir"]
      },
      {
        "name": "actix-web",
        "tags": ["backend", "framework", "rust"]
      },
      {
        "name": "rocket",
        "tags": ["backend", "framework", "rust"]
      },
      {
        "name": "shadcn",
        "tags": ["ui", "component-library", "react"]
      },
      {
        "name": "chakra-ui",
        "tags": ["ui", "component-library", "react"]
      },
      {
        "name": "material-ui",
        "tags": ["ui", "component-library", "react"]
      },
      {
        "name": "tailwind",
        "tags": ["ui", "css", "utility-first"]
      },
      {
        "name": "jetpack-compose",
        "tags": ["ui", "mobile", "android", "kotlin"]
      },
      {
        "name": "tkinter",
        "tags": ["ui", "gui", "python", "desktop"]
      },
      {
        "name": "pyqt",
        "tags": ["ui", "gui", "python", "desktop", "qt"]
      },
      {
        "name": "pyside",
        "tags": ["ui", "gui", "python", "desktop", "qt"]
      },
      {
        "name": "kivy",
        "tags": ["ui", "gui", "python", "cross-platform", "mobile"]
      },
      {
        "name": "pygame",
        "tags": ["ui", "gui", "python", "game-development"]
      },
      {
        "name": "customtkinter",
        "tags": ["ui", "gui", "python", "desktop", "tkinter"]
      },
      {
        "name": "redux",
        "tags": ["state-management", "javascript", "react"]
      },
      {
        "name": "mobx",
        "tags": ["state-management", "javascript", "react"]
      },
      {
        "name": "zustand",
        "tags": ["state-management", "javascript", "react"]
      },
      {
        "name": "riverpod",
        "tags": ["state-management", "flutter", "dart"]
      },
      {
        "name": "supabase",
        "tags": ["database", "sql", "postgresql", "backend-as-service"]
      },
      {
        "name": "postgresql",
        "tags": ["database", "sql", "relational"]
      },
      {
        "name": "prisma",
        "tags": ["database", "orm", "typescript", "javascript"]
      },
      {
        "name": "mongodb",
        "tags": ["database", "nosql", "document"]
      },
      {
        "name": "redis",
        "tags": ["database", "nosql", "key-value", "in-memory"]
      },
      {
        "name": "duckdb",
        "tags": ["database", "analytics", "sql", "olap"]
      },
      {
        "name": "sqlalchemy",
        "tags": ["database", "orm", "python", "sql"]
      },
      {
        "name": "peewee",
        "tags": ["database", "orm", "python", "sql"]
      },
      {
        "name": "pony",
        "tags": ["database", "orm", "python", "sql"]
      },
      {
        "name": "tortoise-orm",
        "tags": ["database", "orm", "python", "sql", "async"]
      },
      {
        "name": "django-orm",
        "tags": ["database", "orm", "python", "sql", "django"]
      },
      {
        "name": "vite",
        "tags": ["development", "build-tool", "javascript"]
      },
      {
        "name": "webpack",
        "tags": ["development", "build-tool", "javascript"]
      },
      {
        "name": "turbopack",
        "tags": ["development", "build-tool", "javascript"]
      },
      {
        "name": "poetry",
        "tags": ["development", "build-tool", "python", "dependency-management"]
      },
      {
        "name": "setuptools",
        "tags": ["development", "build-tool", "python", "packaging"]
      },
      {
        "name": "jest",
        "tags": ["development", "testing", "javascript"]
      },
      {
        "name": "detox",
        "tags": ["development", "testing", "javascript", "react-native", "e2e"]
      },
      {
        "name": "playwright",
        "tags": ["development", "testing", "javascript", "e2e", "browser"]
      },
      {
        "name": "vitest",
        "tags": ["development", "testing", "javascript", "vite"]
      },
      {
        "name": "python",
        "tags": ["python"]
      },
      {
        "name": "pytest",
        "tags": ["development", "testing", "python"]
      },
      {
        "name": "unittest",
        "tags": ["development", "testing", "python", "standard-library"]
      },
      {
        "name": "nose2",
        "tags": ["development", "testing", "python"]
      },
      {
        "name": "hypothesis",
        "tags": ["development", "testing", "python", "property-based"]
      },
      {
        "name": "behave",
        "tags": ["development", "testing", "python", "bdd"]
      },
      {
        "name": "docker",
        "tags": ["development", "containerization", "devops"]
      },
      {
        "name": "kubernetes",
        "tags": ["development", "containerization", "orchestration", "devops"]
      },
      {
        "name": "git",
        "tags": ["development", "version-control"]
      },
      {
        "name": "mkdocs",
        "tags": ["development", "documentation", "markdown"]
      },
      {
        "name": "sphinx",
        "tags": ["development", "documentation", "python", "rst"]
      },
      {
        "name": "pdoc",
        "tags": ["development", "documentation", "python", "auto-generation"]
      },
      {
        "name": "github-actions",
        "tags": ["development", "ci-cd", "devops"]
      },
      {
        "name": "terraform",
        "tags": ["development", "infrastructure", "iac", "devops"]
      },
      {
        "name": "black",
        "tags": ["development", "python", "formatter", "linting"]
      },
      {
        "name": "flake8",
        "tags": ["development", "python", "linting"]
      },
      {
        "name": "pylint",
        "tags": ["development", "python", "linting", "static-analysis"]
      },
      {
        "name": "mypy",
        "tags": ["development", "python", "type-checking", "static-analysis"]
      },
      {
        "name": "isort",
        "tags": ["development", "python", "formatter", "imports"]
      },
      {
        "name": "pydantic",
        "tags": ["development", "python", "data-validation", "type-checking"]
      },
      {
        "name": "pyright",
        "tags": ["development", "python", "type-checking", "static-analysis"]
      },
      {
        "name": "tauri",
        "tags": ["cross-platform", "desktop", "rust", "javascript"]
      },
      {
        "name": "electron",
        "tags": ["cross-platform", "desktop", "javascript"]
      },
      {
        "name": "expo",
        "tags": ["cross-platform", "mobile", "react-native"]
      },
      {
        "name": "flutter",
        "tags": ["cross-platform", "mobile", "dart"]
      },
      {
        "name": "pytorch",
        "tags": ["ai", "ml", "machine-learning", "python", "deep-learning"]
      },
      {
        "name": "scikit-learn",
        "tags": ["ai", "ml", "machine-learning", "python", "data-science"]
      },
      {
        "name": "pandas",
        "tags": ["ai", "ml", "data-science", "python", "data-analysis"]
      },
      {
        "name": "tensorflow",
        "tags": ["ai", "ml", "machine-learning", "python", "deep-learning"]
      },
      {
        "name": "keras",
        "tags": ["ai", "ml", "machine-learning", "python", "deep-learning"]
      },
      {
        "name": "xgboost",
        "tags": ["ai", "ml", "machine-learning", "python", "gradient-boosting"]
      },
      {
        "name": "lightgbm",
        "tags": ["ai", "ml", "machine-learning", "python", "gradient-boosting"]
      },
      {
        "name": "cuda",
        "tags": ["ai", "ml", "gpu-computing", "parallel-computing"]
      },
      {
        "name": "numba",
        "tags": ["ai", "ml", "gpu-computing", "python", "jit-compiler"]
      },
      {
        "name": "langchain",
        "tags": ["ai", "ml", "llm", "python"]
      },
      {
        "name": "huggingface",
        "tags": ["ai", "ml", "llm", "python", "transformers"]
      },
      {
        "name": "vllm",
        "tags": ["ai", "ml", "llm", "python", "inference"]
      },
      {
        "name": "llama-index",
        "tags": ["ai", "ml", "llm", "python", "rag"]
      },
      {
        "name": "modal",
        "tags": ["ai", "ml", "cloud-inference", "serverless", "deployment"]
      },
      {
        "name": "numpy",
        "tags": ["ai", "ml", "data-science", "python", "numerical-computing"]
      },
      {
        "name": "scipy",
        "tags": ["ai", "ml", "data-science", "python", "scientific-computing"]
      },
      {
        "name": "matplotlib",
        "tags": ["ai", "ml", "data-science", "python", "data-visualization"]
      },
      {
        "name": "seaborn",
        "tags": ["ai", "ml", "data-science", "python", "data-visualization"]
      },
      {
        "name": "plotly",
        "tags": ["ai", "ml", "data-science", "python", "interactive-visualization"]
      },
      {
        "name": "statsmodels",
        "tags": ["ai", "ml", "data-science", "python", "statistics"]
      },
      {
        "name": "dask",
        "tags": ["ai", "ml", "data-science", "python", "parallel-computing", "big-data"]
      },
      {
        "name": "htmx",
        "tags": ["web", "javascript", "modern-patterns"]
      },
      {
        "name": "trpc",
        "tags": ["web", "typescript", "api", "modern-patterns"]
      },
      {
        "name": "typescript",
        "tags": ["web", "javascript", "type-checking", "language"]
      },
      {
        "name": "zod",
        "tags": ["web", "typescript", "validation", "type-checking"]
      },
      {
        "name": "axios",
        "tags": ["web", "javascript", "http-client"]
      },
      {
        "name": "guzzle",
        "tags": ["web", "php", "http-client"]
      },
      {
        "name": "requests",
        "tags": ["web", "python", "http-client"]
      },
      {
        "name": "httpx",
        "tags": ["web", "python", "http-client", "async"]
      },
      {
        "name": "aiohttp",
        "tags": ["web", "python", "http-client", "async"]
      },
      {
        "name": "graphql",
        "tags": ["web", "api", "query-language"]
      },
      {
        "name": "apollo-client",
        "tags": ["web", "api", "graphql", "javascript"]
      },
      {
        "name": "flask-restful",
        "tags": ["web", "api", "python", "flask"]
      },
      {
        "name": "solidity",
        "tags": ["blockchain", "ethereum", "smart-contracts", "language"]
      },
      {
        "name": "hardhat",
        "tags": ["blockchain", "ethereum", "development", "javascript"]
      },
      {
        "name": "vercel",
        "tags": ["cloud", "deployment", "serverless", "frontend"]
      },
      {
        "name": "cloudflare",
        "tags": ["cloud", "deployment", "edge-computing", "cdn"]
      },
      {
        "name": "aws-lambda",
        "tags": ["cloud", "serverless", "aws"]
      },
      {
        "name": "aws",
        "tags": ["cloud", "major-platform"]
      },
      {
        "name": "gcp",
        "tags": ["cloud", "major-platform"]
      },
      {
        "name": "azure",
        "tags": ["cloud", "major-platform"]
      },
      {
        "name": "beautifulsoup4",
        "tags": ["python", "web-scraping", "html-parsing"]
      },
      {
        "name": "scrapy",
        "tags": ["python", "web-scraping", "crawler", "framework"]
      },
      {
        "name": "selenium",
        "tags": ["python", "web-scraping", "browser-automation", "testing"]
      },
      {
        "name": "asyncio",
        "tags": ["python", "async", "standard-library"]
      },
      {
        "name": "trio",
        "tags": ["python", "async"]
      },
      {
        "name": "anyio",
        "tags": ["python", "async", "compatibility-layer"]
      },
      {
        "name": "nltk",
        "tags": ["python", "nlp", "text-processing"]
      },
      {
        "name": "spacy",
        "tags": ["python", "nlp", "text-processing"]
      },
      {
        "name": "gensim",
        "tags": ["python", "nlp", "topic-modeling"]
      },
      {
        "name": "transformers",
        "tags": ["python", "nlp", "deep-learning", "huggingface"]
      },
      {
        "name": "pillow",
        "tags": ["python", "image-processing"]
      },
      {
        "name": "opencv-python",
        "tags": ["python", "image-processing", "computer-vision"]
      },
      {
        "name": "scikit-image",
        "tags": ["python", "image-processing", "scientific-computing"]
      },
      {
        "name": "tqdm",
        "tags": ["python", "utilities", "progress-bar"]
      },
      {
        "name": "rich",
        "tags": ["python", "utilities", "terminal", "formatting"]
      },
      {
        "name": "click",
        "tags": ["python", "utilities", "cli"]
      },
      {
        "name": "typer",
        "tags": ["python", "utilities", "cli"]
      },
      {
        "name": "streamlit",
        "tags": ["python", "utilities", "data-apps", "dashboard"]
      },
      {
        "name": "css",
        "tags": ["web", "frontend", "styling", "language"]
      },
      {
        "name": "crewai",
        "tags": ["ai", "ml", "llm", "python", "agent-framework", "multi-agent"]
      },
      {
        "name": "smolagents",
        "tags": ["ai", "ml", "llm", "python", "agent-framework", "lightweight"]
      },
      {
        "name": "langgraph",
        "tags": ["ai", "ml", "llm", "python", "agent-framework", "workflow"]
      },
      {
        "name": "autogen",
        "tags": ["ai", "ml", "llm", "python", "agent-framework", "multi-agent"]
      },
      {
        "name": "llamaindex-js",
        "tags": ["ai", "ml", "llm", "javascript", "rag"]
      },
      {
        "name": "langchain-js",
        "tags": ["ai", "ml", "llm", "javascript"]
      },
      {
        "name": "asp-net",
        "tags": ["backend", "framework", "csharp", "microsoft"]
      },
      {
        "name": "aws-amplify",
        "tags": ["cloud", "frontend", "aws", "full-stack"]
      },
      {
        "name": "aws-cli",
        "tags": ["cloud", "devops", "aws", "command-line"]
      },
      {
        "name": "aws-dynamodb",
        "tags": ["database", "nosql", "aws", "cloud"]
      },
      {
        "name": "aws-ecs",
        "tags": ["cloud", "containerization", "aws", "orchestration"]
      },
      {
        "name": "aws-rds",
        "tags": ["database", "sql", "aws", "cloud"]
      },
      {
        "name": "amazon-ec2",
        "tags": ["cloud", "infrastructure", "aws", "virtual-machines"]
      },
      {
        "name": "amazon-s3",
        "tags": ["cloud", "storage", "aws", "object-storage"]
      },
      {
        "name": "android-sdk",
        "tags": ["mobile", "framework", "java", "kotlin"]
      },
      {
        "name": "ansible",
        "tags": ["devops", "infrastructure", "automation", "configuration-management"]
      },
      {
        "name": "ant-design",
        "tags": ["ui", "component-library", "react", "design-system"]
      },
      {
        "name": "apollo-graphql",
        "tags": ["web", "api", "graphql", "javascript"]
      },
      {
        "name": "astro",
        "tags": ["frontend", "framework", "javascript", "static-site"]
      },
      {
        "name": "auth0",
        "tags": ["authentication", "security", "identity", "saas"]
      },
      {
        "name": "azure-pipelines",
        "tags": ["devops", "ci-cd", "microsoft", "cloud"]
      },
      {
        "name": "bash",
        "tags": ["shell", "scripting", "unix", "command-line"]
      },
      {
        "name": "boto3",
        "tags": ["cloud", "aws", "python", "sdk"]
      },
      {
        "name": "c-sharp",
        "tags": ["language", "microsoft", "dotnet", "backend"]
      },
      {
        "name": "cheerio",
        "tags": ["web-scraping", "javascript", "html-parsing", "nodejs"]
      },
      {
        "name": "circleci",
        "tags": ["devops", "ci-cd", "cloud", "automation"]
      },
      {
        "name": "clerk",
        "tags": ["authentication", "security", "identity", "saas"]
      },
      {
        "name": "codemirror",
        "tags": ["editor", "javascript", "text-editor", "code-editor"]
      },
      {
        "name": "cypress",
        "tags": ["testing", "e2e", "javascript", "browser"]
      },
      {
        "name": "d3",
        "tags": ["data-visualization", "javascript", "svg", "charts"]
      },
      {
        "name": "datadog",
        "tags": ["monitoring", "observability", "devops", "cloud"]
      },
      {
        "name": "deno",
        "tags": ["javascript", "runtime", "typescript", "nodejs-alternative"]
      },
      {
        "name": "digitalocean",
        "tags": ["cloud", "hosting", "infrastructure", "paas"]
      },
      {
        "name": "discord-api",
        "tags": ["api", "messaging", "gaming", "communication"]
      },
      {
        "name": "django-rest-framework",
        "tags": ["api", "python", "django", "rest"]
      },
      {
        "name": "drizzle",
        "tags": ["database", "orm", "typescript", "sql"]
      },
      {
        "name": "elk-stack",
        "tags": ["logging", "monitoring", "search", "analytics"]
      },
      {
        "name": "esbuild",
        "tags": ["build-tool", "javascript", "bundler", "performance"]
      },
      {
        "name": "eslint",
        "tags": ["linting", "javascript", "static-analysis", "code-quality"]
      },
      {
        "name": "elasticsearch",
        "tags": ["search", "database", "full-text", "analytics"]
      },
      {
        "name": "emacs",
        "tags": ["editor", "text-editor", "lisp", "extensible"]
      },
      {
        "name": "ffmpeg",
        "tags": ["multimedia", "video", "audio", "conversion"]
      },
      {
        "name": "fabric-js",
        "tags": ["canvas", "graphics", "javascript", "interactive"]
      },
      {
        "name": "firebase",
        "tags": ["backend-as-service", "database", "authentication", "google"]
      },
      {
        "name": "fontawesome",
        "tags": ["icons", "ui", "web", "design"]
      },
      {
        "name": "gcp-cli",
        "tags": ["cloud", "google", "command-line", "devops"]
      },
      {
        "name": "gitlab-ci",
        "tags": ["devops", "ci-cd", "automation", "git"]
      },
      {
        "name": "go",
        "tags": ["language", "backend", "performance", "google"]
      },
      {
        "name": "godot",
        "tags": ["game-development", "engine", "cross-platform", "open-source"]
      },
      {
        "name": "google-maps-js",
        "tags": ["maps", "geolocation", "javascript", "api"]
      },
      {
        "name": "gradle",
        "tags": ["build-tool", "java", "android", "automation"]
      },
      {
        "name": "grafana",
        "tags": ["monitoring", "visualization", "dashboards", "observability"]
      },
      {
        "name": "heroku",
        "tags": ["cloud", "paas", "hosting", "deployment"]
      },
      {
        "name": "insomnia",
        "tags": ["api", "testing", "development", "http-client"]
      },
      {
        "name": "ionic",
        "tags": ["mobile", "framework", "cross-platform", "javascript"]
      },
      {
        "name": "jax",
        "tags": ["ai", "ml", "numerical-computing", "python"]
      },
      {
        "name": "junit",
        "tags": ["testing", "java", "unit-testing", "framework"]
      },
      {
        "name": "java",
        "tags": ["language", "backend", "enterprise", "jvm"]
      },
      {
        "name": "jenkins",
        "tags": ["devops", "ci-cd", "automation", "build"]
      },
      {
        "name": "jquery",
        "tags": ["javascript", "dom", "library", "frontend"]
      },
      {
        "name": "llvm",
        "tags": ["compiler", "infrastructure", "optimization", "toolchain"]
      },
      {
        "name": "mlx",
        "tags": ["ai", "ml", "apple", "deep-learning"]
      },
      {
        "name": "maven",
        "tags": ["build-tool", "java", "dependency-management", "project-management"]
      },
      {
        "name": "microsoft-teams",
        "tags": ["collaboration", "communication", "microsoft", "enterprise"]
      },
      {
        "name": "mockito",
        "tags": ["testing", "java", "mocking", "unit-testing"]
      },
      {
        "name": "neo4j",
        "tags": ["database", "graph", "nosql", "relationships"]
      },
      {
        "name": "netlify",
        "tags": ["hosting", "deployment", "jamstack", "frontend"]
      },
      {
        "name": "nginx",
        "tags": ["web-server", "proxy", "load-balancer", "performance"]
      },
      {
        "name": "notion-api",
        "tags": ["api", "productivity", "collaboration", "integration"]
      },
      {
        "name": "openai",
        "tags": ["ai", "ml", "llm", "api"]
      },
      {
        "name": "php",
        "tags": ["language", "backend", "web", "server-side"]
      },
      {
        "name": "postman",
        "tags": ["api", "testing", "development", "http-client"]
      },
      {
        "name": "puppeteer",
        "tags": ["web-scraping", "browser-automation", "testing", "javascript"]
      },
      {
        "name": "ros",
        "tags": ["robotics", "framework", "middleware", "distributed-systems"]
      },
      {
        "name": "railway",
        "tags": ["hosting", "deployment", "paas", "devops"]
      },
      {
        "name": "remix",
        "tags": ["frontend", "framework", "react", "javascript"]
      },
      {
        "name": "ruby",
        "tags": ["language", "backend", "web", "scripting"]
      },
      {
        "name": "rust",
        "tags": ["language", "systems", "performance", "safety"]
      },
      {
        "name": "sqlite",
        "tags": ["database", "sql", "embedded", "lightweight"]
      },
      {
        "name": "sentry",
        "tags": ["error-tracking", "monitoring", "debugging", "observability"]
      },
      {
        "name": "socket-io",
        "tags": ["websockets", "real-time", "javascript", "communication"]
      },
      {
        "name": "spring",
        "tags": ["backend", "framework", "java", "enterprise"]
      },
      {
        "name": "stripe",
        "tags": ["payments", "api", "e-commerce", "financial"]
      },
      {
        "name": "three-js",
        "tags": ["3d", "graphics", "webgl", "javascript"]
      },
      {
        "name": "tinygrad",
        "tags": ["ai", "ml", "deep-learning", "lightweight"]
      },
      {
        "name": "unity",
        "tags": ["game-development", "engine", "cross-platform", "c-sharp"]
      },
      {
        "name": "unreal-engine",
        "tags": ["game-development", "engine", "cross-platform", "c++"]
      },
      {
        "name": "vim",
        "tags": ["editor", "text-editor", "terminal", "productivity"]
      },
      {
        "name": "zsh",
        "tags": ["shell", "command-line", "unix", "terminal"]
      }
    ]
  }
</file>

</files>
