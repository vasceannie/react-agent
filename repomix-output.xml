This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: src/react_agent/utils/*.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
src/
  react_agent/
    utils/
      __init__.py
      cache.py
      content.py
      defaults.py
      extraction.py
      llm.py
      logging.py
      statistics.py
      validations.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="src/react_agent/utils/__init__.py">
from react_agent.utils.validations import is_valid_url
from react_agent.utils.logging import (
    get_logger,
    log_dict,
    info_highlight,
    warning_highlight,
    error_highlight,
    log_step
)

__all__ = [
    "is_valid_url",
    "get_logger",
    "log_dict",
    "info_highlight",
    "warning_highlight",
    "error_highlight",
    "log_step"
]
</file>

<file path="src/react_agent/utils/cache.py">
"""Type-safe caching and checkpointing utilities.

This module provides a unified interface for caching and checkpointing with LangGraph,
ensuring type safety and consistent behavior across the application.

Examples:
    Basic usage with simple function:
    >>> @cache_result(ttl=600)
    >>> def compute(a: int, b: int) -> int:
    >>>     return a + b
    >>> result = compute(3, 4)  # Returns 7, either from cache or computed

    Using the ProcessorCache directly:
    >>> cache = ProcessorCache(thread_id="test-thread")
    >>> cache.put("user:123", {"name": "John", "age": 30}, ttl=3600)
    >>> user_data = cache.get("user:123")
    >>> print(user_data)  # Output: {'name': 'John', 'age': 30}

    Cache statistics example:
    >>> cache = ProcessorCache()
    >>> cache.get("missing_key")  # Returns None (cache miss)
    >>> cache.cache_hits  # Returns 0
    >>> cache.cache_misses  # Returns 1
"""

from __future__ import annotations

import hashlib
import json
import time
from datetime import UTC, datetime
from functools import wraps
from typing import Any, Callable, Dict, TypeVar, cast

from langchain_core.runnables import RunnableConfig
from langgraph.checkpoint.base import Checkpoint
from langgraph.checkpoint.memory import MemorySaver
from typing_extensions import TypedDict

from .logging import get_logger, log_performance_metrics

# Get module logger
logger = get_logger(__name__)

# Generic type variables for strict type safety
T = TypeVar('T')
R = TypeVar('R')


# Use TypedDict for better type safety
class CacheEntryData(TypedDict):
    """Data structure for individual cache entries.

    Example:
        {
            "data": {"product": "Widget", "price": 19.99},  # The actual cached data
            "timestamp": "2023-10-15T14:30:00.000000+00:00",  # ISO format timestamp
            "ttl": 3600,  # Time-to-live in seconds
            "version": 1,  # Cache version
            "metadata": {"source": "API", "tags": ["popular"]}  # Optional metadata
        }
    """
    data: Any
    timestamp: str
    ttl: int
    version: int
    metadata: Dict[str, Any]


class CacheState(TypedDict):
    """State representation for the cache system."""
    entries: Dict[str, CacheEntryData]
    stats: Dict[str, int]


class CheckpointMetadataDict(TypedDict):
    """Metadata structure for checkpoints."""
    source: str
    function: str | None
    ttl: int


class ProcessorCache:
    """A type-safe processor cache utilizing LangGraph checkpointing for persistent storage.

    Examples:
        Initialization:
        >>> cache = ProcessorCache(thread_id="user-session-123", version=2)

        Basic operations:
        >>> cache.put("product:456", {"name": "Gadget", "stock": 42}, ttl=300)
        >>> item = cache.get("product:456")
        >>> print(item)  # Output: {'name': 'Gadget', 'stock': 42}

        With metadata:
        >>> cache.put(
        ...     "config:app",
        ...     {"theme": "dark", "locale": "en_US"},
        ...     ttl=86400,
        ...     metadata={"updated_by": "system"}
        ... )
    """
    
    def __init__(self, thread_id: str = "default-processor", version: int = 1) -> None:
        """Initialize the cache with a specific thread ID and version."""
        self.memory_saver = MemorySaver()
        self.thread_id = thread_id
        self.version = version
        self.cache_hits = 0
        self.cache_misses = 0
        # In-memory cache for fallback
        self.memory_cache: Dict[str, CacheEntryData] = {}
        logger.info(f"Initialized ProcessorCache with thread_id={thread_id}, version={version}")
    
    def _get_config(self, checkpoint_id: str) -> RunnableConfig:
        """Create a configuration object for checkpoint operations."""
        return RunnableConfig(configurable={
            "thread_id": self.thread_id,
            "checkpoint_id": checkpoint_id,
        })
    
    def get(self, key: str) -> Any | None:
        """Retrieve data from the cache for a given key.

        Args:
            key: The unique identifier for the cached data

        Returns:
            The cached data if found and valid, otherwise None

        Examples:
            >>> cache.get("user:789")
            {'name': 'Alice', 'email': 'alice@example.com'}

            >>> cache.get("nonexistent_key")
            None
        """
        start_time = time.time()
        
        # First check memory cache for fallback
        if key in self.memory_cache:
            logger.info(f"Cache hit from memory for key: {key}")
            self.cache_hits += 1
            return self.memory_cache[key]["data"]
        
        try:
            # Use LangGraph checkpoint system to retrieve data
            checkpoint = self.memory_saver.get(self._get_config(key))
            
            if checkpoint is not None and (isinstance(checkpoint, dict) and "values" in checkpoint):
                cached_entry = checkpoint["values"].get("entry")
                if cached_entry and self._is_cache_valid(cached_entry):
                    self.cache_hits += 1
                    logger.info(f"Cache hit from checkpoint for key: {key}")
                    return cached_entry["data"]
            
            self.cache_misses += 1
            logger.info(f"Cache miss for key: {key}")
            return None
        except Exception as e:
            logger.error(f"Error retrieving from cache: {str(e)}", exc_info=True)
            return None
        finally:
            end_time = time.time()
            log_performance_metrics("Cache retrieval", start_time, end_time, category="Cache")
    
    def put(
        self,
        key: str,
        data: Any,
        ttl: int = 3600,
        metadata: Dict[str, Any] | None = None
    ) -> None:
        """Store data in the cache under the provided key.

        Args:
            key: Unique identifier for the data
            data: The data to be cached (any serializable type)
            ttl: Time-to-live in seconds (default: 3600)
            metadata: Optional dictionary of metadata (default: None)

        Examples:
            >>> cache.put("session:abc123", {"user_id": 42, "logged_in": True}, ttl=1800)

            With metadata:
            >>> cache.put(
            ...     "report:q3",
            ...     {"revenue": 1_000_000, "growth": 0.15},
            ...     ttl=86400,
            ...     metadata={"generated_by": "report_service", "format": "v2"}
            ... )
        """
        start_time = time.time()

        # Create cache entry
        entry: CacheEntryData = {
            "data": data,
            "timestamp": datetime.now(UTC).isoformat(),
            "ttl": ttl,
            "version": self.version,
            "metadata": metadata or {}
        }

        # Store in memory cache for fallback
        self.memory_cache[key] = entry

        try:
            # Create checkpoint using LangGraph patterns
            values = {"entry": entry}

            # Use LangGraph's checkpoint system
            checkpoint_data: Dict[str, Any] = {
                "id": key,
                "ts": datetime.now(UTC).isoformat(),
                "v": self.version,
                "channel_values": values,
                "channel_versions": {k: self.version for k in values},
                "versions_seen": {self.thread_id: self.version},
                "pending_sends": [],
                "metadata": metadata or {},
            }

            checkpoint = Checkpoint(**checkpoint_data)

            self.memory_saver.put(
                self._get_config(key),
                checkpoint,
                metadata={
                    "source": "input",
                    "step": self.version,
                    "writes": metadata or {},
                    "parents": {},
                },
                new_versions={k: self.version for k in values},
            )

            logger.info(f"Stored data in cache for key: {key}")
        except Exception as e:
            logger.error(f"Error saving to checkpoint system: {str(e)}", exc_info=True)
            logger.warning("Falling back to memory cache only")
        finally:
            end_time = time.time()
            log_performance_metrics("Cache storage", start_time, end_time, category="Cache")
    
    def cache_result(
        self,
        ttl: int = 3600
    ) -> Callable[[Callable[..., R]], Callable[..., R]]:
        """Decorate a function to cache its results with type preservation.

        Args:
            ttl: Time-to-live in seconds for cached results (default: 3600)

        Returns:
            A decorated function that caches its results

        Examples:
            Basic usage:
            >>> @cache.cache_result(ttl=300)
            >>> def get_user_details(user_id: int) -> dict:
            >>>     # Expensive database call here
            >>>     return {"id": user_id, "name": "John"}

            First call (executes function):
            >>> get_user_details(42)  # Returns {'id': 42, 'name': 'John'}

            Subsequent call (returns cached result):
            >>> get_user_details(42)  # Returns cached result immediately
        """
        def decorator(func: Callable[..., R]) -> Callable[..., R]:
            @wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> R:
                start_time = time.time()
                cache_key = self._generate_cache_key(func, args, kwargs)
                
                # Try to get from cache
                cached_result = self.get(cache_key)
                if cached_result is not None:
                    end_time = time.time()
                    log_performance_metrics(f"Cache hit for {func.__name__}", start_time, end_time, category="Cache")
                    return cast(R, cached_result)
                
                # Execute function and cache result
                result = func(*args, **kwargs)
                
                self.put(
                    cache_key,
                    result,
                    ttl=ttl,
                    metadata={"function": func.__name__}
                )
                
                end_time = time.time()
                log_performance_metrics(f"Cache miss for {func.__name__}", start_time, end_time, category="Cache")
                return result
            
            return wrapper
        return decorator
    
    def _generate_cache_key(self, func: Callable, args: tuple, kwargs: dict) -> str:
        """Generate a unique cache key from function and arguments.
        
        Creates a deterministic hash key based on:
        - Function name or ID
        - Arguments (positional and keyword)
        - Cache version
        
        Args:
            func: The function being cached
            args: Positional arguments passed to the function
            kwargs: Keyword arguments passed to the function
            
        Returns:
            A SHA256 hex digest string representing the unique cache key
            
        Examples:
            >>> def example(a: int, b: int = 2) -> int:
            ...     return a + b
            >>> cache._generate_cache_key(example, (1,), {})
            'a1b2c3...'  # SHA256 hash
            
            >>> cache._generate_cache_key(example, (1,), {'b': 3})
            'd4e5f6...'  # Different hash due to changed arguments
            
            With complex objects:
            >>> cache._generate_cache_key(example, ([1,2],), {'b': {'x': 1}})
            'g7h8i9...'  # Hash based on string representation
        """
        try:
            func_name = func.__name__
        except (AttributeError, TypeError):
            func_name = id(func)

        # Handle complex objects in args
        args_str = [str(arg) if isinstance(arg, (list, dict, set)) else arg for arg in args]
        cache_data = {
            'args': str(args_str),
            'kwargs': str(kwargs),
            'func': func_name,
            'version': self.version
        }
        cache_str = json.dumps(cache_data, sort_keys=True, default=str)
        return hashlib.sha256(cache_str.encode()).hexdigest()
    
    def _is_cache_valid(self, cached: Dict[str, Any]) -> bool:
        """Validate if a cached entry is still fresh based on TTL.
        
        Checks:
        - Presence of required timestamp field
        - Valid ISO format timestamp
        - Whether current time is within TTL window
        
        Args:
            cached: The cache entry dictionary containing:
                - timestamp: ISO format string
                - ttl: Time-to-live in seconds
                
        Returns:
            True if cache entry is valid and fresh, False otherwise
            
        Examples:
            Valid case:
            >>> entry = {
            ...     "timestamp": "2023-10-15T14:30:00.000000+00:00",
            ...     "ttl": 3600,
            ...     "data": {...}
            ... }
            >>> cache._is_cache_valid(entry)  # True if within 1 hour of timestamp
            
            Expired case:
            >>> old_entry = {
            ...     "timestamp": "2023-10-01T00:00:00.000000+00:00", 
            ...     "ttl": 3600,
            ...     "data": {...}
            ... }
            >>> cache._is_cache_valid(old_entry)  # False
            
            Invalid format:
            >>> bad_entry = {
            ...     "timestamp": "invalid-date",
            ...     "ttl": 3600
            ... }
            >>> cache._is_cache_valid(bad_entry)  # False
        """
        timestamp_str = cached.get("timestamp", "")
        if not timestamp_str:
            return False
        
        try:
            timestamp = datetime.fromisoformat(timestamp_str)
            elapsed = (datetime.now(UTC) - timestamp).total_seconds()
            return elapsed < cached.get("ttl", 3600)
        except ValueError:
            return False
</file>

<file path="src/react_agent/utils/content.py">
"""Content processing utilities for the research agent.

This module provides utilities for processing and validating content,
including chunking, preprocessing, content type detection, and merging extraction results.

Examples:
    >>> text = "This is a sample text that will be split into chunks."
    >>> chunks = chunk_text(text, chunk_size=20, overlap=5)
    >>> print(chunks)
    ['This is a sample', 'sample text that', 'that will be split', 'split into chunks.']
    
    >>> content_type = detect_content_type("page.html", "<html><body>Content</body></html>")
    >>> print(content_type)
    html
    
    >>> valid = validate_content("This is valid content")
    >>> print(valid)
    True
"""
import contextlib
import json
import re
import time
from typing import Any, Dict, List, Set, TypedDict
from urllib.parse import unquote, urlparse

from react_agent.utils.cache import ProcessorCache
from react_agent.utils.defaults import (
    ChunkConfig,
    get_category_merge_mapping,
    get_default_extraction_result,
)
from react_agent.utils.logging import (
    error_highlight,
    get_logger,
    info_highlight,
    log_performance_metrics,
    log_progress,
    warning_highlight,
)

# Initialize logger
logger = get_logger(__name__)

# Initialize cache
content_cache = ProcessorCache(thread_id="content")


class ContentState(TypedDict):
    """Typed dictionary for content state used in the graph.

    Attributes:
        content (str): The actual text content.
        url (str): Source URL of the content.
        content_type (str): Type of the content (e.g., 'html', 'json', 'text').
        chunks (List[str]): List of text chunks.
        metadata (Dict[str, Any]): Additional metadata about the content.
        timestamp (str): ISO formatted timestamp when the content was processed.
    """
    content: str
    url: str
    content_type: str
    chunks: List[str]
    metadata: Dict[str, Any]
    timestamp: str


# Constants for chunking and token estimation.
DEFAULT_CHUNK_SIZE: int = 40000
DEFAULT_OVERLAP: int = 5000
MAX_CONTENT_LENGTH: int = 100000
TOKEN_CHAR_RATIO: float = 4.0

# Problematic content patterns to skip certain file types.
PROBLEMATIC_PATTERNS: List[str] = [
    r'\.pdf(\?|$)',  # Catch PDF URLs with query params.
    r'\.docx?$',
    r'\.xlsx?$',
    r'\.ppt$',
    r'\.zip$',
    r'\.rar$',
    r'\.exe$',
    r'\.dmg$',
    r'\.iso$',
    r'\.tar$',
    r'\.gz$'
]

# Known problematic sites to avoid.
PROBLEMATIC_SITES: List[str] = [
    'iaeme.com',
    'scribd.com',
    'slideshare.net',
    'academia.edu'
]


@content_cache.cache_result(ttl=3600)
def chunk_text(
    text: str,
    chunk_size: int | None = None,
    overlap: int | None = None,
    use_large_chunks: bool = False,
    min_chunk_size: int = 100
) -> List[str]:
    """Split text into overlapping chunks, returning a list of chunks.

    Args:
        text (str): The text to be split.
        chunk_size (Optional[int]): Desired size for each chunk. If None, defaults are taken from ChunkConfig.
        overlap (Optional[int]): Overlap length between chunks. If None, defaults are taken from ChunkConfig.
        use_large_chunks (bool): Flag to indicate if larger chunk sizes should be used.
        min_chunk_size (int): Minimum acceptable chunk size to avoid very small chunks.

    Returns:
        List[str]: A list of text chunks.

    Examples:
        >>> text = "This is a sample text that needs to be chunked into smaller pieces."
        >>> chunk_text(text, chunk_size=20, overlap=5)
        ['This is a sample', 'sample text that', 'that needs to be', 'be chunked into', 'into smaller pieces.']
        
        >>> # Using large chunks
        >>> chunk_text(text, use_large_chunks=True)
        ['This is a sample text that needs to be chunked into smaller pieces.']
        
        >>> # Empty input returns an empty list
        >>> chunk_text("")
        []
    """
    if not text or text.isspace():
        warning_highlight("Empty or whitespace-only text provided")
        return []

    # Set chunk parameters based on defaults if not provided.
    chunk_size = max(min_chunk_size, chunk_size or (
        ChunkConfig.LARGE_CHUNK_SIZE if use_large_chunks 
        else ChunkConfig.DEFAULT_CHUNK_SIZE
    ))
    overlap = min(chunk_size - 1, max(0, overlap or (
        ChunkConfig.LARGE_OVERLAP if use_large_chunks 
        else ChunkConfig.DEFAULT_OVERLAP
    )))

    chunks: List[str] = []
    start = 0
    text_length = len(text)
    
    # Track chunking performance
    import time
    start_time = time.time()

    while start < text_length:
        end = min(start + chunk_size, text_length)
        # Attempt to find a natural break point.
        if end < text_length:
            end = text.rfind(' ', start + min_chunk_size, end) or end

        if chunk := text[start:end].strip():
            if chunks and len(chunk) < min_chunk_size:
                chunks[-1] = f"{chunks[-1]} {chunk}"
            else:
                chunks.append(chunk)
                # Log progress every 5 chunks
                if len(chunks) % 5 == 0:
                    log_progress(len(chunks), text_length // chunk_size + 1, "chunking", "Creating chunks")
        start = end - overlap

    end_time = time.time()
    log_performance_metrics(
        "Text chunking", 
        start_time, 
        end_time, 
        "chunking",
        {"text_length": text_length, "chunks_created": len(chunks), "avg_chunk_size": text_length / max(1, len(chunks))}
    )
    
    info_highlight(f"Created {len(chunks)} chunks", category="chunking")
    return chunks


def detect_html(content: str) -> str | None:
    """Determine whether the provided content is HTML.

    Args:
        content (str): The content string to be analyzed.

    Returns:
        Optional[str]: Returns 'html' if HTML is detected; otherwise, returns None.

    Examples:
        >>> detect_html("<html><body>Hello</body></html>")
        'html'
        
        >>> detect_html("<!doctype html><div>Content</div>")
        'html'
        
        >>> detect_html("Plain text content")
        None
    """
    if not content:
        return None
    content_lower = content.strip().lower()
    if content_lower.startswith('<!doctype html') or content_lower.startswith('<html'):
        return 'html'
    if '<body' in content_lower and '</body>' in content_lower:
        return 'html'
    if '<div' in content_lower and '</div>' in content_lower:
        return 'html'
    return None


def detect_json(content: str) -> str | None:
    """Determine whether the provided content is valid JSON.

    Args:
        content (str): The content string to analyze.

    Returns:
        Optional[str]: Returns 'json' if the content is valid JSON; otherwise, returns None.

    Examples:
        >>> detect_json('{"key": "value"}')
        'json'
        
        >>> detect_json('[1, 2, 3]')
        'json'
        
        >>> detect_json('Invalid content')
        None
    """
    if not content:
        return None
    content = content.strip()
    if (content.startswith('{') and content.endswith('}')) or (content.startswith('[') and content.endswith(']')):
        with contextlib.suppress(json.JSONDecodeError):
            json.loads(content)
            return 'json'
    return None


def detect_from_url_extension(url: str) -> str | None:
    """Infer content type based on the file extension in the URL.

    Args:
        url (str): The URL to analyze.

    Returns:
        Optional[str]: The inferred content type (e.g., 'pdf', 'html', 'json') based on the extension;
                       None if the extension is not recognized.

    Examples:
        >>> detect_from_url_extension('document.pdf')
        'pdf'
        
        >>> detect_from_url_extension('page.html')
        'html'
        
        >>> detect_from_url_extension('data.json')
        'json'
        
        >>> detect_from_url_extension('noextension')
        None
    """
    if not url:
        return None
        
    extensions = {
        '.pdf': 'pdf', '.doc': 'doc', '.docx': 'doc', '.xls': 'excel', '.xlsx': 'excel',
        '.ppt': 'presentation', '.pptx': 'presentation', '.txt': 'text', '.md': 'text',
        '.rst': 'text', '.html': 'html', '.htm': 'html', '.json': 'json', '.xml': 'xml',
        '.csv': 'data'
    }
    
    try:
        ext = f".{url.lower().split('.')[-1]}"
        return extensions.get(ext)
    except IndexError:
        return None


def detect_from_url_path(url: str) -> str | None:
    """Infer content type from common URL path patterns.

    Args:
        url (str): The URL to be analyzed.

    Returns:
        Optional[str]: Returns 'html' if known HTML path patterns are found; otherwise, None.

    Examples:
        >>> detect_from_url_path('https://example.com/wiki/Article_Title')
        'html'
        
        >>> detect_from_url_path('https://example.com/api/data')
        None
    """
    if not url:
        return None
        
    html_path_patterns = (
        '/wiki/', '/articles/', '/blog/', '/news/',
        '/docs/', '/help/', '/support/', '/pages/',
        '/product/', '/service/', '/consumers/',
        '/detail/', '/view/', '/content/'
    )
    return 'html' if any(pattern in url.lower() for pattern in html_path_patterns) else None


def detect_from_content_heuristics(content: str) -> str | None:
    r"""Infer content type based on heuristics applied directly to the content.

    Args:
        content (str): The text content to analyze.

    Returns:
        Optional[str]: Returns 'xml', 'data', or 'text' based on content patterns; otherwise, None.

    Examples:
        >>> detect_from_content_heuristics("<?xml version='1.0'?><data>123</data>")
        'xml'
        
        >>> detect_from_content_heuristics('{"key": "value"}')
        'data'
        
        >>> detect_from_content_heuristics("This is a simple text with multiple paragraphs.\n\nNew paragraph.")
        'text'
    """
    if not content or len(content) < 50:
        return None

    content = content.strip()
    if content.startswith('<?xml') or (content.startswith('<') and '>' in content):
        return 'xml'
    if '{' in content and '}' in content and '"' in content and ':' in content:
        return 'data'
    return 'text' if '\n\n' in content and len(content) > 200 else None


def detect_from_url_domain(url: str) -> str | None:
    """Infer content type based on URL domain characteristics.

    Args:
        url (str): The URL to analyze.

    Returns:
        Optional[str]: Returns 'html' if the domain indicates a typical web page; otherwise, None.

    Examples:
        >>> detect_from_url_domain('https://www.example.com/path')
        'html'
        
        >>> detect_from_url_domain('ftp://example.org/resource')
        None
    """
    if not url:
        return None
        
    common_web_domains = ('.gov', '.org', '.edu', '.com', '.net', '.io')
    parsed_url = urlparse(url)
    domain = parsed_url.netloc.lower()
    if any(domain.endswith(d) for d in common_web_domains) and (parsed_url.path and '.' not in parsed_url.path.split('/')[-1]):
        return 'html'
    return None


def detect_content_type(url: str, content: str) -> str:
    r"""Determine the content type using multiple detection strategies.

    This function sequentially applies various detectors (HTML, JSON, URL extension/path,
    content heuristics, and domain analysis). If none succeed, a fallback detection is used.

    Args:
        url (str): The URL associated with the content.
        content (str): The content to analyze.

    Returns:
        str: The detected content type (e.g., 'html', 'json', 'text', 'unknown').

    Examples:
        >>> detect_content_type('page.html', '<html><body>Content</body></html>')
        'html'
        
        >>> detect_content_type('data.json', '{"key": "value"}')
        'json'
        
        >>> detect_content_type('article', 'Plain text content\n\nMore content')
        'text'
        
        >>> detect_content_type('', '')
        'unknown'
    """
    info_highlight(f"Detecting content type for URL: {url}", category="content_type")
    
    detector_functions = [
        (detect_html, content),
        (detect_json, content),
        (detect_from_url_extension, url),
        (detect_from_url_path, url),
        (detect_from_content_heuristics, content),
        (detect_from_url_domain, url)
    ]

    for detector, arg in detector_functions:
        if result := detector(arg):
            info_highlight(f"Detected content type: {result}", category="content_type")
            return result

    # Fallback detection if no other detectors return a type.
    result = fallback_detection(url, content)
    info_highlight(f"Using fallback detection, type: {result}", category="content_type")
    return result


def fallback_detection(url: str, content: str) -> str:
    """Fallback detection logic for content type.

    Args:
        url (str): The URL to analyze.
        content (str): The content to analyze.

    Returns:
        str: Returns 'text' if content is non-empty; if URL indicates a web resource, returns 'html';
             otherwise returns 'unknown'.

    Examples:
        >>> fallback_detection("http://example.com", "Some text content")
        'text'
        
        >>> fallback_detection("", "")
        'unknown'
    """
    if content and content.strip():
        return 'text'
    return 'html' if url and url.startswith(('http://', 'https://')) else 'unknown'


def preprocess_content(content: str, url: str) -> str:
    r"""Clean and preprocess content prior to further processing or model ingestion.

    This includes removing boilerplate text, redundant whitespace, site-specific cleaning,
    and truncating overly long content. Results are cached for performance.

    Args:
        content (str): The raw content string to be preprocessed.
        url (str): The URL of the content (used for site-specific rules and caching).

    Returns:
        str: The cleaned and preprocessed content.

    Examples:
        >>> content = "Copyright 2024 Example Corp. All rights reserved.\\nActual content here"
        >>> preprocess_content(content, "example.com")
        'Actual content here'
        
        >>> content = "Please enable JavaScript to continue.\\nImportant content"
        >>> preprocess_content(content, "example.com")
        'Important content'
        
        >>> preprocess_content("", "example.com")
        ''
    """
    if not content:
        warning_highlight("Empty content provided", category="preprocessing")
        return ""

    info_highlight(f"Preprocessing content from {url}", category="preprocessing")
    info_highlight(f"Initial content length: {len(content)}", category="preprocessing")
    
    # Track preprocessing performance
    import time
    start_time = time.time()

    # Generate a cache key based on content and URL.
    cache_key = f"preprocess_content_{hash(f'{content}_{url}')}"
    
    # Retrieve cached content if available and not expired.
    cached_result = content_cache.get(cache_key)
    if cached_result and isinstance(cached_result, dict):
        log_performance_metrics(
            "Content preprocessing (cached)", 
            start_time, 
            time.time(), 
            "preprocessing",
            {"content_length": len(cached_result.get("content", "")), "cache_hit": True}
        )
        return cached_result.get("content", "")

    # Define boilerplate removal regex patterns.
    boilerplate_patterns = [
        (re.compile(r'Copyright \d{4}.*?reserved\.', re.IGNORECASE | re.DOTALL), ''),
        (re.compile(r'Terms of Service.*?Privacy Policy', re.IGNORECASE | re.DOTALL), ''),
        (re.compile(r'Please enable JavaScript.*?continue', re.IGNORECASE | re.DOTALL), '')
    ]
    
    # Log progress for preprocessing steps
    total_steps = 4  # boilerplate removal, whitespace normalization, site-specific, truncation
    current_step = 0
    
    # Remove boilerplate text.
    for pattern, replacement in boilerplate_patterns:
        content = pattern.sub(replacement, content)
    current_step += 1
    log_progress(current_step, total_steps, "preprocessing", "Cleaning content")

    # Normalize whitespace.
    content = ' '.join(content.split())
    current_step += 1
    log_progress(current_step, total_steps, "preprocessing", "Cleaning content")

    # Site-specific cleaning (e.g., for iaeme.com).
    domain = urlparse(url).netloc.lower()
    if 'iaeme.com' in domain:
        iaeme_pattern = re.compile(r'International Journal.*?Indexing', re.IGNORECASE | re.DOTALL)
        content = iaeme_pattern.sub('', content)
    current_step += 1
    log_progress(current_step, total_steps, "preprocessing", "Cleaning content")

    # Truncate content if it exceeds maximum length.
    original_length = len(content)
    if original_length > MAX_CONTENT_LENGTH:
        warning_highlight(
            f"Content exceeds {MAX_CONTENT_LENGTH} characters, truncating",
            category="preprocessing"
        )
        content = f"{content[:MAX_CONTENT_LENGTH]}..."
    current_step += 1
    log_progress(current_step, total_steps, "preprocessing", "Cleaning content")

    # Cache the preprocessed content.
    content_cache.put(
        cache_key,
        {"content": content},
        ttl=3600  # 1 hour TTL
    )
    
    end_time = time.time()
    log_performance_metrics(
        "Content preprocessing", 
        start_time, 
        end_time, 
        "preprocessing",
        {
            "original_length": original_length, 
            "final_length": len(content), 
            "reduction_percent": round((1 - len(content) / max(1, original_length)) * 100, 2),
            "cache_hit": False
        }
    )
    
    info_highlight(f"Final content length: {len(content)}", category="preprocessing")
    return content


def estimate_tokens(text: str) -> int:
    """Estimate the number of tokens in a text based on a fixed character-to-token ratio.

    Args:
        text (str): The text whose tokens are to be estimated.

    Returns:
        int: The estimated token count.

    Examples:
        >>> # Assuming TOKEN_CHAR_RATIO = 4.0
        >>> estimate_tokens("This is a test string")
        5  # Approximately 20 characters / 4.0
        
        >>> estimate_tokens("")
        0
        
        >>> estimate_tokens("Short")
        1  # Approximately 5 characters / 4.0
    """
    return int(len(text) / TOKEN_CHAR_RATIO) if text else 0


def should_skip_content(url: str) -> bool:
    """Determine if the content from a given URL should be skipped based on certain rules.

    The function checks for problematic file extensions, MIME type patterns, and known problematic sites.

    Args:
        url (str): The URL to evaluate.

    Returns:
        bool: True if the content should be skipped; False otherwise.

    Examples:
        >>> should_skip_content("http://example.com/document.pdf")
        True
        
        >>> should_skip_content("http://example.com/page.html")
        False
        
        >>> should_skip_content("http://scribd.com/document")
        True  # Due to problematic site.
        
        >>> should_skip_content("")
        True
    """
    try:
        decoded_url = unquote(url).lower()
    except Exception as e:
        error_highlight(f"Error decoding URL: {str(e)}", category="validation")
        decoded_url = url.lower()

    # Enhanced PDF detection.
    if any(p in decoded_url for p in ('.pdf', '%2Fpdf', '%3Fpdf')):
        info_highlight(f"Skipping PDF content: {url}", category="validation")
        return True

    # MIME-type pattern detection.
    mime_patterns = [
        r'application/pdf',
        r'application/\w+?pdf',
        r'content-type:.*pdf'
    ]
    if any(re.match(p, decoded_url) for p in mime_patterns):
        info_highlight(f"Skipping PDF MIME-type pattern: {url}", category="validation")
        return True

    if not url:
        return True

    url_lower = url.lower()

    # Check for problematic file types.
    for pattern in PROBLEMATIC_PATTERNS:
        if re.search(pattern, url_lower):
            info_highlight(f"Skipping content with pattern {pattern}: {url}", category="validation")
            return True

    # Check for problematic sites.
    domain = urlparse(url).netloc.lower()
    for site in PROBLEMATIC_SITES:
        if site in domain:
            info_highlight(
                f"Skipping content from problematic site {site}: {url}",
                category="validation"
            )
            return True

    return False


def _merge_field(merged: Dict[str, Any], results: List[Dict[str, Any]], field: str, operation: str, seen_items: set) -> None:
    """Merge a specific field from a list of result dictionaries into the merged dictionary.

    The operation can be:
      - 'extend': Append unique items to a list.
      - 'update': Update dictionary values.
      - 'max' or 'min': Keep the maximum or minimum value.
      - 'avg': Append values to compute an average later.

    Args:
        merged (Dict[str, Any]): The dictionary accumulating merged results.
        results (List[Dict[str, Any]]): The list of result dictionaries from which to merge data.
        field (str): The field key to merge.
        operation (str): The merge operation to perform ('extend', 'update', 'max', 'min', 'avg').
        seen_items (set): A set to track items already merged to avoid duplicates.

    Examples:
        >>> merged = {}
        >>> results = [{'score': 8}, {'score': 9}]
        >>> _merge_field(merged, results, 'score', 'max', set())
        >>> merged
        {'score': 9}
    """
    for result in results:
        if field not in result:
            continue

        value = result[field]
        if operation == "extend":
            merged[field] = merged.get(field, [])
            item_key = json.dumps(value, sort_keys=True)
            if item_key not in seen_items:
                merged[field].append(value)
                seen_items.add(item_key)
        elif operation == "update":
            merged[field] = merged.get(field, {})
            merged[field].update(value)
        elif operation in {"max", "min"}:
            if field not in merged or (operation == "max" and value > merged[field]) or (operation == "min" and value < merged[field]):
                merged[field] = value
        elif operation == "avg":
            merged[field] = merged.get(field, [])
            merged[field].append(value)


def merge_chunk_results(results: List[Dict[str, Any]], category: str, merge_strategy: Dict[str, str] | None = None) -> Dict[str, Any]:
    """Merge multiple chunk extraction results into a single consolidated result.

    The function uses a merge strategy (or a default one based on the category)
    to determine how to merge each field (e.g., 'extend' lists, 'update' dictionaries,
    or compute 'max', 'min', or 'avg' values).

    Args:
        results (List[Dict[str, Any]]): A list of dictionaries containing results from each chunk.
        category (str): The category of the extraction (e.g., 'research', 'summary').
        merge_strategy (Optional[Dict[str, str]]): Optional mapping of field names to merge operations.
            If not provided, a default mapping for the category is used.

    Returns:
        Dict[str, Any]: A single dictionary containing the merged results.

    Examples:
        >>> results = [
        ...     {'findings': ['finding1'], 'score': 5},
        ...     {'findings': ['finding2'], 'score': 8}
        ... ]
        >>> merge_strategy = {'findings': 'extend', 'score': 'max'}
        >>> merge_chunk_results(results, 'research', merge_strategy)
        {'findings': ['finding1', 'finding2'], 'score': 8}
        
        >>> # Using average merge operation
        >>> results = [{'rating': 4.0}, {'rating': 6.0}]
        >>> merge_strategy = {'rating': 'avg'}
        >>> merge_chunk_results(results, 'review', merge_strategy)
        {'rating': 5.0}
    """
    if not results:
        warning_highlight(f"No results to merge for category: {category}")
        return get_default_extraction_result(category)

    start_time = time.time()
    
    info_highlight(f"Merging {len(results)} chunk results for category: {category}")
    
    # Get the default merge strategy for this category if none provided.
    if not merge_strategy:
        merge_strategy = get_category_merge_mapping(category)
        
    merged: Dict[str, Any] = {}
    seen_items: Set[str] = set()
    
    # Track progress of merging
    total_fields = len(merge_strategy) if merge_strategy else 0
    if total_fields == 0 and results:
        # If no merge strategy, count fields in first result
        total_fields = len(results[0].keys())
    
    current_field = 0
    
    # Process each field according to its merge operation.
    for field, operation in merge_strategy.items():
        _merge_field(merged, results, field, operation, seen_items)
        current_field += 1
        if total_fields > 0:
            log_progress(current_field, total_fields, "merging", f"Merging {category} results")
    
    # Process any fields in the results that weren't in the merge strategy.
    for result in results:
        for field in result:
            if field not in merge_strategy and field not in merged:
                # Default to 'extend' for lists, 'update' for dicts, 'max' for numbers.
                if isinstance(result[field], list):
                    _merge_field(merged, results, field, 'extend', seen_items)
                elif isinstance(result[field], dict):
                    _merge_field(merged, results, field, 'update', seen_items)
                elif isinstance(result[field], (int, float)):
                    _merge_field(merged, results, field, 'max', seen_items)
                else:
                    # For other types, just take the first non-None value.
                    if result[field] is not None and field not in merged:
                        merged[field] = result[field]
    
    # Calculate final averages for 'avg' operations.
    for field, operation in merge_strategy.items():
        if operation == 'avg' and isinstance(merged.get(field), list):
            values = merged[field]
            if values:
                merged[field] = sum(values) / len(values)
            else:
                merged[field] = 0.0
    
    end_time = time.time()
    log_performance_metrics(
        f"Merging {category} results", 
        start_time, 
        end_time, 
        "merging",
        {"num_results": len(results), "num_fields": len(merged)}
    )
    
    return merged


def validate_content(content: str) -> bool:
    """Validate that the provided content meets minimum requirements.

    This function checks that the content is a string, is non-empty,
    and meets a minimum length requirement.

    Args:
        content (str): The content string to validate.

    Returns:
        bool: True if the content is valid; False otherwise.

    Examples:
        >>> validate_content("This is valid content")
        True
        
        >>> validate_content("")  # Empty string
        False
        
        >>> validate_content("Hi")  # Too short
        False
        
        >>> validate_content(123)  # Invalid type (non-string)
        False
    """
    if not content or not isinstance(content, str):
        warning_highlight("Invalid content type or empty content", category="validation")
        return False
        
    if len(content) < 10:  # Minimum content length requirement.
        warning_highlight(
            f"Content too short: {len(content)} characters",
            category="validation"
        )
        return False
        
    return True


__all__ = [
    "chunk_text",
    "preprocess_content",
    "estimate_tokens",
    "should_skip_content",
    "merge_chunk_results",
    "validate_content",
    "detect_content_type"
]
</file>

<file path="src/react_agent/utils/defaults.py">
"""Default values and configurations for the research agent.

This module consolidates all default values, configurations, and common structures
used across the research agent to maintain consistency and reduce duplication.
"""

from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass

# Default chunking configurations
@dataclass
class ChunkConfig:
    """Configuration for text chunking operations."""
    DEFAULT_CHUNK_SIZE: int = 4000
    DEFAULT_OVERLAP: int = 500
    LARGE_CHUNK_SIZE: int = 40000
    LARGE_OVERLAP: int = 5000

# Default extraction result structure
DEFAULT_EXTRACTION_RESULTS = {
    "market_dynamics": {
        "extracted_facts": [],
        "market_metrics": {
            "market_size": None,
            "growth_rate": None,
            "forecast_period": None
        },
        "relevance_score": 0.0
    },
    "provider_landscape": {
        "extracted_vendors": [],
        "vendor_relationships": [],
        "relevance_score": 0.0
    },
    "technical_requirements": {
        "extracted_requirements": [],
        "standards": [],
        "relevance_score": 0.0
    },
    "regulatory_landscape": {
        "extracted_regulations": [],
        "compliance_requirements": [],
        "relevance_score": 0.0
    },
    "cost_considerations": {
        "extracted_costs": [],
        "pricing_models": [],
        "relevance_score": 0.0
    },
    "best_practices": {
        "extracted_practices": [],
        "methodologies": [],
        "relevance_score": 0.0
    },
    "implementation_factors": {
        "extracted_factors": [],
        "challenges": [],
        "relevance_score": 0.0
    }
}

# Category-specific merge mappings
CATEGORY_MERGE_MAPPINGS = {
    "market_dynamics": {
        "extracted_facts": "extend",
        "market_metrics": "update"
    },
    "provider_landscape": {
        "extracted_vendors": "extend",
        "vendor_relationships": "extend"
    },
    "technical_requirements": {
        "extracted_requirements": "extend",
        "standards": "extend"
    },
    "regulatory_landscape": {
        "extracted_regulations": "extend",
        "compliance_requirements": "extend"
    },
    "cost_considerations": {
        "extracted_costs": "extend",
        "pricing_models": "extend"
    },
    "best_practices": {
        "extracted_practices": "extend",
        "methodologies": "extend"
    },
    "implementation_factors": {
        "extracted_factors": "extend",
        "challenges": "extend"
    }
}

def get_default_extraction_result(category: str) -> Dict[str, Any]:
    """Get a default empty extraction result when parsing fails.
    
    Args:
        category: Research category
        
    Returns:
        Default empty result dictionary
    """
    return DEFAULT_EXTRACTION_RESULTS.get(category, {"extracted_facts": [], "relevance_score": 0.0})

def get_category_merge_mapping(category: str) -> Dict[str, str]:
    """Get the merge mapping for a specific category.
    
    Args:
        category: Research category
        
    Returns:
        Dictionary mapping field names to merge operations
    """
    return CATEGORY_MERGE_MAPPINGS.get(category, {})

# Export all defaults
__all__ = [
    "ChunkConfig",
    "DEFAULT_EXTRACTION_RESULTS",
    "CATEGORY_MERGE_MAPPINGS",
    "get_default_extraction_result",
    "get_category_merge_mapping"
]
</file>

<file path="src/react_agent/utils/extraction.py">
"""Enhanced extraction module for research categories with statistics focus.

This module improves the extraction of facts and statistics from search results,
with a particular emphasis on numerical data, trends, and statistical information.

Examples:
    Input text example:
        >>> text = '''
        ... According to a recent survey by TechCorp, 75% of enterprises adopted cloud
        ... computing in 2023, up from 60% in 2022. The global cloud market reached
        ... $483.3 billion in revenue, with AWS maintaining a 32% market share.
        ... A separate study by MarketWatch revealed that cybersecurity spending
        ... increased by 15% year-over-year.
        ... '''

    Extracting citations:
        >>> citations = extract_citations(text)
        >>> citations
        [
            {
                "source": "TechCorp",
                "context": "According to a recent survey by TechCorp, 75% of enterprises"
            },
            {
                "source": "MarketWatch",
                "context": "A separate study by MarketWatch revealed that cybersecurity"
            }
        ]

    Extracting statistics:
        >>> stats = extract_statistics(text)
        >>> stats
        [
            {
                "text": "75% of enterprises adopted cloud computing in 2023",
                "type": "percentage",
                "citations": [{"source": "TechCorp", "context": "...survey by TechCorp..."}],
                "year_mentioned": 2023,
                "source_quality": 0.7,
                "quality_score": 0.95,
                "credibility_terms": []
            },
            {
                "text": "global cloud market reached $483.3 billion in revenue",
                "type": "financial",
                "citations": [],
                "quality_score": 0.85,
                "year_mentioned": None
            },
            {
                "text": "AWS maintaining a 32% market share",
                "type": "market",
                "citations": [],
                "quality_score": 0.75,
                "year_mentioned": None
            }
        ]

    Rating statistic quality:
        >>> quality = rate_statistic_quality("According to Gartner's 2023 survey, 78.5% of Fortune 500 companies...")
        >>> quality
        0.95

    Inferring statistic type:
        >>> infer_statistic_type("Market share increased to 45%")
        'percentage'
        >>> infer_statistic_type("$50 million in revenue")
        'financial'

    Extracting year:
        >>> extract_year("In 2023, cloud adoption grew by 25%")
        2023

    Finding JSON objects:
        >>> text_with_json = 'Some text {"key": "value", "nested": {"data": 123}} more text'
        >>> json_obj = find_json_object(text_with_json)
        >>> json_obj
        '{"key": "value", "nested": {"data": 123}}'

    Enriching extracted facts:
        >>> fact = {
        ...     "text": "Cloud adoption grew by 25% in 2023",
        ...     "confidence": 0.8,
        ...     "source_text": "According to AWS, cloud adoption grew by 25% in 2023"
        ... }
        >>> enriched = enrich_extracted_fact(fact, url="https://example.com/report", source_title="Cloud Market Report")
        >>> enriched
        {
            "text": "Cloud adoption grew by 25% in 2023",
            "confidence": 0.8,
            "source_text": "According to AWS, cloud adoption grew by 25% in 2023",
            "source_url": "https://example.com/report",
            "source_title": "Cloud Market Report",
            "source_domain": "example.com",
            "extraction_timestamp": "2024-03-14T10:30:00",
            "statistics": [...],
            "additional_citations": [...],
            "confidence_score": 0.9
        }

    Full category information extraction (asynchronous):
        >>> facts, relevance = await extract_category_information(
        ...     content=text,
        ...     url="https://example.com/cloud-report",
        ...     title="Cloud Computing Trends 2023",
        ...     category="market_dynamics",
        ...     original_query="cloud computing adoption trends",
        ...     prompt_template="Extract facts for {query} from {url}: {content}",
        ...     extraction_model=model
        ... )
        >>> facts
        [
            {
                "type": "fact",
                "data": {
                    "text": "Enterprise cloud adoption increased to 75% in 2023",
                    "source_url": "https://example.com/cloud-report",
                    "source_title": "Cloud Computing Trends 2023",
                    "source_domain": "example.com",
                    "extraction_timestamp": "2024-03-14T10:30:00",
                    "confidence_score": 0.9,
                    "statistics": [...],
                    "additional_citations": [...]
                }
            }
        ]
        >>> relevance
        0.95
"""

import contextlib
import json
import re
import time
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional, Tuple, Union
from urllib.parse import urlparse

from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig
from langgraph.graph import StateGraph

from react_agent.utils.cache import ProcessorCache
from react_agent.utils.content import (
    chunk_text,
    merge_chunk_results,
    preprocess_content,
)
from react_agent.utils.defaults import get_default_extraction_result
from react_agent.utils.logging import (
    error_highlight,
    get_logger,
    info_highlight,
    warning_highlight,
)

# Import statistical utilities
from react_agent.utils.statistics import (
    HIGH_CREDIBILITY_TERMS,
    assess_authoritative_sources,
    calculate_category_quality_score,
)
from react_agent.utils.validations import is_valid_url

# Initialize logger and JSON cache.
logger = get_logger(__name__)
json_cache = ProcessorCache(thread_id="json_parser")

# Regular expressions for identifying statistical content.
STAT_PATTERNS: List[str] = [
    r"\d+%",  # Percentage
    r"\$\d+(?:,\d+)*(?:\.\d+)?(?:\s?(?:million|billion|trillion))?",  # Currency
    r"\d+(?:\.\d+)?(?:\s?(?:million|billion|trillion))?",  # Numbers with scale
    r"increased by|decreased by|grew by|reduced by|rose|fell",  # Trend language
    r"majority|minority|fraction|proportion|ratio",  # Proportion language
    r"survey|respondents|participants|study found",  # Research language
    r"statistics show|data indicates|report reveals",  # Statistical citation
    r"market share|growth rate|adoption rate|satisfaction score",  # Business metrics
    r"average|mean|median|mode|range|standard deviation",  # Statistical terms
]
COMPILED_STAT_PATTERNS: List[re.Pattern] = [
    re.compile(pattern, re.IGNORECASE) for pattern in STAT_PATTERNS
]


def extract_citations(text: str) -> List[Dict[str, str]]:
    """Extract citation information from a text.

    Searches for patterns such as "(Source: X)", "[X]", "cited from X", etc.

    Args:
        text (str): The input text.

    Returns:
        List[Dict[str, str]]: A list of dictionaries, each with keys "source" and "context".

    Examples:
        >>> extract_citations("According to a recent survey by TechCorp, 75%...")
        [{'source': 'TechCorp', 'context': '...survey by TechCorp, 75%...'}]
    """
    citations: List[Dict[str, str]] = []
    citation_patterns = [
        r"\(Source:?\s+([^)]+)\)",
        r"\[([^]]+)\]",
        r"cited\s+from\s+([^,.;]+)",
        r"according\s+to\s+([^,.;]+)",
        r"reported\s+by\s+([^,.;]+)",
        r"([^,.;]+)\s+reports",
    ]
    for pattern in citation_patterns:
        for match in re.finditer(pattern, text, re.IGNORECASE):
            citation = match.group(1).strip()
            # Skip if the citation appears to be a year.
            if not re.match(r"^(19|20)\d{2}$", citation):
                citations.append({
                    "source": citation,
                    "context": text[
                        max(0, match.start() - 50) : min(len(text), match.end() + 50)
                    ],
                })
    return citations


def infer_statistic_type(text: str) -> str:
    """
    Infer the type of statistic from text.

    Determines the type based on keywords and symbols.

    Args:
        text (str): The text to analyze.

    Returns:
        str: The inferred type (e.g., 'percentage', 'financial', etc.).

    Examples:
        >>> infer_statistic_type("Market share increased to 45%")
        'percentage'
    """
    if re.search(r"%|percent|percentage", text, re.IGNORECASE):
        return "percentage"
    elif re.search(
        r"\$|\beuro\b|\beur\b|\bgbp\b|\bjpy\b|cost|price|spend|budget",
        text,
        re.IGNORECASE,
    ):
        return "financial"
    elif re.search(
        r"time|duration|period|year|month|week|day|hour", text, re.IGNORECASE
    ):
        return "temporal"
    elif re.search(r"ratio|proportion|fraction", text, re.IGNORECASE):
        return "ratio"
    elif re.search(r"increase|decrease|growth|decline|trend", text, re.IGNORECASE):
        return "trend"
    elif re.search(r"survey|respondent|participant", text, re.IGNORECASE):
        return "survey"
    elif re.search(r"market share|market size", text, re.IGNORECASE):
        return "market"
    else:
        return "general"


def rate_statistic_quality(stat_text: str) -> float:
    """
    Rate the quality of a statistic on a scale from 0.0 to 1.0.

    Increases the base score based on numerical presence, citation indicators,
    and a mentioned year; penalizes vague language.

    Args:
        stat_text (str): The statistic text.

    Returns:
        float: A quality score between 0.0 and 1.0.

    Examples:
        >>> rate_statistic_quality("According to Gartner's 2023 survey, 78.5%...")
        0.95
    """
    score = 0.5  # Base score
    if re.search(r"\d+(?:\.\d+)?%", stat_text):
        score += 0.15
    elif re.search(r"\$\d+(?:,\d+)*(?:\.\d+)?", stat_text):
        score += 0.15
    if re.search(
        r"according to|reported by|cited from|source|study|survey",
        stat_text,
        re.IGNORECASE,
    ):
        score += 0.2
    if extract_year(stat_text):
        score += 0.1
    if re.search(
        r"may|might|could|possibly|potentially|estimated", stat_text, re.IGNORECASE
    ):
        score -= 0.1
    return max(0.0, min(1.0, score))


def extract_year(text: str) -> Optional[int]:
    """
    Extract a year from text if present.

    Args:
        text (str): The input text.

    Returns:
        Optional[int]: The year found, or None.

    Examples:
        >>> extract_year("In 2023, cloud adoption grew.")
        2023
    """
    year_match = re.search(r"\b(19\d{2}|20\d{2})\b", text)
    return int(year_match[1]) if year_match else None


def extract_credibility_terms(text: str) -> List[str]:
    """
    Extract credibility-indicating terms from text.

    Args:
        text (str): The text to analyze.

    Returns:
        List[str]: A list of credibility terms found.

    Examples:
        >>> extract_credibility_terms("Reported by a renowned research institute")
        ['research', 'institute']
    """
    return [term for term in HIGH_CREDIBILITY_TERMS if term.lower() in text.lower()]


def assess_source_quality(text: str) -> float:
    """
    Assess the quality of a source from a text snippet.

    Awards points based on the presence of credibility terms, citation phrases,
    and authoritative source indicators (e.g. .gov, .edu).

    Args:
        text (str): The text containing source information.

    Returns:
        float: A quality score between 0.0 and 1.0.

    Examples:
        >>> assess_source_quality("According to a study by a .edu institution, ...")
        0.8
    """
    score = 0.5
    credibility_count = sum(
        term.lower() in text.lower() for term in HIGH_CREDIBILITY_TERMS
    )
    if credibility_count >= 2:
        score += 0.3
    elif credibility_count == 1:
        score += 0.15
    if re.search(
        r"according to|reported by|cited from|source|study|survey", text, re.IGNORECASE
    ):
        score += 0.2
    if any(
        domain in text.lower()
        for domain in [".gov", ".edu", ".org", "research", "university"]
    ):
        score += 0.2
    return min(1.0, score)


def enrich_extracted_fact(
    fact: Dict[str, Any], url: str, source_title: str
) -> Dict[str, Any]:
    """
    Enrich an extracted fact with additional metadata and context.

    Adds source URL, title, domain, timestamp, and further extracts statistics
    and citations from an optional "source_text" field. It also adjusts the confidence
    score based on available evidence and applies a small boost if the source is authoritative.

    Args:
        fact (Dict[str, Any]): The initial fact.
        url (str): The source URL.
        source_title (str): The source document title.

    Returns:
        Dict[str, Any]: The enriched fact.

    Examples:
        >>> fact = {"text": "Cloud adoption grew by 25% in 2023", "confidence": 0.8, "source_text": "..."}
        >>> enrich_extracted_fact(fact, "https://example.com/report", "Cloud Market Report")
    """
    fact["source_url"] = url
    fact["source_title"] = source_title
    try:
        fact["source_domain"] = urlparse(url).netloc
    except Exception:
        fact["source_domain"] = ""
    fact["extraction_timestamp"] = datetime.now(timezone.utc).isoformat()

    if isinstance(fact.get("source_text"), str):
        if extracted_stats := extract_statistics(
            fact["source_text"], url, source_title
        ):
            fact["statistics"] = extracted_stats
        if citations := extract_citations(fact["source_text"]):
            fact["additional_citations"] = citations

    # Normalize confidence to a float value.
    confidence_score = fact.get("confidence", 0.5)
    if isinstance(confidence_score, str):
        mapping = {"high": 0.9, "medium": 0.7, "low": 0.4}
        confidence_score = mapping.get(confidence_score.lower(), 0.5)

    # Boost confidence if statistics or additional citations are present.
    if fact.get("statistics"):
        confidence_score = min(1.0, confidence_score + 0.1)
    if fact.get("additional_citations"):
        confidence_score = min(1.0, confidence_score + 0.1)

    # Use assess_authoritative_sources to give a small extra boost if the URL is authoritative.
    if url and is_valid_url(url):
        source_info = {
            "url": url,
            "title": source_title,
            "source": urlparse(url).netloc,
            "quality_score": 0.8,
        }
        if assess_authoritative_sources([source_info]):
            confidence_score = min(1.0, confidence_score + 0.05)

    fact["confidence_score"] = confidence_score
    return fact


def find_json_object(text: str) -> Optional[str]:
    """
    Find a JSON object or array in text using balanced brace matching.

    Args:
        text (str): The input text.

    Returns:
        Optional[str]: The JSON-like string if found; otherwise, None.

    Examples:
        >>> find_json_object('Some text {"key": "value", "nested": {"data": 123}} more text')
        '{"key": "value", "nested": {"data": 123}}'
    """
    for start_char, end_char in [("{", "}"), ("[", "]")]:
        start_positions = [pos for pos, char in enumerate(text) if char == start_char]
        for start_pos in start_positions:
            level = 0
            pos = start_pos
            while pos < len(text):
                char = text[pos]
                if char == start_char:
                    level += 1
                elif char == end_char:
                    level -= 1
                    if level == 0:
                        return text[start_pos : pos + 1]
                pos += 1
    return None


def _clean_json_string(text: str) -> str:
    """
    Clean and normalize a JSON string.

    Removes code block markers, trims whitespace, replaces single quotes with double quotes,
    removes trailing commas, and ensures proper bracing.

    Args:
        text (str): The raw JSON string.

    Returns:
        str: The cleaned JSON string.

    Examples:
        >>> _clean_json_string("```json\\n{'key': 'value',}\\n```")
        '{"key": []}'
    """
    text = re.sub(r"```(?:json)?\s*|\s*```", "", text)
    text = text.strip().replace("'", '"')
    text = re.sub(r",(\s*[}\]])", r"\1", text)
    if not text.startswith("{"):
        text = "{" + text
    if not text.endswith("}"):
        text = text + "}"
    return re.sub(r'"(\w+)":\s*"', r'"\1": []', text)


def _merge_with_default(parsed: Dict[str, Any], category: str) -> Dict[str, Any]:
    """
    Merge a parsed JSON object with the default extraction result template for a category.

    Only updates keys that exist in the default template and for certain numeric keys,
    converts values to float if needed.

    Args:
        parsed (Dict[str, Any]): The parsed JSON.
        category (str): The extraction category.

    Returns:
        Dict[str, Any]: The merged result.

    Examples:
        >>> _merge_with_default({"confidence_score": "0.85"}, "research")
    """
    result = get_default_extraction_result(category)
    for key, value in parsed.items():
        if key not in result:
            continue
        if isinstance(value, type(result[key])):
            if isinstance(value, (list, dict)):
                result[key] = value
            elif isinstance(value, (int, float)) and key in [
                "relevance_score",
                "confidence_score",
            ]:
                result[key] = float(value)
    return result


def _check_cache(response: str, category: str) -> Dict[str, Any]:
    """
    Check and cache the JSON parsing result of a response.

    Cleans the response string, looks up a cache key, and if not found,
    extracts, parses, merges with the default template, and caches the result.

    Args:
        response (str): The raw JSON response string.
        category (str): The extraction category.

    Returns:
        Dict[str, Any]: The parsed and merged JSON result.

    Examples:
        >>> _check_cache('{"key": "value"}', "research")
    """
    response = _clean_json_string(response)
    cache_key = f"json_parse_{hash(response)}"
    if (
        (cached_result := json_cache.get(cache_key))
        and isinstance(cached_result, dict)
        and cached_result.get("data")
    ):
        return cached_result["data"]
    json_text = find_json_object(response)
    if not json_text:
        return get_default_extraction_result(category)
    parsed = json.loads(json_text)
    result = _merge_with_default(parsed, category)
    json_cache.put(
        cache_key,
        {
            "data": result,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "ttl": 3600,
        },
    )
    return result


@json_cache.cache_result(ttl=3600)
def safe_json_parse(
    response: Union[str, Dict[str, Any]], category: str
) -> Dict[str, Any]:
    """
    Safely parse a JSON response with enhanced error handling and cleanup.

    If the response is already a dictionary, it is returned as is; otherwise, it is cleaned,
    parsed, merged with the default template, and cached.

    Args:
        response (Union[str, Dict[str, Any]]): The response to parse.
        category (str): The extraction category.

    Returns:
        Dict[str, Any]: The parsed JSON as a dictionary.

    Examples:
        >>> safe_json_parse('{"key": "value"}', "research")
    """
    if isinstance(response, dict):
        return response
    if not isinstance(response, str):
        return get_default_extraction_result(category)
    try:
        return _check_cache(response, category)
    except Exception as e:
        error_highlight(f"Error parsing JSON: {str(e)}")
        return get_default_extraction_result(category)


def extract_statistics(
    text: str, url: str = "", source_title: str = ""
) -> List[Dict[str, Any]]:
    """
    Extract statistics and numerical data from text along with metadata.

    Scans the text sentence by sentence and applies several regex patterns to detect statistical information.
    For each detected statistic, infers its type, extracts citations and a mentioned year, assesses source quality,
    rates its quality, and extracts credibility terms. The fact is then enriched with additional metadata.

    Args:
        text (str): The text to process.
        url (str): Optional URL associated with the text.
        source_title (str): Optional title of the source document.

    Returns:
        List[Dict[str, Any]]: A list of extracted statistic dictionaries.

    Examples:
        >>> extract_statistics("According to a recent survey by TechCorp, 75% of enterprises adopted cloud computing in 2023.")
    """
    statistics: List[Dict[str, Any]] = []
    sentences = re.split(r"(?<=[.!?])\s+", text)
    for sentence in sentences:
        for pattern in COMPILED_STAT_PATTERNS:
            if pattern.search(sentence):
                stat_text = sentence.strip()
                if all(s["text"] != stat_text for s in statistics):
                    statistic: Dict[str, Any] = {
                        "text": stat_text,
                        "type": infer_statistic_type(stat_text),
                        "citations": extract_citations(stat_text),
                        "year_mentioned": extract_year(stat_text),
                        "source_quality": assess_source_quality(stat_text),
                        "quality_score": rate_statistic_quality(stat_text),
                        "credibility_terms": extract_credibility_terms(stat_text),
                    }
                    # Enrich the statistic with additional metadata.
                    if enriched := enrich_extracted_fact(statistic, url, source_title):
                        statistic |= enriched
                    statistics.append(statistic)
                break
    return statistics


async def extract_category_information(
    content: str,
    url: str,
    title: str,
    category: str,
    original_query: str,
    prompt_template: str,
    extraction_model: Any,
    config: Optional[RunnableConfig] = None,
) -> Tuple[List[Dict[str, Any]], float]:
    """
    Extract information for a specific category with enhanced statistical focus.

    Preprocesses the content, builds a prompt using a template, processes the content with the extraction model,
    extracts and enriches facts, and returns the facts sorted by confidence along with an overall relevance score.

    Args:
        content (str): The raw content.
        url (str): The source URL.
        title (str): The source title.
        category (str): The extraction category (e.g., "market_dynamics").
        original_query (str): The original search query.
        prompt_template (str): A template for building the prompt.
        extraction_model (Any): The extraction model to use.
        config (Optional[RunnableConfig]): Optional model configuration.

    Returns:
        Tuple[List[Dict[str, Any]], float]:
            - A list of enriched fact dictionaries.
            - A relevance score indicating overall relevance.

    Examples:
        >>> facts, relevance = await extract_category_information(
        ...     content="Some lengthy content...",
        ...     url="https://example.com/report",
        ...     title="Market Report 2023",
        ...     category="market_dynamics",
        ...     original_query="cloud computing trends",
        ...     prompt_template="Extract facts for {query} from {url}: {content}",
        ...     extraction_model=model
        ... )
    """
    if not _validate_inputs(content, url):
        return [], 0.0

    info_highlight(f"Extracting from {url} for {category}")
    try:
        content = preprocess_content(content, url)
        prompt = prompt_template.format(query=original_query, url=url, content=content)
        extraction_result = await _process_content(
            content=content,
            prompt=prompt,
            category=category,
            extraction_model=extraction_model,
            config=config,
            url=url,
            title=title,
        )
        facts = _get_category_facts(category, extraction_result)
        enriched_facts = [enrich_extracted_fact(fact, url, title) for fact in facts]
        sorted_facts = sorted(
            enriched_facts, key=lambda x: x.get("confidence_score", 0), reverse=True
        )
        return sorted_facts, extraction_result.get("relevance_score", 0.0)
    except Exception as e:
        error_highlight(f"Error extracting from {url}: {str(e)}")
        return [], 0.0


def _validate_inputs(content: str, url: str) -> bool:
    """
    Validate that content and URL are suitable for extraction.

    Args:
        content (str): The text content.
        url (str): The URL to validate.

    Returns:
        bool: True if valid; otherwise, False.

    Examples:
        >>> _validate_inputs("Some content", "https://example.com")
        True
    """
    if not content or not url or not is_valid_url(url):
        warning_highlight(f"Invalid content or URL for extraction: {url}")
        return False
    return True


async def _process_content(
    content: str,
    prompt: str,
    category: str,
    extraction_model: Any,
    config: Optional[RunnableConfig],
    url: str,
    title: str,
) -> Dict[str, Any]:
    """
    Process content with the extraction model.

    If the content is very large, it delegates to chunked processing.

    Args:
        content (str): Preprocessed content.
        prompt (str): The prompt to send to the model.
        category (str): The extraction category.
        extraction_model (Any): The extraction model.
        config (Optional[RunnableConfig]): Additional configuration.
        url (str): The source URL.
        title (str): The source title.

    Returns:
        Dict[str, Any]: The extraction result.

    Examples:
        >>> result = await _process_content("Some content", "Prompt here", "market", model, None, "https://example.com", "Report")
    """
    if len(content) > 40000:
        return await _process_chunked_content(
            content, prompt, category, extraction_model, config, url, title
        )
    model_response = await extraction_model(
        messages=[{"role": "human", "content": prompt}], config=config
    )
    extraction_result = safe_json_parse(model_response, category)
    if stats := extract_statistics(content, url, title):
        extraction_result["statistics"] = stats
    return extraction_result


async def _process_chunked_content(
    content: str,
    prompt: str,
    category: str,
    extraction_model: Any,
    config: Optional[RunnableConfig],
    url: str,
    title: str,
) -> Dict[str, Any]:
    """
    Process content in chunks when it exceeds a size limit.

    Splits the content, processes each chunk, merges the results, and aggregates statistics.

    Args:
        content (str): The large content.
        prompt (str): The prompt template.
        category (str): The extraction category.
        extraction_model (Any): The extraction model.
        config (Optional[RunnableConfig]): Optional configuration.
        url (str): The source URL.
        title (str): The source title.

    Returns:
        Dict[str, Any]: The merged extraction result.

    Examples:
        >>> result = await _process_chunked_content(long_content, "Prompt", "market", model, None, "https://example.com", "Report")
    """
    info_highlight(f"Content too large ({len(content)} chars), chunking...")
    chunks = chunk_text(content)
    all_statistics: List[Any] = []
    chunk_results: List[Dict[str, Any]] = []
    for chunk_idx, chunk in enumerate(chunks):
        info_highlight(f"Processing chunk {chunk_idx + 1}/{len(chunks)}")
        chunk_prompt = prompt.format(content=chunk)
        chunk_response = await extraction_model(
            messages=[{"role": "human", "content": chunk_prompt}], config=config
        )
        if chunk_result := safe_json_parse(chunk_response, category):
            chunk_statistics = extract_statistics(chunk, url, title)
            all_statistics.extend(chunk_statistics)
            chunk_results.append(chunk_result)
    result = merge_chunk_results(chunk_results, category)
    if all_statistics:
        result["statistics"] = all_statistics
    return result


def _get_category_facts(
    category: str, extraction_result: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Extract facts from the extraction result based on category structure.

    Uses a mapping of categories to their corresponding fact types and keys,
    returning a list of fact dictionaries.

    Args:
        category (str): The extraction category.
        extraction_result (Dict[str, Any]): The extraction result.

    Returns:
        List[Dict[str, Any]]: A list of fact dictionaries.

    Examples:
        >>> _get_category_facts("market_dynamics", {"extracted_facts": [{"text": "Example fact"}]})
    """
    if not extraction_result or not isinstance(extraction_result, dict):
        return []
    category_mapping: Dict[str, List[Tuple[str, str]]] = {
        "market_dynamics": [("fact", "extracted_facts")],
        "provider_landscape": [
            ("vendor", "extracted_vendors"),
            ("relationship", "vendor_relationships"),
        ],
        "technical_requirements": [
            ("requirement", "extracted_requirements"),
            ("standard", "standards"),
        ],
        "regulatory_landscape": [
            ("regulation", "extracted_regulations"),
            ("compliance", "compliance_requirements"),
        ],
        "cost_considerations": [
            ("cost", "extracted_costs"),
            ("pricing_model", "pricing_models"),
        ],
        "best_practices": [
            ("practice", "extracted_practices"),
            ("methodology", "methodologies"),
        ],
        "implementation_factors": [
            ("factor", "extracted_factors"),
            ("challenge", "challenges"),
        ],
    }
    facts: List[Dict[str, Any]] = []
    for fact_type, key in category_mapping.get(category, [("fact", "extracted_facts")]):
        items = extraction_result.get(key, [])
        facts.extend([{"type": fact_type, "data": item} for item in items])
    return facts
</file>

<file path="src/react_agent/utils/llm.py">
"""LLM utility functions for handling model calls and content processing.

This module provides utilities for interacting with language models,
including content length management, error handling, and JSON parsing.
The LLMClient class consolidates chat, JSON, and embedding calls into a
single interface, minimizing breaking changes while providing a strictly typed
interface for LangGraph nodes.

Examples:
    Basic usage of call_model:

    ```python
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ]
    response = await _call_model(messages)
    # Returns: {"content": "The capital of France is Paris."}
    ```

    Using _call_model_json with chunking:

    ```python
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Analyze this long text: " + large_document}
    ]
    config = {"configurable": {"model": "openai/gpt-4"}}
    response = await _call_model_json(
        messages,
        config=config,
        chunk_size=1000,  # Process in 1000-token chunks
        overlap=100       # 100-token overlap between chunks
    )
    # Returns a merged JSON response.
    ```

    Using the LLMClient for LangGraph nodes:

    ```python
    # Initialize the client
    llm_client = LLMClient(default_model="openai/gpt-4")

    # Use in an async function or LangGraph node
    async def my_node(state: dict) -> dict:
        # Simple chat completion
        response = await llm_client.llm_chat(
            prompt="What is the capital of France?",
            system_prompt="You are a helpful assistant."
        )

        # Get structured JSON output
        data = await llm_client.llm_json(
            prompt="List the top 3 largest countries by area in JSON format",
            system_prompt="You are a helpful assistant that outputs valid JSON."
        )

        # Get embeddings
        embedding = await llm_client.llm_embed("This is a text to embed")

        return {"response": response, "data": data, "embedding": embedding}
    ```
"""

import asyncio
import json
import os
import re
from datetime import UTC, datetime
from typing import Any, Dict, Iterable, List, TypedDict, Union, cast

from anthropic import AsyncAnthropic
from anthropic.types import MessageParam
from langchain_core.runnables import RunnableConfig
from openai import AsyncClient
from openai.types.chat import ChatCompletionMessageParam

from react_agent.configuration import Configuration
from react_agent.utils.content import (
    chunk_text,
    estimate_tokens,
    merge_chunk_results,
    preprocess_content,
    validate_content,
)
from react_agent.utils.defaults import ChunkConfig
from react_agent.utils.extraction import safe_json_parse
from react_agent.utils.logging import (
    error_highlight,
    get_logger,
    info_highlight,
    warning_highlight,
)

logger = get_logger(__name__)

# Constants for token limits
MAX_TOKENS: int = 16000
MAX_SUMMARY_TOKENS: int = 2000

# Initialize API clients
openai_client: AsyncClient = AsyncClient(
    api_key=os.getenv("OPENAI_API_KEY"), base_url=os.getenv("OPENAI_API_BASE")
)
anthropic_client: AsyncAnthropic = AsyncAnthropic(
    api_key=os.getenv("ANTHROPIC_API_KEY")
)


class Message(TypedDict):
    role: str
    content: str


async def _summarize_content(
    input_content: str, max_tokens: int = MAX_SUMMARY_TOKENS
) -> str:
    """Summarize content using a more efficient model.

    Args:
        input_content: The content to summarize.
        max_tokens: Maximum tokens for the summary.

    Returns:
        A concise summary of the input content.
    """
    try:
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "You are a helpful assistant that creates concise summaries. "
                        "Focus on key points and maintain factual accuracy."
                    ),
                },
                {
                    "role": "user",
                    "content": f"Please summarize the following content concisely:\n\n{input_content}",
                },
            ],
            max_tokens=max_tokens,
            temperature=0.3,
        )
        response_content: str | None = cast(
            str | None, response.choices[0].message.content
        )
        return response_content if response_content is not None else ""
    except Exception as e:
        error_highlight(f"Error in _summarize_content: {str(e)}")
        return input_content


async def _format_openai_messages(
    messages: List[Message], system_prompt: str, max_tokens: int | None = None
) -> List[ChatCompletionMessageParam]:
    """Format messages for the OpenAI API with content handling.

    Args:
        messages: List of message dictionaries.
        system_prompt: System prompt to include.
        max_tokens: Optional maximum tokens per message.

    Returns:
        A list of formatted messages for the API.
    """
    if not messages:
        return [{"role": "system", "content": system_prompt}]

    formatted_messages: List[ChatCompletionMessageParam] = []
    max_tokens = max_tokens or MAX_TOKENS

    for msg in messages:
        if msg["role"] == "system":
            formatted_messages.append({"role": "system", "content": msg["content"]})
        else:
            content: str = msg["content"]
            if estimate_tokens(content) > max_tokens:
                info_highlight("Content too long, summarizing...")
                content = await _summarize_content(content, max_tokens)
            formatted_messages.append({"role": "user", "content": content})

    if all(msg["role"] != "system" for msg in formatted_messages):
        formatted_messages.insert(0, {"role": "system", "content": system_prompt})

    return formatted_messages


async def _call_openai_api(
    model: str, messages: List[ChatCompletionMessageParam]
) -> Dict[str, Any]:
    """Call the OpenAI API using formatted messages.

    Args:
        model: The model identifier.
        messages: Formatted messages for the API.

    Returns:
        A dictionary containing the model's response.
    """
    try:
        response = await openai_client.chat.completions.create(
            model=model, messages=messages, max_tokens=MAX_TOKENS, temperature=0.7
        )
        content: str | None = response.choices[0].message.content
        return {"content": content} if content else {}
    except Exception as e:
        error_highlight(f"Error in _call_openai_api: {str(e)}")
        return {}


async def _call_model(
    messages: List[Message], config: RunnableConfig | None = None
) -> Dict[str, Any]:
    """Call the language model with the provided messages.

    Args:
        messages: List of messages (each with 'role' and 'content').
        config: Optional configuration for the model call.

    Returns:
        A dictionary with the model's output under the key 'content'.
    """
    if not messages:
        error_highlight("No messages provided to _call_model")
        return {}

    try:
        config = config or {}
        configurable: Dict[str, Any] = config.get("configurable", {})
        configurable["timestamp"] = datetime.now(UTC).isoformat()
        config = {**config, "configurable": configurable}
        configuration: Configuration = Configuration.from_runnable_config(config)
        logger.info(f"Calling model with {len(messages)} messages")
        logger.debug(f"Config: {config}")
        provider, model = configuration.model.split("/", 1)
        if provider == "openai":
            openai_formatted_messages = await _format_openai_messages(
                messages, configuration.system_prompt
            )
            return await _call_openai_api(model, openai_formatted_messages)
        elif provider == "anthropic":
            # Check if a system message is already present
            has_system_message: bool = any(msg["role"] == "system" for msg in messages)

            # Create properly typed messages for Anthropic
            anthropic_formatted_messages: List[Dict[str, str]] = []

            # Add system message if not already present
            if not has_system_message and configuration.system_prompt:
                # For Anthropic, we need to handle system messages differently
                # as they have specific role requirements
                anthropic_formatted_messages.append({
                    "role": "user",  # Using user role as a workaround for system
                    "content": f"System instruction: {configuration.system_prompt}",
                })

            for msg in messages:
                # Anthropic only accepts user and assistant roles
                if msg["role"] == "system":
                    # Convert system messages to user messages with a prefix
                    anthropic_formatted_messages.append({
                        "role": "user",
                        "content": f"System instruction: {msg['content']}",
                    })
                elif msg["role"] in ["user", "assistant"]:
                    anthropic_formatted_messages.append({
                        "role": msg["role"],
                        "content": msg["content"],
                    })

            # Make the API call
            try:
                # Cast the messages to the expected type for Anthropic
                typed_messages = cast(
                    Iterable[MessageParam], anthropic_formatted_messages
                )
                response = await anthropic_client.messages.create(
                    model=model,
                    messages=typed_messages,
                    max_tokens=MAX_TOKENS,
                    temperature=0.7,
                )
                # Access the content correctly based on the response structure
                content_block = response.content[0]
                # Check the type of content block and extract text appropriately
                if hasattr(content_block, "text"):
                    return {"content": content_block.text}
                else:
                    return {"content": str(content_block)}
            except Exception as e:
                error_highlight(f"Error in Anthropic API call: {str(e)}")
                raise
        else:
            error_highlight(f"Unsupported model provider: {provider}")
            return {}
    except Exception as e:
        error_highlight(f"Error in _call_model: {str(e)}")
        raise


async def _process_chunk(
    chunk: str,
    previous_messages: List[Message],
    config: RunnableConfig | None = None,
    model: str | None = None,
) -> Dict[str, Any]:
    """Process a single chunk of content with error handling.

    Args:
        chunk: The content chunk to process.
        previous_messages: Previous conversation messages.
        config: Optional configuration.
        model: Optional model override.

    Returns:
        A dictionary with the processed chunk result.
    """
    if not chunk or not previous_messages:
        return {}
    try:
        messages: List[Message] = previous_messages + [
            {"role": "human", "content": chunk}
        ]
        max_retries: int = 3
        retry_delay: int = 1
        for attempt in range(max_retries):
            try:
                # Convert dict to RunnableConfig if needed
                runnable_config: RunnableConfig | None = config
                if (
                    config
                    and not isinstance(config, dict)
                    and not isinstance(config, RunnableConfig)
                ):
                    runnable_config = cast(RunnableConfig, config)

                response = await _call_model(messages, runnable_config)
                if not response or not response.get("content"):
                    error_highlight("Empty response from model")
                    return {}
                parsed: Dict[str, Any] = safe_json_parse(
                    response["content"], "model_response"
                )
                return parsed if parsed is not None else {}
            except Exception as e:
                if attempt < max_retries - 1:
                    warning_highlight(
                        f"Attempt {attempt + 1} failed: {str(e)}. Retrying..."
                    )
                    await asyncio.sleep(retry_delay * (attempt + 1))
                else:
                    error_highlight(f"All retry attempts failed: {str(e)}")
                    return {}
    except Exception as e:
        error_highlight(f"Error processing chunk: {str(e)}")
        return {}
    return {}


async def _call_model_json(
    messages: List[Message],
    config: RunnableConfig | None = None,
    chunk_size: int | None = None,
    overlap: int | None = None,
) -> Dict[str, Any]:
    """Call the model for JSON output, handling chunking if necessary.

    Args:
        messages: List of message dictionaries.
        config: Optional configuration for the model call.
        chunk_size: Optional custom chunk size.
        overlap: Optional token overlap between chunks.

    Returns:
        A dictionary containing the merged JSON response.
    """
    try:
        if not messages:
            error_highlight("No messages provided to _call_model_json")
            return {}
        content: str = messages[-1]["content"]
        if estimate_tokens(content) <= MAX_TOKENS:
            return await _process_chunk(content, messages[:-1], config)
        info_highlight(
            f"Content too large ({estimate_tokens(content)} tokens), chunking..."
        )
        chunks: List[str] = chunk_text(
            content, chunk_size=chunk_size, overlap=overlap, use_large_chunks=True
        )
        if len(chunks) <= 1:
            return await _process_chunk(content, messages[:-1], config)
        chunk_results: List[Dict[str, Any]] = []
        for chunk in chunks:
            result: Dict[str, Any] = await _process_chunk(chunk, messages[:-1], config)
            if result:
                chunk_results.append(result)
        if not chunk_results:
            error_highlight("No valid results from chunks")
            return {}
        return merge_chunk_results(chunk_results, "model_response")
    except Exception as e:
        error_highlight(f"Error in _call_model_json: {str(e)}")
        return {}


def _parse_json_response(response: Union[str, Dict[str, Any]]) -> Dict[str, Any]:
    """Parse and clean a JSON response from the model.

    Args:
        response: The raw response as a string or dictionary.

    Returns:
        A dictionary parsed from the JSON response.
    """
    if isinstance(response, dict):
        return response
    try:
        cleaned: str = re.sub(r"```json\s*|\s*```", "", response)
        return json.loads(cleaned)
    except Exception:
        return {}


class LLMClient:
    """Asynchronous LLM utility for chat, JSON output, and embeddings.

    Provides a consolidated interface for calling language models,
    abstracting away the details of message formatting, chunking, and error handling.

    Attributes:
        default_model: An optional default model to use for calls.
    """

    def __init__(self, default_model: str | None = None) -> None:
        """Initialize the LLMClient.

        Args:
            default_model: Optional default model name (e.g., "openai/gpt-4").
                If not provided, defaults from configuration are used.
        """
        self.default_model: str | None = default_model

    async def llm_chat(
        self, prompt: str, system_prompt: str | None = None, **kwargs: Any
    ) -> str:
        """Get a chat completion as plain text.

        Args:
            prompt: The user prompt.
            system_prompt: Optional system prompt.
            **kwargs: Additional parameters (e.g., model, temperature).

        Returns:
            The model's response as a string.
        """
        messages: List[Message] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        config: Dict[str, Any] = kwargs.pop("config", {})
        configurable: Dict[str, Any] = config.get("configurable", {})
        if self.default_model and "model" not in configurable:
            configurable["model"] = self.default_model
        for key, value in kwargs.items():
            configurable[key] = value
        config = {**config, "configurable": configurable}

        # Convert dict to RunnableConfig
        runnable_config: RunnableConfig | None = (
            cast(RunnableConfig | None, config) if config else None
        )

        response: Dict[str, Any] = await _call_model(messages, runnable_config)
        return response.get("content", "")

    async def llm_json(
        self, prompt: str, system_prompt: str | None = None, **kwargs: Any
    ) -> Dict[str, Any]:
        """Get a structured JSON response as a Python dictionary.

        Args:
            prompt: The user prompt.
            system_prompt: Optional system prompt.
            **kwargs: Additional parameters (e.g., model, temperature, chunk_size, overlap).

        Returns:
            A dictionary containing the parsed JSON response.
        """
        chunk_size: int | None = kwargs.pop("chunk_size", None)
        overlap: int | None = kwargs.pop("overlap", None)
        messages: List[Message] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        config: Dict[str, Any] = kwargs.pop("config", {})
        configurable: Dict[str, Any] = config.get("configurable", {})
        if self.default_model and "model" not in configurable:
            configurable["model"] = self.default_model
        for key, value in kwargs.items():
            configurable[key] = value
        config = {**config, "configurable": configurable}

        # Convert dict to RunnableConfig
        runnable_config: RunnableConfig | None = (
            cast(RunnableConfig | None, config) if config else None
        )

        return await _call_model_json(
            messages=messages,
            config=runnable_config,
            chunk_size=chunk_size,
            overlap=overlap,
        )

    async def llm_embed(self, text: str, **kwargs: Any) -> List[float]:
        """Get embeddings for a given text.

        Args:
            text: The text to embed.
            **kwargs: Additional parameters (e.g., model).

        Returns:
            A list of floats representing the embedding vector.
        """
        config: Dict[str, Any] = kwargs.pop("config", {})
        configurable: Dict[str, Any] = config.get("configurable", {})
        embedding_model: str | None = kwargs.pop("embedding_model", None)
        if embedding_model:
            configurable["model"] = embedding_model
        elif self.default_model and "model" not in configurable:
            configurable["model"] = self.default_model
        for key, value in kwargs.items():
            configurable[key] = value
        config = {**config, "configurable": configurable}

        try:
            # Ensure config is properly typed for from_runnable_config
            runnable_config = cast(RunnableConfig | None, config)
            configuration: Configuration = Configuration.from_runnable_config(
                runnable_config
            )
            provider, model = configuration.model.split("/", 1)
            if provider == "openai":
                response = await openai_client.embeddings.create(
                    model=model, input=text
                )
                return response.data[0].embedding
            else:
                error_highlight(f"Embedding not supported for provider: {provider}")
                return []
        except Exception as e:
            error_highlight(f"Error in llm_embed: {str(e)}")
            return []


__all__ = ["LLMClient"]
</file>

<file path="src/react_agent/utils/logging.py">
"""Logging utilities.

This module provides enhanced logging utilities and convenience methods for the agent framework.
It builds upon the logging configuration defined in log_config.py to enable rich, formatted logging output.
"""

import logging
from typing import Any, Mapping, Optional, Dict

# Module: log_config.py
# This module provides logging configuration for the enrichment agent, including functions
# for setting up and configuring loggers with rich formatting.

import threading
from typing import Optional  # noqa: F401

from rich.console import Console
from rich.logging import RichHandler

# Create a rich console for formatted output (logs will be printed to stderr).
console = Console(stderr=True)

# Logging configuration constants.
LOG_FORMAT = "%(message)s"  # Maintained for backward compatibility with tests.
DATE_FORMAT = "[%X]"

# A thread-safe lock to ensure logger configuration is not accessed concurrently.
_logger_lock = threading.Lock()


def setup_logger(name: str = "enrichment_agent", level: int = logging.INFO) -> logging.Logger:
    """
    Set up and configure a logger with rich formatting.

    Args:
        name (str): The name of the logger. Defaults to "enrichment_agent".
        level (int): The logging level to set (e.g., logging.DEBUG, logging.INFO). Defaults to logging.INFO.

    Returns:
        logging.Logger: A configured logger instance with rich formatting enabled.

    Examples:
        >>> logger = setup_logger("my_agent", level=logging.DEBUG)
        >>> logger.info("This is an info message.")
    """
    with _logger_lock:
        logger = logging.getLogger(name)
        # Configure the logger only if it hasn't been set up already.
        if not logger.handlers:
            logger.setLevel(level)
            handler = RichHandler(
                console=console,
                rich_tracebacks=True,
                tracebacks_show_locals=True,
                show_time=True,
                show_path=True,
                markup=True,
                log_time_format=DATE_FORMAT,
                omit_repeated_times=False,
                level=level,
            )
            logger.addHandler(handler)
            logger.propagate = False
        return logger


# Create a default logger instance.
logger = setup_logger()


def set_level(level: int) -> None:
    """
    Set the logging level for both the default logger and the root logger.

    Args:
        level (int): The logging level to set (e.g., logging.DEBUG, logging.INFO).

    Returns:
        None

    Examples:
        >>> set_level(logging.DEBUG)
    """
    with _logger_lock:
        logger.setLevel(level)
        for handler in logger.handlers:
            handler.setLevel(level)
        # Update the root logger's level to affect the entire logging hierarchy.
        root_logger = logging.getLogger()
        root_logger.setLevel(level)


def get_logger(name: str) -> logging.Logger:
    """
    Retrieve a logger with the specified name, configured with rich formatting.

    Args:
        name (str): The name for the logger (typically __name__ from the calling module).

    Returns:
        logging.Logger: A logger instance with rich formatting and proper log levels.

    Examples:
        >>> my_logger = get_logger(__name__)
        >>> my_logger.info("Logger retrieved and ready to use.")
    """
    return setup_logger(name)


def info_success(message: str, exc_info: bool | BaseException | None = None) -> None:
    """
    Log a success message with green formatting.

    Args:
        message (str): The message to log.
        exc_info (bool | BaseException | None): Optional exception information to include in the log.

    Returns:
        None

    Examples:
        >>> info_success("Operation completed successfully.")
    """
    logger.info("[bold green] %s[/bold green]", message, exc_info=exc_info)


def info_highlight(
    message: str, 
    category: Optional[str] = None, 
    progress: Optional[str] = None, 
    exc_info: bool | BaseException | None = None
) -> None:
    """
    Log an informational message with blue highlighting, optionally tagged with a category and progress.

    Args:
        message (str): The message to log.
        category (Optional[str]): An optional category to tag the message.
        progress (Optional[str]): An optional progress indicator to prefix the message.
        exc_info (bool | BaseException | None): Optional exception information to include.

    Returns:
        None

    Examples:
        >>> info_highlight("Data loaded successfully", category="DataLoader")
        >>> info_highlight("50% completed", progress="50%")
    """
    if progress:
        message = f"[{progress}] {message}"
    if category:
        message = f"[{category}] {message}"
    logger.info("[bold blue] %s[/bold blue]", message, exc_info=exc_info)


def warning_highlight(
    message: str, 
    category: Optional[str] = None, 
    exc_info: bool | BaseException | None = None
) -> None:
    """
    Log a warning message with yellow highlighting, optionally tagged with a category.

    Args:
        message (str): The warning message to log.
        category (Optional[str]): An optional category tag.
        exc_info (bool | BaseException | None): Optional exception information to include.

    Returns:
        None

    Examples:
        >>> warning_highlight("Low disk space", category="System")
    """
    if category:
        message = f"[{category}] {message}"
    logger.warning("[bold yellow] %s[/bold yellow]", message, exc_info=exc_info)


def error_highlight(
    message: str, 
    category: Optional[str] = None, 
    exc_info: bool | BaseException | None = None
) -> None:
    """
    Log an error message with red highlighting, optionally tagged with a category.

    Args:
        message (str): The error message to log.
        category (Optional[str]): An optional category tag.
        exc_info (bool | BaseException | None): Optional exception information to include.

    Returns:
        None

    Examples:
        >>> error_highlight("Failed to connect to database", category="Database")
    """
    if category:
        message = f"[{category}] {message}"
    logger.error("[bold red] %s[/bold red]", message, exc_info=exc_info)


def log_dict(data: Mapping[str, Any], level: int = logging.INFO, title: Optional[str] = None) -> None:
    """
    Log a dictionary with pretty formatting for easier readability.

    Args:
        data (Mapping[str, Any]): The dictionary data to log.
        level (int): The logging level to use (e.g., logging.INFO). Defaults to logging.INFO.
        title (Optional[str]): An optional title to display before the dictionary output.

    Raises:
        ValueError: If an invalid logging level is provided.

    Returns:
        None

    Examples:
        >>> log_dict({"key1": "value1", "key2": 42}, level=logging.DEBUG, title="Config Data")
    """
    if level not in (
        logging.DEBUG,
        logging.INFO,
        logging.WARNING,
        logging.ERROR,
        logging.CRITICAL,
    ):
        raise ValueError(f"Invalid logging level: {level}")

    if title:
        logger.log(level, "[bold]%s[/bold]", title)

    for key, value in data.items():
        logger.log(level, "  [cyan]%s[/cyan]: %s", key, value)


def log_step(step_name: str, step_number: Optional[int] = None, total_steps: Optional[int] = None) -> None:
    """
    Log a processing step, optionally including the step number within a sequence.

    Args:
        step_name (str): The name or description of the step.
        step_number (Optional[int]): The current step number in the sequence.
        total_steps (Optional[int]): The total number of steps in the sequence.

    Raises:
        ValueError: If one of step_number or total_steps is provided without the other.
        ValueError: If step_number is not within the valid range (1 to total_steps).

    Returns:
        None

    Examples:
        >>> log_step("Loading data")
        >>> log_step("Processing data", step_number=2, total_steps=5)
    """
    if (step_number is None) != (total_steps is None):
        raise ValueError("Both step_number and total_steps must be provided together")

    if step_number is None or total_steps is None:
        logger.info("[bold magenta]Step:[/bold magenta] %s", step_name)
    elif not 1 <= step_number <= total_steps:
        raise ValueError(f"Invalid step numbers: {step_number}/{total_steps}")
    else:
        logger.info("[bold magenta]Step %s/%s:[/bold magenta] %s", step_number, total_steps, step_name)


def log_progress(current: int, total: int, category: str, operation: str) -> None:
    """
    Log progress for long-running operations.

    Args:
        current (int): The current progress count.
        total (int): The total count representing completion.
        category (str): Category of the operation for grouping logs.
        operation (str): Description of the operation (e.g., "processing", "extracting").

    Returns:
        None

    Examples:
        >>> log_progress(5, 20, category="DataLoad", operation="Loading")
    """
    if total > 0:
        percentage = (current / total) * 100
        info_highlight(f"{operation} {current}/{total} ({percentage:.1f}%)", category=category)


def log_performance_metrics(
    operation: str,
    start_time: float,
    end_time: float,
    category: Optional[str] = None,
    additional_info: Optional[Dict[str, Any]] = None
) -> None:
    """
    Log performance metrics for a given operation.

    Args:
        operation (str): The name or description of the operation.
        start_time (float): The start time (e.g., as returned by time.time()).
        end_time (float): The end time (e.g., as returned by time.time()).
        category (Optional[str]): An optional category for grouping metrics.
        additional_info (Optional[Dict[str, Any]]): Optional additional metrics to log.

    Returns:
        None

    Examples:
        >>> import time
        >>> start = time.time()
        >>> # ... perform operation ...
        >>> end = time.time()
        >>> log_performance_metrics("Data Processing", start, end, category="Performance", additional_info={"records": 1000})
    """
    duration = end_time - start_time
    message = f"{operation} completed in {duration:.2f}s"
    if additional_info:
        info_parts = [f"{k}: {v}" for k, v in additional_info.items()]
        message += f" ({', '.join(info_parts)})"
    info_highlight(message, category=category)
</file>

<file path="src/react_agent/utils/statistics.py">
"""Improved confidence scoring with statistical validation.

This module enhances the confidence scoring logic to focus on statistical
validation, source quality assessment, and cross-validation to achieve
confidence scores above 80%.

Examples:
    >>> # Example usage of calculate_category_quality_score:
    >>> category = "market_dynamics"
    >>> extracted_facts = [{"text": "Fact 1", "source_text": "Report by Gov.", "data": {}},
    ...                    {"text": "Fact 2", "source_text": "Study from Uni.", "data": {}}]
    >>> sources = [{"url": "https://example.gov/report", "quality_score": 0.9, "title": "Gov Report", "source": "Gov"},
    ...            {"url": "https://university.edu/study", "quality_score": 0.85, "title": "University Study", "source": "Uni"}]
    >>> thresholds = {"min_facts": 3, "min_sources": 2, "authoritative_source_ratio": 0.5, "recency_threshold_days": 365}
    >>> score = calculate_category_quality_score(category, extracted_facts, sources, thresholds)
    >>> print(score)  # Outputs a quality score between 0.0 and 1.0
"""

from typing import Dict, List, Any, Optional, Set
from datetime import datetime, timezone
from dateutil import parser
import re
from urllib.parse import urlparse
from collections import Counter

from react_agent.utils.logging import get_logger, info_highlight, warning_highlight, error_highlight

# Initialize logger
logger = get_logger(__name__)

# Authority domain patterns for authoritative sources.
AUTHORITY_DOMAINS = [
    r'\.gov($|/)',    # Government domains
    r'\.edu($|/)',    # Educational institutions
    r'\.org($|/)',    # Non-profit organizations
    r'research\.',    # Research organizations
    r'\.ac\.($|/)',   # Academic institutions
    r'journal\.',     # Academic journals
    r'university\.',   # Universities
    r'institute\.',    # Research institutes
    r'association\.'   # Professional associations
]

# Compile patterns for efficiency.
COMPILED_AUTHORITY_PATTERNS = [re.compile(pattern) for pattern in AUTHORITY_DOMAINS]

# High-credibility source terms.
HIGH_CREDIBILITY_TERMS = [
    'study', 'research', 'survey', 'report', 'analysis',
    'journal', 'publication', 'paper', 'review', 'assessment',
    'statistics', 'data', 'findings', 'results', 'evidence'
]


def calculate_category_quality_score(
    category: str,
    extracted_facts: List[Dict[str, Any]],
    sources: List[Dict[str, Any]],
    thresholds: Dict[str, Any]
) -> float:
    """
    Calculate an enhanced quality score for a research category based on extracted facts and sources.

    The score is built from several weighted components:
      1. Quantity assessment of facts and sources.
      2. Presence of statistical content.
      3. Authoritative source evaluation.
      4. Recency of the sources.
      5. Consistency and cross-validation of the extracted facts.

    Args:
        category (str): The research category (e.g., "market_dynamics").
        extracted_facts (List[Dict[str, Any]]): A list of fact dictionaries extracted from content.
        sources (List[Dict[str, Any]]): A list of source dictionaries.
        thresholds (Dict[str, Any]): A dictionary of threshold values including:
            - "min_facts": Minimum number of facts expected.
            - "min_sources": Minimum number of sources expected.
            - "authoritative_source_ratio": Desired ratio of authoritative sources.
            - "recency_threshold_days": Maximum age (in days) for a source to be considered recent.

    Returns:
        float: The final quality score between 0.0 and 1.0.

    Examples:
        >>> score = calculate_category_quality_score("market_dynamics", extracted_facts, sources, thresholds)
        >>> print(score)
    """
    score = 0.35  # Start with a slightly higher base score.

    # Retrieve thresholds.
    min_facts = thresholds.get("min_facts", 3)
    min_sources = thresholds.get("min_sources", 2)
    auth_ratio = thresholds.get("authoritative_source_ratio", 0.5)
    recency_threshold = thresholds.get("recency_threshold_days", 365)

    # 1. Quantity Assessment (up to 0.25)
    if len(extracted_facts) >= min_facts * 3:
        score += 0.15
    elif len(extracted_facts) >= min_facts * 2:
        score += 0.12
    elif len(extracted_facts) >= min_facts:
        score += 0.08
    else:
        fact_ratio = len(extracted_facts) / min_facts if min_facts else 0
        score += fact_ratio * 0.06

    if len(sources) >= min_sources * 3:
        score += 0.10
    elif len(sources) >= min_sources * 2:
        score += 0.08
    elif len(sources) >= min_sources:
        score += 0.05
    else:
        source_ratio = len(sources) / min_sources if min_sources else 0
        score += source_ratio * 0.03

    # 2. Statistical Content (up to 0.20)
    stat_facts = [
        f for f in extracted_facts
        if "statistics" in f or
        (isinstance(f.get("source_text"), str) and re.search(r'\d+', f.get("source_text", "")) is not None)
    ]
    if stat_facts:
        stat_ratio = len(stat_facts) / len(extracted_facts) if extracted_facts else 0
        if stat_ratio >= 0.5:
            score += 0.20
        elif stat_ratio >= 0.3:
            score += 0.15
        elif stat_ratio >= 0.1:
            score += 0.10
        else:
            score += 0.05

    # 3. Source Quality (up to 0.25)
    authoritative_sources = assess_authoritative_sources(sources)
    if sources:
        auth_source_ratio = len(authoritative_sources) / len(sources)
        if auth_source_ratio >= auth_ratio * 1.5:
            score += 0.25
        elif auth_source_ratio >= auth_ratio:
            score += 0.20
        elif auth_source_ratio >= auth_ratio * 0.7:
            score += 0.15
        elif auth_source_ratio >= auth_ratio * 0.5:
            score += 0.10
        else:
            score += 0.05

    # 4. Recency (up to 0.15)
    recent_sources = count_recent_sources(sources, recency_threshold)
    if sources:
        recency_ratio = recent_sources / len(sources)
        if recency_ratio >= 0.8:
            score += 0.15
        elif recency_ratio >= 0.6:
            score += 0.12
        elif recency_ratio >= 0.4:
            score += 0.08
        elif recency_ratio >= 0.2:
            score += 0.05
        else:
            score += 0.02

    # 5. Consistency and Cross-Validation (up to 0.15)
    consistency_score = assess_fact_consistency(extracted_facts)
    stat_validation_score = perform_statistical_validation(extracted_facts)
    combined_cross_val_score = (consistency_score * 0.10) + (stat_validation_score * 0.05)
    score += combined_cross_val_score

    # Log detailed breakdown.
    info_highlight(f"Category {category} quality score breakdown:")
    info_highlight(f"  - Facts: {len(extracted_facts)}/{min_facts} min")
    info_highlight(f"  - Sources: {len(sources)}/{min_sources} min")
    info_highlight(f"  - Statistical content: {len(stat_facts)}/{len(extracted_facts)} facts")
    info_highlight(f"  - Authoritative sources: {len(authoritative_sources)}/{len(sources)} sources")
    info_highlight(f"  - Recent sources: {recent_sources}/{len(sources)} sources")
    info_highlight(f"  - Consistency score: {consistency_score:.2f}")
    info_highlight(f"  - Statistical validation score: {stat_validation_score:.2f}")
    info_highlight(f"  - Final category score: {min(1.0, score):.2f}")

    return min(1.0, score)


def assess_authoritative_sources(sources: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Assess and return sources considered authoritative based on domain and credibility terms.

    A source is considered authoritative if its URL domain matches known patterns,
    if it has a high quality score, or if its title/source field contains multiple
    high-credibility terms.

    Args:
        sources (List[Dict[str, Any]]): A list of source dictionaries.

    Returns:
        List[Dict[str, Any]]: A list of sources deemed authoritative.

    Examples:
        >>> auth_sources = assess_authoritative_sources(sources)
        >>> print(len(auth_sources))
    """
    authoritative_sources = []
    for source in sources:
        url = source.get("url", "")
        quality_score = source.get("quality_score", 0)
        is_authoritative_domain = any(
            pattern.search(url) for pattern in COMPILED_AUTHORITY_PATTERNS
        )
        title = source.get("title", "").lower()
        source_name = source.get("source", "").lower()
        credibility_term_count = sum(
            1 for term in HIGH_CREDIBILITY_TERMS if term in title or term in source_name
        )
        if is_authoritative_domain or quality_score >= 0.8 or credibility_term_count >= 2:
            authoritative_sources.append(source)
    return authoritative_sources


def count_recent_sources(sources: List[Dict[str, Any]], recency_threshold: int) -> int:
    """
    Count how many sources are considered recent based on a recency threshold (in days).

    A source is recent if its published date (as ISO string or parseable format) is within
    the specified number of days from the current time.

    Args:
        sources (List[Dict[str, Any]]): A list of source dictionaries.
        recency_threshold (int): The maximum age in days for a source to be considered recent.

    Returns:
        int: The count of recent sources.

    Examples:
        >>> recent_count = count_recent_sources(sources, 365)
        >>> print(recent_count)
    """
    recent_count = 0
    current_time = datetime.now().replace(tzinfo=timezone.utc)
    for source in sources:
        published_date = source.get("published_date")
        if not published_date:
            continue
        try:
            try:
                date = datetime.fromisoformat(published_date.replace('Z', '+00:00'))
            except (ValueError, AttributeError):
                date = parser.parse(published_date)
            if date.tzinfo is None:
                date = date.replace(tzinfo=timezone.utc)
            days_old = (current_time - date).days
            if days_old <= recency_threshold:
                recent_count += 1
        except Exception:
            pass
    return recent_count


def assess_fact_consistency(facts: List[Dict[str, Any]]) -> float:
    """
    Assess consistency among extracted facts based on common topics.

    The function extracts topics from each fact and calculates what percentage
    of the facts mention recurring topics. A higher percentage indicates higher consistency.

    Args:
        facts (List[Dict[str, Any]]): A list of fact dictionaries.

    Returns:
        float: A consistency score between 0.0 and 1.0.

    Examples:
        >>> consistency = assess_fact_consistency(extracted_facts)
        >>> print(consistency)
    """
    if not facts or len(facts) < 2:
        return 0.5  # Neutral score if insufficient facts
    topics = extract_topics_from_facts(facts)
    topic_counts = Counter(topics)
    if not topic_counts:
        return 0.5
    recurring_topics = {topic for topic, count in topic_counts.items() if count > 1}
    if not recurring_topics:
        return 0.5
    facts_with_recurring = sum(
        1 for fact in facts if any(topic in get_topics_in_fact(fact) for topic in recurring_topics)
    )
    return min(1.0, facts_with_recurring / len(facts))


def extract_topics_from_facts(facts: List[Dict[str, Any]]) -> List[str]:
    """
    Extract key topics or entities from a list of facts.

    This function aggregates topics from individual facts and returns a combined list.

    Args:
        facts (List[Dict[str, Any]]): A list of fact dictionaries.

    Returns:
        List[str]: A list of topics extracted from the facts.

    Examples:
        >>> topics = extract_topics_from_facts(extracted_facts)
        >>> print(topics)
    """
    all_topics: List[str] = []
    for fact in facts:
        fact_topics = get_topics_in_fact(fact)
        all_topics.extend(fact_topics)
    return all_topics


def get_topics_in_fact(fact: Dict[str, Any]) -> Set[str]:
    """
    Extract topics from a single fact.

    Topics are extracted from the 'data' field or 'source_text' if available.
    For example, vendor names or technical terms.

    Args:
        fact (Dict[str, Any]): A fact dictionary.

    Returns:
        Set[str]: A set of topics found in the fact.

    Examples:
        >>> topics = get_topics_in_fact(fact)
        >>> print(topics)
    """
    topics = set()
    if "data" in fact and isinstance(fact["data"], dict):
        data = fact["data"]
        if fact.get("type") == "vendor":
            if "vendor_name" in data:
                topics.add(data["vendor_name"].lower())
        elif fact.get("type") == "relationship":
            entities = data.get("entities", [])
            for entity in entities:
                if isinstance(entity, str):
                    topics.add(entity.lower())
        elif fact.get("type") in ["requirement", "standard", "regulation", "compliance"]:
            if "description" in data:
                extract_noun_phrases(data["description"], topics)
    if "source_text" in fact and isinstance(fact["source_text"], str):
        extract_noun_phrases(fact["source_text"], topics)
    return topics


def extract_noun_phrases(text: str, topics: Set[str]) -> None:
    """
    Extract potential noun phrases from text and add them to a topics set.

    This basic extraction finds capitalized multi-word sequences and acronyms.

    Args:
        text (str): The text from which to extract noun phrases.
        topics (Set[str]): A set to which the extracted phrases will be added.

    Returns:
        None

    Examples:
        >>> topics = set()
        >>> extract_noun_phrases("Cloud Computing Trends", topics)
        >>> print(topics)
        {'cloud computing trends'}
    """
    if not text:
        return
    for match in re.finditer(r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)+)', text):
        topics.add(match.group(1).lower())
    for match in re.finditer(r'\b([A-Z]{2,})\b', text):
        topics.add(match.group(1).lower())


def perform_statistical_validation(facts: List[Dict[str, Any]]) -> float:
    """
    Validate numeric data consistency among extracted facts.

    The function extracts numeric values from each fact's 'source_text' and 'data' fields,
    then computes the relative standard deviation. A lower relative standard deviation indicates
    higher consistency.

    Returns a score between 0.0 and 1.0 based on the consistency.

    Args:
        facts (List[Dict[str, Any]]): A list of fact dictionaries.

    Returns:
        float: The statistical validation score.

    Examples:
        >>> validation_score = perform_statistical_validation(extracted_facts)
        >>> print(validation_score)
    """
    numeric_values: List[float] = []
    pattern = re.compile(r"\b\d+(?:\.\d+)?\b")
    for fact in facts:
        source_text = fact.get("source_text", "")
        if isinstance(source_text, str):
            found_numbers = pattern.findall(source_text)
            numeric_values.extend(float(n) for n in found_numbers)
        if "data" in fact and isinstance(fact["data"], dict):
            for key, val in fact["data"].items():
                if isinstance(val, (int, float)):
                    numeric_values.append(float(val))
                elif isinstance(val, str):
                    match = pattern.search(val)
                    if match:
                        numeric_values.append(float(match.group(0)))
    if len(numeric_values) < 3:
        return 0.5  # Neutral score if insufficient numeric data.
    import statistics
    try:
        mean_val = statistics.mean(numeric_values)
        stdev_val = statistics.pstdev(numeric_values)
        if abs(mean_val) < 1e-9:
            if all(abs(x) < 1e-9 for x in numeric_values):
                return 1.0
            return 0.5
        rel_stdev = stdev_val / abs(mean_val)
        if rel_stdev < 0.1:
            return 1.0
        elif rel_stdev < 0.3:
            return 0.8
        elif rel_stdev < 0.6:
            return 0.6
        else:
            return 0.4
    except statistics.StatisticsError:
        return 0.5


def calculate_overall_confidence(
    category_scores: Dict[str, float],
    synthesis_quality: float,
    validation_score: float
) -> float:
    """
    Calculate an overall confidence score from category scores, synthesis quality, and validation score.

    The overall score is a weighted average of:
      - Average category score (50%)
      - Synthesis quality (30%)
      - Validation score (20%)

    Additional boosts are applied for full category coverage and strong statistical content.

    Args:
        category_scores (Dict[str, float]): A mapping of category names to quality scores.
        synthesis_quality (float): The quality score of the synthesis process.
        validation_score (float): The validation score from statistical checks.

    Returns:
        float: The overall confidence score between 0.0 and 1.0.

    Examples:
        >>> overall = calculate_overall_confidence(category_scores, 0.8, 0.7)
        >>> print(overall)
    """
    if not category_scores:
        return 0.3
    avg_category_score = sum(category_scores.values()) / len(category_scores)
    base_score = (
        avg_category_score * 0.5 +
        synthesis_quality * 0.3 +
        validation_score * 0.2
    )
    if len(category_scores) >= 7 and all(score >= 0.6 for score in category_scores.values()):
        base_score += 0.1
    stats_categories = sum(
        1 for cat, score in category_scores.items() if cat in ['market_dynamics', 'cost_considerations'] and score >= 0.7
    )
    if stats_categories >= 2:
        base_score += 0.05
    return min(1.0, base_score)


def assess_synthesis_quality(synthesis: Dict[str, Any]) -> float:
    """
    Assess the quality of synthesis output based on section content, citations, and statistics.

    The function checks for the presence and coverage of synthesis sections, their content,
    and associated citations and statistics to determine a quality score.

    Args:
        synthesis (Dict[str, Any]): A dictionary representing the synthesis output.

    Returns:
        float: A synthesis quality score between 0.0 and 1.0.

    Examples:
        >>> quality = assess_synthesis_quality(synthesis_output)
        >>> print(quality)
    """
    if not synthesis:
        return 0.3
    score = 0.5
    synthesis_content = synthesis.get("synthesis", {})
    if not synthesis_content:
        return 0.3
    sections_with_content = sum(
        1 for section in synthesis_content.values()
        if isinstance(section, dict) and section.get("content") and len(section.get("content", "")) > 50
    )
    section_ratio = sections_with_content / max(1, len(synthesis_content))
    score += section_ratio * 0.2
    sections_with_citations = sum(
        1 for section in synthesis_content.values()
        if isinstance(section, dict) and section.get("citations") and len(section.get("citations", [])) > 0
    )
    citation_ratio = sections_with_citations / max(1, len(synthesis_content))
    score += citation_ratio * 0.15
    sections_with_stats = sum(
        1 for section in synthesis_content.values()
        if isinstance(section, dict) and section.get("statistics") and len(section.get("statistics", [])) > 0
    )
    stats_ratio = sections_with_stats / max(1, len(synthesis_content))
    score += stats_ratio * 0.15
    return min(1.0, score)
</file>

<file path="src/react_agent/utils/validations.py">
import re
import urllib
from urllib.parse import urlparse, unquote

def is_valid_url(url: str) -> bool:
    """Validate if a URL is properly formatted."""
    # Add PDF detection to URL validation
    decoded_url = unquote(url).lower()
    if '.pdf' in decoded_url:
        return False
    
    # Enhanced fake URL detection
    fake_patterns = [
        r'example\.(com|org|net)',
        r'\b(test|sample|dummy|placeholder)\.',
        r'\b(mock|fake|staging|dev)\.(com|org|net)\b'
    ]
    if any(re.search(p, url, re.IGNORECASE) for p in fake_patterns):
        return False
    
    if not url:
        return False

    # Check for example/fake URLs
    fake_url_patterns = [
        r'example\.com',
        r'sample\.org',
        r'test\.net',
        r'domain\.com',
        r'yourcompany\.com',
        r'acme\.com',
        r'widget\.com',
        r'placeholder\.net',
        r'company\.org'
    ]

    for pattern in fake_url_patterns:
        if re.search(pattern, url, re.IGNORECASE):
            return False

    # Basic URL validation
    try:
        result = urlparse(url)
        return all([result.scheme in ('http', 'https'), result.netloc])
    except Exception:
        return False
</file>

</files>
